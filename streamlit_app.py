import streamlit as st
import pandas as pd
import numpy as np
import os
import tempfile
import base64
from PIL import Image
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import io
import time
import logging
from datetime import datetime, timedelta
import fitz  # PyMuPDF
import re # ì •ê·œì‹ ì¶”ê°€
import boto3
import json
from botocore.exceptions import ClientError
import hashlib
from pdf2image import convert_from_path  # PDFë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
from openpyxl import Workbook # Workbook ì„í¬íŠ¸ í™•ì¸
from openpyxl.drawing.image import Image as XLImage
from openpyxl.utils.dataframe import dataframe_to_rows # dataframe_to_rows ì„í¬íŠ¸ ì¶”ê°€
from openpyxl import load_workbook
from functools import lru_cache
import concurrent.futures
from typing import List, Dict

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
import pdf3_module
import data_analyzer
from data_analyzer import get_unique_departments, filter_by_department, load_excel_data

# ì•± ì„¤ì •
st.set_page_config(
    page_title="ìƒê³„ë°±ë³‘ì› ì¸ìˆ˜ì¦ & ì—‘ì…€ ë°ì´í„° ë¹„êµ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)



# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# botocore ë¡œê¹… ë ˆë²¨ ì„¤ì • (DEBUG -> WARNING)
logging.getLogger('botocore').setLevel(logging.ERROR)
logging.getLogger('boto3').setLevel(logging.ERROR)
logging.getLogger('s3transfer').setLevel(logging.ERROR)
logging.getLogger('urllib3').setLevel(logging.ERROR)



# S3 ì„¤ì •ì„ secretsì—ì„œ ê°€ì ¸ì˜¤ê¸°
AWS_CONFIG = {
    "aws_access_key_id": st.secrets["aws"]["AWS_ACCESS_KEY_ID"],
    "aws_secret_access_key": st.secrets["aws"]["AWS_SECRET_ACCESS_KEY"],
    "region_name": st.secrets["aws"]["AWS_REGION"]
}
S3_BUCKET = st.secrets["aws"]["S3_BUCKET"]

# S3 í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
def get_s3_client():
    try:
        return boto3.client('s3', **AWS_CONFIG)
    except Exception as e:
        logger.error(f"S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return None

# S3 ë””ë ‰í† ë¦¬ êµ¬ì¡°
S3_DIRS = {
    "EXCEL": "excel/",
    "PDF": "pdf/",
    "EXTRACTED": "extracted/",
    "OCR_RESULTS": "ocr_results/",
    "METADATA": "metadata/",
    "DB": "db/",  # DB ë””ë ‰í† ë¦¬ ì¶”ê°€
    "RESULTS": "results/",  # ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ì¶”ê°€
    "PREVIEW_IMAGES": "preview_images/"  # ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì¶”ê°€
}

def get_s3_file_modified_time(s3_handler, key):
    """S3 íŒŒì¼ì˜ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°ì„ ë°˜í™˜ (datetime)"""
    try:
        response = s3_handler.s3_client.head_object(Bucket=s3_handler.bucket, Key=key)
        return response['LastModified']
    except Exception as e:
        return None
    
def save_all_date_mismatches(s3_handler, mismatch_data):
    """
    ì „ì²´ ë¶ˆì¼ì¹˜ ë°ì´í„° DataFrame(mismatch_data)ì—ì„œ
    ë‚ ì§œë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ S3ì— /results/{ë‚ ì§œ}/mismatches.json ì €ì¥.
    """
    if mismatch_data.empty:
        logger.info("[ìë™í™”] ë‚ ì§œë³„ ë¶ˆì¼ì¹˜ ë°ì´í„° ì €ì¥: ë°ì´í„° ì—†ìŒ")
        return
    
    logger.info(f"[ìë™í™”] ì „ì²´ ë¶ˆì¼ì¹˜ ë°ì´í„° ì €ì¥ ì‹œì‘: ì´ {len(mismatch_data)}ê°œ í•­ëª©")
    total_saved = 0
    
    for date in mismatch_data['ë‚ ì§œ'].unique():
        # ë‚ ì§œ í˜•ì‹ì´ ì˜ëª»ëœ í´ë” ìƒì„±ì„ ë°©ì§€!
        # ë‚ ì§œëŠ” í•­ìƒ 'YYYY-MM-DD' í˜•íƒœë¡œ ë³€í™˜
        safe_date = pd.to_datetime(date, errors='coerce').strftime('%Y-%m-%d')
        date_df = mismatch_data[mismatch_data['ë‚ ì§œ'] == date].copy()
        s3_handler.save_mismatch_data(safe_date, date_df)
        total_saved += len(date_df)
        logger.info(f"[ìë™í™”] {safe_date} ë¶ˆì¼ì¹˜ ë°ì´í„° {len(date_df)}í–‰ ì €ì¥ ì™„ë£Œ")
    
    logger.info(f"[ìë™í™”] ë‚ ì§œë³„ ë¶ˆì¼ì¹˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ: ì´ {total_saved}ê°œ í•­ëª© ì €ì¥ë¨")

class S3Handler:
    def __init__(self):
        self.s3_client = get_s3_client()
        if self.s3_client is None:
            raise Exception("S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨")
        self.bucket = S3_BUCKET
        self.dirs = S3_DIRS
        self.image_cache = ImageCache()
    
    def generate_file_key(self, date_str, filename, dir_type):
        """íŒŒì¼ í‚¤ ìƒì„± (ê²½ë¡œ)"""
        return f"{self.dirs[dir_type]}{date_str}/{filename}"

    def upload_file(self, file_obj, date_str, original_filename, dir_type):
        """íŒŒì¼ ì—…ë¡œë“œ"""
        try:
            file_key = self.generate_file_key(date_str, original_filename, dir_type)
            self.s3_client.upload_fileobj(file_obj, self.bucket, file_key)
            return {"status": "success", "key": file_key}
        except Exception as e:
            logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨ ({original_filename}): {e}")
            return {"status": "error", "message": str(e)}

    def download_file(self, file_key):
        """íŒŒì¼ ë‹¤ìš´ë¡œë“œ"""
        try:
            response = self.s3_client.get_object(Bucket=self.bucket, Key=file_key)
            return {"status": "success", "data": response['Body'].read()}
        except Exception as e:
            logger.error(f"S3 ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ({file_key}): {e}")
            return {"status": "error", "message": str(e)}

    def get_s3_file_modified_time(s3_handler, key):
        """S3 íŒŒì¼ì˜ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°ì„ ë°˜í™˜ (datetime)"""
        try:
            response = s3_handler.s3_client.head_object(Bucket=s3_handler.bucket, Key=key)
            return response['LastModified']
        except Exception as e:
            return None

    def save_metadata(self, date_str, metadata):
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        try:
            metadata_key = f"{self.dirs['METADATA']}{date_str}/metadata.json"
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=metadata_key,
                Body=json.dumps(metadata, ensure_ascii=False)
            )
            return {"status": "success", "key": metadata_key}
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({date_str}): {e}")
            return {"status": "error", "message": str(e)}

    def load_metadata(self, date_str):
        """ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        try:
            metadata_key = f"{self.dirs['METADATA']}{date_str}/metadata.json"
            response = self.s3_client.get_object(Bucket=self.bucket, Key=metadata_key)
            return {"status": "success", "data": json.loads(response['Body'].read())}
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                return {"status": "not_found"}
            logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {e}")
            return {"status": "error", "message": str(e)}

    def list_processed_dates(self):
        """ì²˜ë¦¬ëœ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ë©”íƒ€ë°ì´í„° ë””ë ‰í† ë¦¬ ì¡°íšŒ
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket,
                Prefix=self.dirs['METADATA']
            )
            
            dates = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
            
            # metadata ë””ë ‰í† ë¦¬ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
            for obj in response.get('Contents', []):
                # metadata/YYYY-MM-DD/metadata.json í˜•ì‹ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                parts = obj['Key'].split('/')
                if len(parts) >= 2:
                    date_str = parts[1]
                    if date_str and date_str != "":
                        dates.add(date_str)
            
            # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ë‹¤ë¥¸ ë””ë ‰í† ë¦¬ë„ í™•ì¸
            if not dates:
                # OCR ê²°ê³¼ ë””ë ‰í† ë¦¬ í™•ì¸
                ocr_response = self.s3_client.list_objects_v2(
                    Bucket=self.bucket,
                    Prefix=self.dirs['OCR_RESULTS']
                )
                for obj in ocr_response.get('Contents', []):
                    parts = obj['Key'].split('/')
                    if len(parts) >= 2:
                        date_str = parts[1]
                        if date_str and date_str != "":
                            dates.add(date_str)
                
                # ì¶”ì¶œëœ PDF ë””ë ‰í† ë¦¬ í™•ì¸
                pdf_response = self.s3_client.list_objects_v2(
                    Bucket=self.bucket,
                    Prefix=self.dirs['EXTRACTED']
                )
                for obj in pdf_response.get('Contents', []):
                    parts = obj['Key'].split('/')
                    if len(parts) >= 2:
                        date_str = parts[1]
                        if date_str and date_str != "":
                            dates.add(date_str)
            
            # ë‚ ì§œ ëª©ë¡ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì •ë ¬
            dates = sorted(list(dates))
            
            if not dates:
                logger.warning("ì²˜ë¦¬ëœ ë‚ ì§œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "success", "dates": []}
            
            return {"status": "success", "dates": dates}
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ëœ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": str(e)}

    def save_extracted_pdf(self, date_str, dept_name, page_num, pdf_content):
        """ì¶”ì¶œëœ PDF ì €ì¥"""
        try:
            file_key = f"{self.dirs['EXTRACTED']}{date_str}/{dept_name}/page_{page_num}.pdf"
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=file_key,
                Body=pdf_content
            )
            return {"status": "success", "key": file_key}
        except Exception as e:
            logger.error(f"ì¶”ì¶œ PDF ì €ì¥ ì‹¤íŒ¨ ({date_str}/{dept_name}/{page_num}): {e}")
            return {"status": "error", "message": str(e)}

    def get_extracted_pdf(self, date_str, dept_name, page_num):
        """ì¶”ì¶œëœ PDF ì¡°íšŒ"""
        try:
            file_key = f"{self.dirs['EXTRACTED']}{date_str}/{dept_name}/page_{page_num}.pdf"
            # S3ì—ì„œ ê°ì²´ ë©”íƒ€ë°ì´í„°ë§Œ í™•ì¸ (ì¡´ì¬ ì—¬ë¶€ë§Œ íŒë‹¨)
            self.s3_client.head_object(Bucket=self.bucket, Key=file_key)
            # íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ì„±ê³µ ìƒíƒœì™€ íŒŒì¼ í‚¤ ë°˜í™˜
            return {"status": "success", "key": file_key}
        except ClientError as e:
            if e.response['Error']['Code'] == '404' or e.response['Error']['Code'] == 'NoSuchKey':
                return {"status": "not_found"}
            logger.error(f"ì¶”ì¶œ PDF ì¡°íšŒ ì‹¤íŒ¨ ({file_key}): {e}")
            return {"status": "error", "message": str(e)}

    def save_mismatch_data(self, date_str, mismatch_df):
    
        try:
            # ë‚ ì§œ í˜•ì‹ ë³´ì •
            if not isinstance(date_str, str) or len(date_str) != 10 or not date_str[:4].isdigit():
                # íŒë‹¤ìŠ¤, ë„˜íŒŒì´, datetime ë“± ëª¨ë“  ì¼€ì´ìŠ¤ ì•ˆì „í•˜ê²Œ ë³´ì •
                try:
                    date_str = pd.to_datetime(date_str).strftime('%Y-%m-%d')
                except Exception:
                    date_str = str(date_str)[:10]
            else:
                # í˜¹ì‹œë¼ë„ '2025-03-30 00:00:00'ì²˜ëŸ¼ ë“¤ì–´ì˜¨ ê²½ìš°
                if ' ' in date_str:
                    date_str = date_str.split(' ')[0]

            # ì €ì¥ ì „ ì¤‘ë³µ ì œê±° (ê°™ì€ ë‚ ì§œ ë‚´ì—ì„œ)
            if not mismatch_df.empty:
                before_dedup = len(mismatch_df)
                mismatch_df = mismatch_df.drop_duplicates(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='last')
                after_dedup = len(mismatch_df)
                if before_dedup != after_dedup:
                    logger.info(f"ë‚ ì§œë³„ ì €ì¥ ì‹œ ì¤‘ë³µ ì œê±°: {before_dedup}ê°œ â†’ {after_dedup}ê°œ (ë‚ ì§œ: {date_str})")
                
                # ë‚ ì§œ í˜•ì‹ ë³´ì¥ (JSON ì €ì¥ ì „)
                if 'ë‚ ì§œ' in mismatch_df.columns:
                    # ë‚ ì§œë¥¼ ë¬¸ìì—´ë¡œ í™•ì‹¤íˆ ë³€í™˜
                    mismatch_df['ë‚ ì§œ'] = mismatch_df['ë‚ ì§œ'].astype(str)
                    # YYYY-MM-DD í˜•ì‹ì´ ì•„ë‹Œ ê²½ìš° íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •
                    invalid_mask = ~mismatch_df['ë‚ ì§œ'].str.match(r'^\d{4}-\d{2}-\d{2}$')
                    if invalid_mask.any():
                        logger.warning(f"ë‚ ì§œë³„ ì €ì¥ ì‹œ ì˜ëª»ëœ ë‚ ì§œ í˜•ì‹ {invalid_mask.sum()}ê°œ ë°œê²¬. íŒŒì¼ëª…({date_str})ìœ¼ë¡œ ìˆ˜ì •")
                        mismatch_df.loc[invalid_mask, 'ë‚ ì§œ'] = date_str

            mismatch_key = f"{self.dirs['RESULTS']}{date_str}/mismatches.json"
            json_data = mismatch_df.to_json(orient="records", indent=4, date_format='iso')
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=mismatch_key,
                Body=json_data
            )
            logger.info(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {mismatch_key}")
            return {"status": "success", "key": mismatch_key}
        except Exception as e:
            logger.error(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({date_str}): {e}")
            return {"status": "error", "message": str(e)}

    def load_mismatch_data(self, date_str):
    
        try:
            # ë‚ ì§œ í˜•ì‹ ë³´ì •(ë™ì¼)
            if not isinstance(date_str, str) or len(date_str) != 10 or not date_str[:4].isdigit():
                try:
                    date_str = pd.to_datetime(date_str).strftime('%Y-%m-%d')
                except Exception:
                    date_str = str(date_str)[:10]
            else:
                if ' ' in date_str:
                    date_str = date_str.split(' ')[0]

            mismatch_key = f"{self.dirs['RESULTS']}{date_str}/mismatches.json"
            response = self.s3_client.get_object(Bucket=self.bucket, Key=mismatch_key)
            json_data = response['Body'].read()
            df = pd.read_json(io.StringIO(json_data.decode('utf-8')), orient="records")
            logger.info(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {mismatch_key}")
            return {"status": "success", "data": df}
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning(f"S3ì— ë¶ˆì¼ì¹˜ ë°ì´í„° ì—†ìŒ ({mismatch_key})")
                return {"status": "not_found"}
            logger.error(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {e}")
            return {"status": "error", "message": str(e)}

            
    def save_ocr_text(self, date_str, ocr_text):
        """OCR í…ìŠ¤íŠ¸ ê²°ê³¼ë¥¼ S3ì— ì €ì¥"""
        try:
            # í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥ (í˜ì´ì§€ë³„)
            for i, page_text in enumerate(ocr_text):
                text_key = f"{self.dirs['OCR_RESULTS']}{date_str}/page_{i+1}.txt"
                self.s3_client.put_object(
                    Bucket=self.bucket,
                    Key=text_key,
                    Body=page_text.encode('utf-8')
                )
            
            # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹œ íŒŒì¼ (ì„ íƒì )
            all_text_key = f"{self.dirs['OCR_RESULTS']}{date_str}/all_pages.txt"
            all_text = "\n\n--- í˜ì´ì§€ êµ¬ë¶„ì„  ---\n\n".join(ocr_text)
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=all_text_key,
                Body=all_text.encode('utf-8')
            )
            
            logger.info(f"OCR í…ìŠ¤íŠ¸ ì €ì¥ ì™„ë£Œ: {date_str}ì˜ {len(ocr_text)}ê°œ í˜ì´ì§€")
            return {"status": "success", "pages": len(ocr_text)}
        except Exception as e:
            logger.error(f"OCR í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨ ({date_str}): {e}")
            return {"status": "error", "message": str(e)}
    
    def load_ocr_text(self, date_str):
        """S3ì—ì„œ OCR í…ìŠ¤íŠ¸ ê²°ê³¼ ë¡œë“œ"""
        try:
            # ë¨¼ì € ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  í…ìŠ¤íŠ¸ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            response = self.s3_client.list_objects_v2(
                Bucket=self.bucket,
                Prefix=f"{self.dirs['OCR_RESULTS']}{date_str}/"
            )
            
            # page_*.txt í˜•ì‹ì˜ íŒŒì¼ë§Œ í•„í„°ë§
            page_files = []
            for obj in response.get('Contents', []):
                if 'page_' in obj['Key'] and obj['Key'].endswith('.txt'):
                    page_num = int(obj['Key'].split('page_')[1].split('.')[0])
                    page_files.append((page_num, obj['Key']))
            
            # í˜ì´ì§€ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
            page_files.sort(key=lambda x: x[0])
            
            ocr_text = []
            for _, page_key in page_files:
                page_response = self.s3_client.get_object(Bucket=self.bucket, Key=page_key)
                page_content = page_response['Body'].read().decode('utf-8')
                ocr_text.append(page_content)
            
            logger.info(f"OCR í…ìŠ¤íŠ¸ ë¡œë“œ ì™„ë£Œ: {date_str}ì˜ {len(ocr_text)}ê°œ í˜ì´ì§€")
            return {"status": "success", "data": ocr_text}
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning(f"S3ì— OCR í…ìŠ¤íŠ¸ ì—†ìŒ ({date_str})")
                return {"status": "not_found"}
            logger.error(f"OCR í…ìŠ¤íŠ¸ ë¡œë“œ ì‹¤íŒ¨ ({date_str}): {e}")
            return {"status": "error", "message": str(e)}
    
    def get_file_hash(self, file_obj):
        """íŒŒì¼ ë‚´ìš©ì˜ MD5 í•´ì‹œê°’ ê³„ì‚°"""
        try:
            file_obj.seek(0)
            file_bytes = file_obj.read()
            file_hash = hashlib.md5(file_bytes).hexdigest()
            file_obj.seek(0)  # íŒŒì¼ í¬ì¸í„° ì´ˆê¸°í™”
            return {"status": "success", "hash": file_hash}
        except Exception as e:
            logger.error(f"íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"status": "error", "message": str(e)}
    
    def check_file_exists(self, date_str, file_hash, file_type):
        """í•´ì‹œê°’ìœ¼ë¡œ ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ S3ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            # í•´ì‹œ ì •ë³´ê°€ ì €ì¥ëœ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
            metadata_result = self.load_metadata(date_str)
            if metadata_result["status"] == "success":
                metadata = metadata_result["data"]
                
                # íŒŒì¼ íƒ€ì…ì— ë”°ë¼ í™•ì¸
                if file_type == "PDF" and "pdf_hash" in metadata:
                    # í•´ì‹œê°’ ë¹„êµ
                    if metadata["pdf_hash"] == file_hash:
                        return {
                            "status": "success", 
                            "exists": True, 
                            "metadata": metadata
                        }
                
                elif file_type == "EXCEL" and "excel_hash" in metadata:
                    if metadata["excel_hash"] == file_hash:
                        return {
                            "status": "success", 
                            "exists": True, 
                            "metadata": metadata
                        }
            
            # íŒŒì¼ì´ ì—†ê±°ë‚˜ í•´ì‹œê°’ì´ ë‹¤ë¦„
            return {"status": "success", "exists": False}
        
        except Exception as e:
            logger.error(f"íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨ ({date_str}, {file_type}): {e}")
            return {"status": "error", "message": str(e)}

    def save_missing_items_by_date(self, missing_df, date_str):

        try:
            mismatch_key = f"{self.dirs['RESULTS']}{date_str}/mismatches.json"
            try:
                s3_obj = self.s3_client.get_object(Bucket=self.bucket, Key=mismatch_key)
                json_bytes = s3_obj["Body"].read()
                mismatch_df = pd.read_json(io.BytesIO(json_bytes), orient="records")
            except Exception:
                # ê¸°ì¡´ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë¹ˆ DFë¡œ ì‹œì‘
                mismatch_df = pd.DataFrame()

            # 1. ì»¬ëŸ¼ êµ¬ì¡° ë§ì¶”ê¸°
            for col in missing_df.columns:
                if col not in mismatch_df.columns:
                    mismatch_df[col] = ""
            for col in mismatch_df.columns:
                if col not in missing_df.columns:
                    missing_df[col] = ""
            missing_df = missing_df[mismatch_df.columns]

            # 2. ê¸°ì¡´ + ì‹ ê·œ(ì „ì‚°ëˆ„ë½) append/concat
            combined = pd.concat([mismatch_df, missing_df], ignore_index=True)

            # 3. (í•„ìš”ì‹œ) ì¤‘ë³µì œê±° (ex: ë‚ ì§œ, ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ)
            combined = combined.drop_duplicates(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='last')

            # 4. ë‚ ì§œë³„ íŒŒì¼ ì €ì¥ (í†µí•© ì‘ì—…ì€ ë¶€ì„œë³„ í†µê³„ íƒ­ì—ì„œ ìˆ˜ë™ ì‹¤í–‰)
            mismatch_json = combined.to_json(orient="records", indent=4)
            self.s3_client.put_object(Bucket=self.bucket, Key=mismatch_key, Body=mismatch_json)
            logger.info(f"ë‚ ì§œë³„ mismatches.json({date_str}) ì €ì¥/ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(combined)}ê°œ")
            
            # 5. ì „ì²´ í†µí•© íŒŒì¼ ì¦‰ì‹œ ì—…ë°ì´íŠ¸ ì œê±° (ë¶€ì„œë³„ í†µê³„ íƒ­ì—ì„œ ìˆ˜ë™ ë³‘í•©)
            # update_result = self.update_full_mismatches_json()  # ì£¼ì„ ì²˜ë¦¬
            # if update_result["status"] == "success":
            #     logger.info(f"ì „ì‚°ëˆ„ë½ ì €ì¥ í›„ ì „ì²´ í†µí•© íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {update_result.get('count', 0)}ê°œ í•­ëª©")
            # else:
            #     logger.warning(f"ì „ì‚°ëˆ„ë½ ì €ì¥ í›„ ì „ì²´ í†µí•© íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_result.get('message')}")
            
            return {"status": "success", "message": f"{date_str} ì „ì‚°ëˆ„ë½ ë°ì´í„° ì €ì¥ ì™„ë£Œ. ë¶€ì„œë³„ í†µê³„ì—ì„œ 'ë‚ ì§œë³„ ì‘ì—… ë‚´ìš© ë³‘í•©' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”."}

        except Exception as e:
            logger.error(f"ë‚ ì§œë³„ mismatches ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)
            return {"status": "error", "message": str(e)}
    
    def list_all_dates_in_results(self):

        prefix = self.dirs['RESULTS']
        paginator = self.s3_client.get_paginator('list_objects_v2')
        operation_parameters = {'Bucket': self.bucket, 'Prefix': prefix, 'Delimiter': '/'}
        page_iterator = paginator.paginate(**operation_parameters)

        date_folders = set()
        date_pattern = re.compile(r'(\d{4}-\d{2}-\d{2})/')  # 2025-05-21/ íŒ¨í„´

        for page in page_iterator:
            if "CommonPrefixes" in page:
                for cp in page["CommonPrefixes"]:
                    folder = cp["Prefix"][len(prefix):]
                    match = date_pattern.match(folder)
                    if match:
                        date_folders.add(match.group(1))
            # (í˜¹ì‹œ ë‚ ì§œ í´ë”ê°€ Prefix ë§ê³  Keyì—ì„œë§Œ ë°œê²¬ë˜ëŠ” êµ¬ì¡°ë¼ë©´ ì•„ë˜ ì½”ë“œë„ ì¶”ê°€)
            if "Contents" in page:
                for obj in page["Contents"]:
                    key = obj["Key"][len(prefix):]
                    parts = key.split('/')
                    if len(parts) > 1 and re.match(r'\d{4}-\d{2}-\d{2}', parts[0]):
                        date_folders.add(parts[0])

        return sorted(list(date_folders))

    def update_full_mismatches_json(self):
        """ë‚ ì§œë³„ mismatches.json íŒŒì¼ë“¤ì„ í†µí•©í•˜ì—¬ ì „ì²´ íŒŒì¼ ìƒì„±"""
        try:
            prefix = f"{self.dirs['RESULTS']}"
            date_folders = self.list_all_dates_in_results()
            
            if not date_folders:
                logger.warning("í†µí•©í•  ë‚ ì§œë³„ mismatches.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "error", "message": "ë‚ ì§œë³„ íŒŒì¼ ì—†ìŒ"}
            
            all_mismatches = []
            for date_str in date_folders:
                key = f"{prefix}{date_str}/mismatches.json"
                try:   
                    s3_obj = self.s3_client.get_object(Bucket=self.bucket, Key=key)
                    json_bytes = s3_obj["Body"].read()
                    df = pd.read_json(io.BytesIO(json_bytes), orient="records")
                    if not df.empty:
                        logger.debug(f"{date_str} íŒŒì¼ ë¡œë“œ í›„ ë‚ ì§œ ìƒ˜í”Œ: {df['ë‚ ì§œ'].head().tolist()}")
                        
                        # ë‚ ì§œ í˜•ì‹ ê²€ì¦ ë° ìˆ˜ì •
                        try:
                            # ë¨¼ì € í˜„ì¬ ë‚ ì§œ ì»¬ëŸ¼ ìƒíƒœ í™•ì¸
                            if 'ë‚ ì§œ' in df.columns:
                                # ë‚ ì§œê°€ ìˆ«ì(timestamp)ë¡œ ì €ì¥ëœ ê²½ìš° ì²˜ë¦¬
                                if df['ë‚ ì§œ'].dtype in ['int64', 'float64']:
                                    logger.warning(f"{date_str}: ë‚ ì§œê°€ ìˆ«ì í˜•íƒœë¡œ ì €ì¥ë¨. íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •")
                                    df['ë‚ ì§œ'] = date_str
                                else:
                                    # ë¬¸ìì—´ì´ì§€ë§Œ ì˜ëª»ëœ í˜•ì‹ì¸ ê²½ìš°
                                    df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'], errors='coerce')
                                    invalid_dates = df['ë‚ ì§œ'].isna()
                                    if invalid_dates.any():
                                        logger.warning(f"{date_str}: {invalid_dates.sum()}ê°œ ì˜ëª»ëœ ë‚ ì§œ ë°œê²¬. íŒŒì¼ëª… ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •")
                                        df.loc[invalid_dates, 'ë‚ ì§œ'] = pd.to_datetime(date_str)
                                    
                                    # ìµœì¢… ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                    df['ë‚ ì§œ'] = df['ë‚ ì§œ'].dt.strftime('%Y-%m-%d')
                                
                                logger.debug(f"{date_str} ë‚ ì§œ ìˆ˜ì • í›„ ìƒ˜í”Œ: {df['ë‚ ì§œ'].head().tolist()}")
                                all_mismatches.append(df)
                            else:
                                logger.warning(f"{date_str}: 'ë‚ ì§œ' ì»¬ëŸ¼ì´ ì—†ìŒ")
                                continue
                        except Exception as e:
                            logger.warning(f"{date_str} ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                            # ì˜¤ë¥˜ ë°œìƒ ì‹œ íŒŒì¼ëª…ì„ ë‚ ì§œë¡œ ì‚¬ìš©
                            df['ë‚ ì§œ'] = date_str
                            all_mismatches.append(df)
                except Exception as e:
                    logger.warning(f"{key} ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue

            if not all_mismatches:
                logger.warning("ìœ íš¨í•œ mismatches ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "error", "message": "ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ"}
            
            # ë°ì´í„° í†µí•©
            merged_df = pd.concat(all_mismatches, ignore_index=True)
            logger.info(f"ë‚ ì§œë³„ íŒŒì¼ í†µí•© í›„ ì´ í•­ëª© ìˆ˜: {len(merged_df)}ê°œ")
            
            # ì¤‘ë³µ ì œê±° ì „ ì¤‘ë³µ í•­ëª© ë¶„ì„
            duplicate_mask = merged_df.duplicated(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep=False)
            duplicate_items = merged_df[duplicate_mask]
            if not duplicate_items.empty:
                logger.warning(f"ì¤‘ë³µ í•­ëª© ë°œê²¬: {len(duplicate_items)}ê°œ")
                # ì¤‘ë³µ í•­ëª©ì˜ ìƒìœ„ 10ê°œ ìƒ˜í”Œ ë¡œê¹…
                sample_duplicates = duplicate_items.groupby(['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ']).size().reset_index(name='ì¤‘ë³µìˆ˜').head(10)
                for _, row in sample_duplicates.iterrows():
                    logger.warning(f"ì¤‘ë³µ ì˜ˆì‹œ: {row['ë‚ ì§œ']} {row['ë¶€ì„œëª…']} {row['ë¬¼í’ˆì½”ë“œ']} - {row['ì¤‘ë³µìˆ˜']}ê°œ")
            
            # ì¤‘ë³µ ì œê±° (ë‚ ì§œ, ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ ê¸°ì¤€)
            before_dedup = len(merged_df)
            merged_df = merged_df.drop_duplicates(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='last')
            after_dedup = len(merged_df)
            logger.info(f"ì¤‘ë³µ ì œê±°: {before_dedup}ê°œ â†’ {after_dedup}ê°œ (ì œê±°ëœ ì¤‘ë³µ: {before_dedup - after_dedup}ê°œ)")
            
            # ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ í•„í„°ë§ ì ìš© (í†µí•© ì‹œì ì—ì„œ ìµœì¢… í•„í„°ë§)
            try:
                completion_logs_result = self.load_completion_logs()
                if completion_logs_result["status"] == "success":
                    completion_logs = completion_logs_result["data"]
                    if completion_logs:
                        before_completion_filter = len(merged_df)
                        # filter_completed_items í•¨ìˆ˜ ì‚¬ìš©í•˜ì—¬ ì™„ë£Œëœ í•­ëª© ì œê±° (ë‚ ì§œ ë²”ìœ„ ì—†ì´ ì „ì²´ ì ìš©)
                        merged_df = filter_completed_items(merged_df, completion_logs)
                        after_completion_filter = len(merged_df)
                        logger.info(f"í†µí•© ì‹œì  ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§: {before_completion_filter}ê°œ â†’ {after_completion_filter}ê°œ (ì œì™¸ëœ í•­ëª©: {before_completion_filter - after_completion_filter}ê°œ)")
                    else:
                        logger.info("ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ê°€ ì—†ì–´ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                else:
                    logger.warning(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {completion_logs_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}. í•„í„°ë§ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
            except Exception as filter_err:
                logger.error(f"í†µí•© ì‹œì  ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {filter_err}. í•„í„°ë§ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
            
            # ì •ë ¬ (ë‚ ì§œ, ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ ìˆœ)
            merged_df = merged_df.sort_values(['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'])
            
            # ì €ì¥
            full_mismatches_key = f"{self.dirs['RESULTS']}mismatches_full.json"
            json_data = merged_df.to_json(orient="records", indent=4)
            self.s3_client.put_object(
                Bucket=self.bucket,
                Key=full_mismatches_key,
                Body=json_data
            )
            
            logger.info(f"ì „ì²´ í†µí•© mismatches_full.json ì €ì¥ ì™„ë£Œ (ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ì ìš©): {len(merged_df)}ê°œ í•­ëª©")
            return {"status": "success", "count": len(merged_df)}
            
        except Exception as e:
            logger.error(f"ì „ì²´ í†µí•© mismatches_full.json ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {"status": "error", "message": str(e)}


    def load_full_mismatches(self):
      
        full_mismatches_key = f"{self.dirs['RESULTS']}mismatches_full.json"
        try:
            s3_obj = self.s3_client.get_object(Bucket=self.bucket, Key=full_mismatches_key)
            json_bytes = s3_obj["Body"].read()
            df = pd.read_json(io.BytesIO(json_bytes), orient="records")
            return df
        except Exception as e:
            logger.error(f"ì „ì²´ í†µí•© mismatches_full.json ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return pd.DataFrame()


    def save_pdf_preview_image(self, date_str, dept_name, page_num, img_obj: Image.Image):
        """
    PDF ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ë¥¼ ì¸ë„¤ì¼ë¡œ ë³€í™˜í•´ S3ì— ì €ì¥í•˜ê³ ,
    ë‚ ì§œë³„ ë©”íƒ€ë°ì´í„°(preview_images)ì— ì •ë³´ ë°˜ì˜.
    """
        try:
            if self.s3_client is None:
                logger.error("S3 í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return {"status": "error", "message": "S3 ì—°ê²° ì‹¤íŒ¨"}

            # ë¶€ì„œëª… í´ë”/íŒŒì¼ëª… ì•ˆì „í™”
            safe_dept_name = dept_name.replace('/', '_').replace('\\', '_')

            # --- ì¸ë„¤ì¼ ë³€í™˜ (ì˜ˆ: 400x600) ---
            img = img_obj.copy()
            img.thumbnail((350, 500))  # ë¹„ìœ¨ìœ ì§€ ìµœëŒ€ 400x600

            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format='PNG', optimize=True, compress_level=6)
            img_byte_arr.seek(0)

            # --- íŒŒì¼ ê²½ë¡œ ---
            file_key = f"preview_images/{date_str}/{safe_dept_name}_page{page_num}_preview.png"
            
            try:
                self.s3_client.upload_fileobj(img_byte_arr, self.bucket, file_key)
            except ClientError as ce:
                logger.error(f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {ce}")
                return {"status": "error", "message": f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {ce.response.get('Error',{}).get('Message', 'ì•Œ ìˆ˜ ì—†ìŒ')}"}

            # --- ë©”íƒ€ë°ì´í„°ì— ì´ë¯¸ì§€ ì •ë³´ ë°˜ì˜ ---
            metadata_result = self.load_metadata(date_str)
            if metadata_result.get("status") != "success":
                metadata = {}
            else:
                metadata = metadata_result.get("data", {})
            if not isinstance(metadata, dict):
                metadata = {}

            if "preview_images" not in metadata:
                metadata["preview_images"] = []

            image_info = {
                "dept": dept_name,  # ì›ë³¸ ë¶€ì„œëª…(í‘œì‹œìš©)
                "page": page_num,
                "file_key": file_key
            }

            # ê¸°ì¡´ì— ë™ì¼ ë¶€ì„œ/í˜ì´ì§€ê°€ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
            exists = False
            for img_info_item in metadata["preview_images"]:
                if img_info_item.get("dept") == dept_name and img_info_item.get("page") == page_num:
                    img_info_item["file_key"] = file_key
                    exists = True
                    break
            if not exists:
                metadata["preview_images"].append(image_info)

            save_result = self.save_metadata(date_str, metadata)
            if save_result.get("status") != "success":
                logger.warning(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨ ({date_str}): {save_result.get('message')}")

            return {"status": "success", "message": "ì¸ë„¤ì¼ ì´ë¯¸ì§€ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê¸°ë¡ ì™„ë£Œ", "file_key": file_key}

        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
            return {"status": "error", "message": f"ì´ë¯¸ì§€ ì €ì¥ ì˜¤ë¥˜: {str(e)}"}
        


    def save_completion_log(self, completed_items):
        """ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ë¥¼ JSON í˜•íƒœë¡œ S3ì— ì €ì¥ (ê°•í™”ëœ ìœ íš¨ì„± ê²€ì‚¬)"""
        try:
            log_key = f"{self.dirs['RESULTS']}completion_logs.json"
            logger.info(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥ ì‹œì‘ - ì…ë ¥ í•­ëª© ìˆ˜: {len(completed_items) if isinstance(completed_items, list) else 'None (ì…ë ¥ê°’ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜)'}")

            # ì…ë ¥ ë°ì´í„° ê²€ì¦ (ë¦¬ìŠ¤íŠ¸ ì—¬ë¶€)
            if not isinstance(completed_items, list):
                logger.error(f"completed_itemsëŠ” ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤. ì‹¤ì œ íƒ€ì…: {type(completed_items)}")
                return {"status": "error", "message": "ì˜ëª»ëœ ë°ì´í„° í˜•ì‹ (ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜)"}

            # numpy/pandas íƒ€ì…ì„ íŒŒì´ì¬ ê¸°ë³¸ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
            def convert_to_serializable(item):
                if not isinstance(item, dict):
                    # ì´ ê²½ìš°ëŠ” ì•„ë˜ì—ì„œ ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” None ë°˜í™˜
                    return None
                converted = {}
                for key, value in item.items():
                    if hasattr(value, 'item'):  # numpy type check
                        converted[key] = value.item()
                    elif pd.isna(value):  # null check
                        converted[key] = None
                    else:
                        converted[key] = value
                return converted

            # ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œë“œ
            existing_logs = []
            try:
                response = self.s3_client.get_object(Bucket=self.bucket, Key=log_key)
                loaded_content = response['Body'].read().decode('utf-8')
                if loaded_content: # íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ì—ë§Œ íŒŒì‹± ì‹œë„
                    existing_logs = json.loads(loaded_content)
                    if not isinstance(existing_logs, list):
                        logger.warning(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼({log_key})ì´ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹™ë‹ˆë‹¤ (íƒ€ì…: {type(existing_logs)}). ìƒˆë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                        existing_logs = []
                    else:
                        # ê¸°ì¡´ ë¡œê·¸ë„ ê²€ì¦ (ë”•ì…”ë„ˆë¦¬, í•„ìˆ˜ í‚¤)
                        valid_existing_logs = []
                        for i, log_item in enumerate(existing_logs):
                            if not isinstance(log_item, dict):
                                logger.warning(f"ê¸°ì¡´ ë¡œê·¸ {i}ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {log_item}")
                                continue
                            if not all(k in log_item for k in ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ']): # ë¬¼í’ˆì½”ë“œë„ í•„ìˆ˜ì ìœ¼ë¡œ ê²€ì‚¬
                                logger.warning(f"ê¸°ì¡´ ë¡œê·¸ {i}ë²ˆì§¸ í•­ëª©ì— í•„ìˆ˜ í‚¤(ë‚ ì§œ, ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ) ëˆ„ë½: {log_item}")
                                continue
                            valid_existing_logs.append(log_item)
                        existing_logs = valid_existing_logs
                        logger.info(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ë¡œë“œ ë° ê²€ì¦ ì™„ë£Œ - ìœ íš¨ í•­ëª© ìˆ˜: {len(existing_logs)}")
                else:
                    logger.info(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼({log_key})ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchKey':
                    logger.info(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼({log_key})ì´ ì—†ì–´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                else:
                    logger.warning(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘ S3 ì˜¤ë¥˜ ë°œìƒ({log_key}): {e}")
            except json.JSONDecodeError as e:
                logger.warning(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ íŒŒì‹± ì¤‘ JSON ì˜¤ë¥˜ ë°œìƒ({log_key}): {e}")
            except Exception as e:
                logger.warning(f"ê¸°ì¡´ ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜({log_key}): {e}")

            # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ í‚¤ ìƒì„± í•¨ìˆ˜
            def get_item_key(item):
                # ì´ í•¨ìˆ˜ëŠ” itemì´ dictë¼ê³  ê°€ì •í•˜ê³  í˜¸ì¶œë¨
                return f"{item.get('ë‚ ì§œ', '')}_{item.get('ë¶€ì„œëª…', '')}_{item.get('ë¬¼í’ˆì½”ë“œ', '')}"

            # ê¸°ì¡´ ë¡œê·¸ì—ì„œ í‚¤ ì§‘í•© ìƒì„± (ì´ë¯¸ ê²€ì¦ëœ ë¡œê·¸ ì‚¬ìš©)
            existing_keys = {get_item_key(item) for item in existing_logs}

            new_items_to_add = []
            invalid_item_count = 0

            for i, item in enumerate(completed_items):
                # 1. ê° í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
                if not isinstance(item, dict):
                    logger.warning(f"ìƒˆë¡œ ì¶”ê°€í•  {i}ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {item} (íƒ€ì…: {type(item)}). ê±´ë„ˆëœë‹ˆë‹¤.")
                    invalid_item_count += 1
                    continue

                # 2. í•„ìˆ˜ í‚¤ í™•ì¸ (ë‚ ì§œ, ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ) - ë¬¼í’ˆì½”ë“œ ëˆ„ë½ ì‹œì—ë„ ê±´ë„ˆëœ€
                required_keys = ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ']
                missing_keys = [key for key in required_keys if key not in item or pd.isna(item[key])] # NaN/Noneë„ ëˆ„ë½ìœ¼ë¡œ ê°„ì£¼
                if missing_keys:
                    logger.warning(f"ìƒˆë¡œ ì¶”ê°€í•  {i}ë²ˆì§¸ í•­ëª©ì— í•„ìˆ˜ í‚¤ {missing_keys} ëˆ„ë½ ë˜ëŠ” ê°’ ì—†ìŒ: {item}. ê±´ë„ˆëœë‹ˆë‹¤.")
                    invalid_item_count += 1
                    continue

                # 3. ë°ì´í„° íƒ€ì… ë³€í™˜ (ì›ë³¸ itemì— ëŒ€í•´)
                serializable_item = convert_to_serializable(item) # convert_to_serializableì€ ì´ë¯¸ itemì´ dictì„ì„ ê°€ì •
                if not serializable_item: # ë³€í™˜ ì‹¤íŒ¨ (ë‚´ë¶€ ë¡œì§ìƒ ê±°ì˜ ë°œìƒ ì•ˆí•¨)
                    logger.warning(f"ìƒˆë¡œ ì¶”ê°€í•  {i}ë²ˆì§¸ í•­ëª© ì§ë ¬í™” ì‹¤íŒ¨: {item}. ê±´ë„ˆëœë‹ˆë‹¤.")
                    invalid_item_count += 1
                    continue
                
                # ë‚ ì§œ í˜•ì‹ í‘œì¤€í™” (YYYY-MM-DD)
                try:
                    serializable_item['ë‚ ì§œ'] = pd.to_datetime(serializable_item['ë‚ ì§œ']).strftime('%Y-%m-%d')
                except Exception as e:
                    logger.warning(f"ìƒˆë¡œ ì¶”ê°€í•  {i}ë²ˆì§¸ í•­ëª©ì˜ ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì‹¤íŒ¨ ('{serializable_item.get('ë‚ ì§œ')}'): {e}. ê±´ë„ˆëœë‹ˆë‹¤.")
                    invalid_item_count += 1
                    continue

                # 4. ì¤‘ë³µ í™•ì¸ (ë³€í™˜ëœ serializable_item ê¸°ì¤€)
                item_key = get_item_key(serializable_item) # ì—¬ê¸°ì„œ serializable_itemì€ í•­ìƒ dict
                if item_key not in existing_keys:
                    new_items_to_add.append(serializable_item)
                    existing_keys.add(item_key)
                # else:
                #     logger.info(f"ìƒˆë¡œ ì¶”ê°€í•  {i}ë²ˆì§¸ í•­ëª©ì€ ì´ë¯¸ ì¡´ì¬: {item_key}") # ì¤‘ë³µì€ ë¡œê·¸ ì•ˆ ë‚¨ê¹€ (ë„ˆë¬´ ë§ì„ ìˆ˜ ìˆìŒ)

            logger.info(f"ìƒˆë¡œ ì¶”ê°€ë  í•­ëª© ìˆ˜: {len(new_items_to_add)}, ìœ íš¨í•˜ì§€ ì•Šì•„ ê±´ë„ˆë›´ í•­ëª© ìˆ˜: {invalid_item_count}")

            if not new_items_to_add and invalid_item_count == len(completed_items) and len(completed_items) > 0:
                logger.warning("ëª¨ë“  ì…ë ¥ í•­ëª©ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ì¶”ê°€í•  ìƒˆ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {"status": "no_valid_items", "message": "ìœ íš¨í•œ ë¡œê·¸ í•­ëª© ì—†ìŒ", "added_items": 0}

            if new_items_to_add:
                all_logs_to_save = existing_logs + new_items_to_add
                try:
                    # JSONìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
                    json_data = json.dumps(all_logs_to_save, ensure_ascii=False, indent=2)
                    self.s3_client.put_object(
                        Bucket=self.bucket,
                        Key=log_key,
                        Body=json_data.encode('utf-8')
                    )
                    logger.info(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥ ì„±ê³µ ({log_key}) - ì´ {len(all_logs_to_save)}ê°œ í•­ëª© ì €ì¥ (ìƒˆ í•­ëª© {len(new_items_to_add)}ê°œ ì¶”ê°€).")
                    return {"status": "success", "key": log_key, "added_items": len(new_items_to_add), "total_items": len(all_logs_to_save)}
                except Exception as e:
                    logger.error(f"S3 ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ({log_key}): {e}")
                    return {"status": "error", "message": f"S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {str(e)}"}
            else:
                logger.info(f"ì¶”ê°€í•  ìƒˆë¡œìš´ ìœ íš¨ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤ (ê¸°ì¡´ ë¡œê·¸ ìˆ˜: {len(existing_logs)}). ì €ì¥ ì‘ì—… ê±´ë„ˆëœë‹ˆë‹¤.")
                return {"status": "success", "key": log_key, "added_items": 0, "total_items": len(existing_logs), "message": "ìƒˆë¡œ ì¶”ê°€ëœ í•­ëª© ì—†ìŒ"}

        except Exception as e:
            logger.error(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return {"status": "error", "message": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"}

    def load_completion_logs(self):
        """ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ë¥¼ S3ì—ì„œ ë¡œë“œ (ê°•í™”ëœ ìœ íš¨ì„± ê²€ì‚¬)"""
        try:
            log_key = f"{self.dirs['RESULTS']}completion_logs.json"
            
            # S3ì—ì„œ íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
            try:
                response = self.s3_client.get_object(Bucket=self.bucket, Key=log_key)
                file_content = response['Body'].read().decode('utf-8')
            except ClientError as e:
                if e.response['Error']['Code'] == 'NoSuchKey':
                    logger.info(f"completion_logs.json íŒŒì¼({log_key})ì´ ì—†ìŠµë‹ˆë‹¤.")
                    return {"status": "not_found", "data": []}
                logger.error(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ S3ì—ì„œ ë¡œë“œ ì‹¤íŒ¨ ({log_key}): {e}")
                return {"status": "error", "data": [], "message": f"S3 ë¡œë“œ ì˜¤ë¥˜: {str(e)}"}
            except Exception as e:
                logger.error(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ S3ì—ì„œ ì½ëŠ” ì¤‘ ì˜ˆì™¸ ë°œìƒ ({log_key}): {e}", exc_info=True)
                return {"status": "error", "data": [], "message": f"S3 íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}"}

            # íŒŒì¼ ë‚´ìš©ì´ ë¹„ì–´ìˆëŠ” ê²½ìš°
            if not file_content:
                logger.info(f"completion_logs.json íŒŒì¼({log_key}) ë‚´ìš©ì€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return {"status": "success", "data": []} # ë¹ˆ íŒŒì¼ë„ ì„±ê³µìœ¼ë¡œ ê°„ì£¼, ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

            # JSON íŒŒì‹±
            try:
                logs = json.loads(file_content)
            except json.JSONDecodeError as e:
                logger.error(f"completion_logs.json íŒŒì¼({log_key}) JSON íŒŒì‹± ì˜¤ë¥˜: {e}. íŒŒì¼ ë‚´ìš© ì¼ë¶€: {file_content[:200]}")
                return {"status": "error", "data": [], "message": f"ì˜ëª»ëœ JSON í˜•ì‹: {str(e)}"}
            
            # ë°ì´í„° ê²€ì¦ ë° ì •ì œ (ë¦¬ìŠ¤íŠ¸ íƒ€ì… í™•ì¸)
            if not isinstance(logs, list):
                logger.warning(f"completion_logs.jsonì— ë¦¬ìŠ¤íŠ¸ ì´ì™¸ì˜ ìë£Œê°€ ìˆìŒ (íƒ€ì…: {type(logs)}). íŒŒì¼ ë‚´ìš©: {file_content[:200]}")
                return {"status": "error", "data": [], "message": "ì €ì¥ëœ ë¡œê·¸ê°€ ë¦¬ìŠ¤íŠ¸ê°€ ì•„ë‹˜"}
                
            # ê° í•­ëª© ê²€ì¦ (ë”•ì…”ë„ˆë¦¬, í•„ìˆ˜ í‚¤, ë‚ ì§œ í˜•ì‹)
            valid_logs = []
            invalid_count = 0
            for i, item in enumerate(logs):
                if not isinstance(item, dict):
                    logger.warning(f"ë¡œê·¸ í•­ëª© {i}ê°€ ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹˜: {item}")
                    invalid_count += 1
                    continue
                    
                required_keys = ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'] # ë¬¼í’ˆì½”ë“œë„ í•„ìˆ˜ë¡œ ê²€ì‚¬
                missing_keys = [key for key in required_keys if key not in item or pd.isna(item[key])]
                if missing_keys:
                    logger.warning(f"ë¡œê·¸ í•­ëª© {i}ì— í•„ìˆ˜ í‚¤ {missing_keys} ëˆ„ë½ ë˜ëŠ” ê°’ ì—†ìŒ: {item}")
                    invalid_count += 1
                    continue
                    
                # ë‚ ì§œ í˜•ì‹ ê²€ì¦ ë° í‘œì¤€í™” (YYYY-MM-DD)
                try:
                    # í‘œì¤€í™” ì‹œë„. ì‹¤íŒ¨í•˜ë©´ ê±´ë„ˆëœ€
                    item['ë‚ ì§œ'] = pd.to_datetime(item['ë‚ ì§œ']).strftime('%Y-%m-%d')
                except Exception as e:
                    logger.warning(f"ë¡œê·¸ í•­ëª© {i}ì˜ ë‚ ì§œ í˜•ì‹ ('{item.get('ë‚ ì§œ')}') ë³€í™˜ ì‹¤íŒ¨: {e}. ê±´ë„ˆëœë‹ˆë‹¤.")
                    invalid_count += 1
                    continue
                    
                valid_logs.append(item)
            
            if invalid_count > 0:
                logger.warning(f"ì´ {len(logs)}ê°œ ë¡œê·¸ ì¤‘ {invalid_count}ê°œ í•­ëª©ì´ ìœ íš¨í•˜ì§€ ì•Šì•„ ì œì™¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
            logger.info(f"ìœ íš¨í•œ ì™„ë£Œ ë¡œê·¸ {len(valid_logs)}ê°œ ë¡œë“œ ì™„ë£Œ ({log_key}).")
            return {"status": "success", "data": valid_logs}
            
        except Exception as e:
            logger.error(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            return {"status": "error", "data": [], "message": f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}"}


# --- ë‚ ì§œ í‘œì¤€í™” í•¨ìˆ˜ (streamlit_app.py ë‚´ì— ì§ì ‘ ì •ì˜) ---
def standardize_date(date_str):
    """ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë‚ ì§œ ë¬¸ìì—´ì„ YYYY-MM-DDë¡œ í‘œì¤€í™”í•©ë‹ˆë‹¤.
    
    ì…ë ¥:
    - date_str: ë‚ ì§œ ë¬¸ìì—´ (íŒŒì¼ëª…, ì‹œíŠ¸ëª… ë“±)
    
    ì²˜ë¦¬í•˜ëŠ” í˜•ì‹:
    1. YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD
    2. MM.DD, MM-DD, M.D, M-D
    3. íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ (ì˜ˆ: "ë¶ˆì¶œëŒ€ì¥_12.15.pdf" -> "12.15")
    
    ì¶œë ¥:
    - í‘œì¤€í™”ëœ ë‚ ì§œ (YYYY-MM-DD í˜•ì‹)
    - ë‚ ì§œê°€ ì•„ë‹Œ ê²½ìš° ì›ë³¸ ë°˜í™˜
    """
    now = datetime.now()
    year = now.year  # ê¸°ë³¸ ì—°ë„ëŠ” í˜„ì¬ ì—°ë„
    
    # íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ íŒ¨í„´ ì¶”ì¶œ ì‹œë„
    # íŒŒì¼ëª…ì—ì„œ MM.DD íŒ¨í„´ ì¶”ì¶œ
    file_date_match = re.search(r'(\d{1,2})[.-](\d{1,2})', str(date_str))
    if file_date_match:
        try:
            m, d = map(int, file_date_match.groups())
            # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ë‚ ì§œì¸ì§€ ê²€ì¦
            if 1 <= m <= 12 and 1 <= d <= 31:
                return datetime(year, m, d).strftime('%Y-%m-%d')
        except ValueError:
            pass

    # YYYY-MM-DD ë˜ëŠ” YYYY.MM.DD í˜•ì‹ í™•ì¸
    match_ymd = re.match(r'(\d{4})[.-]?(\d{1,2})[.-]?(\d{1,2})', str(date_str).strip())
    if match_ymd:
        try:
            y, m, d = map(int, match_ymd.groups())
            return datetime(y, m, d).strftime('%Y-%m-%d')
        except ValueError:
            pass  # ì˜ëª»ëœ ë‚ ì§œë©´ ë‹¤ìŒ íŒ¨í„´ ì‹œë„

    # MM.DD, MM-DD, M.D, M-D í˜•ì‹ í™•ì¸ (ë§ˆì¹¨í‘œ í¬í•¨)
    match_md = re.match(r'(\d{1,2})[.-](\d{1,2})\.?$', str(date_str).strip())
    if match_md:
        try:
            m, d = map(int, match_md.groups())
            # ì—°ë„ ì¶”ì • - í˜„ì¬ë³´ë‹¤ ë¯¸ë˜ ë‚ ì§œë©´ ì‘ë…„ìœ¼ë¡œ ì²˜ë¦¬
            date_with_current_year = datetime(year, m, d)
            if date_with_current_year > now and m > now.month:
                year -= 1
            return datetime(year, m, d).strftime('%Y-%m-%d')
        except ValueError:
            pass

    # ë‚ ì§œ í˜•ì‹ì„ ì¸ì‹í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ì›ë³¸ ë°˜í™˜
    logger.warning(f"ë‚ ì§œ í˜•ì‹ ì¸ì‹ ë¶ˆê°€: {date_str}")
    return str(date_str).strip()  # ì…ë ¥ê°’ì„ ë¬¸ìì—´ë¡œ ë°˜í™˜
# ----------------------------------------------------

# --- ì™„ë£Œ ì²˜ë¦¬ í•­ëª© í•„í„°ë§ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ---
def is_item_completed(item, completion_logs):
    """ì£¼ì–´ì§„ í•­ëª©ì´ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ì— ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    
    Args:
        item (dict): ë¶ˆì¼ì¹˜ ë°ì´í„° í•­ëª© (ë‚ ì§œ, ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ í¬í•¨)
        completion_logs (list): ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ëª©ë¡
    
    Returns:
        bool: ì™„ë£Œ ì²˜ë¦¬ ì—¬ë¶€
    """
    for log in completion_logs:
        if (str(log.get('ë‚ ì§œ')) == str(item.get('ë‚ ì§œ')) and
            str(log.get('ë¶€ì„œëª…')) == str(item.get('ë¶€ì„œëª…')) and
            str(log.get('ë¬¼í’ˆì½”ë“œ')) == str(item.get('ë¬¼í’ˆì½”ë“œ'))):
            return True
    return False

def filter_completed_items(mismatch_data, completion_logs, date_range=None):
    """ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©ì„ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        mismatch_data: ë¶ˆì¼ì¹˜ ë°ì´í„° DataFrame
        completion_logs: ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ë¦¬ìŠ¤íŠ¸
        date_range: (start_date, end_date) íŠœí”Œ, Noneì´ë©´ ì „ì²´ ê¸°ê°„
    """
    try:
        if mismatch_data.empty or not completion_logs:
            return mismatch_data
            
        # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§ì´ ì§€ì •ëœ ê²½ìš° ì™„ë£Œ ë¡œê·¸ë¥¼ ë¨¼ì € í•„í„°ë§
        filtered_completion_logs = completion_logs
        if date_range:
            start_date, end_date = date_range
            filtered_completion_logs = []
            for log in completion_logs:
                try:
                    log_date = pd.to_datetime(log.get('ë‚ ì§œ', '')).date()
                    if start_date <= log_date <= end_date:
                        filtered_completion_logs.append(log)
                except:
                    continue
            logger.info(f"ì™„ë£Œ ë¡œê·¸ ë‚ ì§œ í•„í„°ë§: {len(completion_logs)}ê°œ â†’ {len(filtered_completion_logs)}ê°œ (ê¸°ê°„: {start_date} ~ {end_date})")
            
        # ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©ì˜ ê³ ìœ  ì‹ë³„ì ìƒì„±
        completed_items = set()
        invalid_completion_logs = 0
        for log in filtered_completion_logs:
            try:
                date = str(log.get('ë‚ ì§œ', ''))
                dept = str(log.get('ë¶€ì„œëª…', ''))
                code = str(log.get('ë¬¼í’ˆì½”ë“œ', ''))
                if date and dept and code:
                    # ë‚ ì§œ í˜•ì‹ í†µì¼ (YYYY-MM-DD)
                    try:
                        date = pd.to_datetime(date).strftime('%Y-%m-%d')
                    except:
                        invalid_completion_logs += 1
                        continue
                    completed_key = f"{date}_{dept}_{code}"
                    completed_items.add(completed_key)
                else:
                    invalid_completion_logs += 1
            except Exception as e:
                logger.warning(f"ì™„ë£Œ í•­ëª© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                invalid_completion_logs += 1
                continue
        
        if invalid_completion_logs > 0:
            logger.warning(f"ìœ íš¨í•˜ì§€ ì•Šì€ ì™„ë£Œ ë¡œê·¸ {invalid_completion_logs}ê°œ ê±´ë„ˆëœ€")
        
        logger.info(f"ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§: ì™„ë£Œ í•­ëª© {len(completed_items)}ê°œ ìƒì„±ë¨")
        
        # ë””ë²„ê¹…: ì™„ë£Œ ë¡œê·¸ì™€ ë°ì´í„°ì˜ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        if filtered_completion_logs:
            completion_dates = [log.get('ë‚ ì§œ', '') for log in filtered_completion_logs]
            completion_date_range = f"{min(completion_dates)} ~ {max(completion_dates)}"
            logger.info(f"í•„í„°ë§ëœ ì™„ë£Œ ë¡œê·¸ ë‚ ì§œ ë²”ìœ„: {completion_date_range}")
        
        if not mismatch_data.empty and 'ë‚ ì§œ' in mismatch_data.columns:
            data_dates = mismatch_data['ë‚ ì§œ'].astype(str).unique()
            data_date_range = f"{min(data_dates)} ~ {max(data_dates)}"
            logger.info(f"ë°ì´í„° ë‚ ì§œ ë²”ìœ„: {data_date_range}")
        
        # ì™„ë£Œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ í•­ëª©ë§Œ í•„í„°ë§
        filtered_data = mismatch_data.copy()
        
        # ë‚ ì§œ í˜•ì‹ í™•ì¸ ë° ì²˜ë¦¬
        if pd.api.types.is_datetime64_any_dtype(filtered_data['ë‚ ì§œ']):
            # datetime íƒ€ì…ì¸ ê²½ìš° NaT ê°’ ì œê±° í›„ ë¬¸ìì—´ë¡œ ë³€í™˜
            before_nat_filter = len(filtered_data)
            filtered_data = filtered_data.dropna(subset=['ë‚ ì§œ'])
            after_nat_filter = len(filtered_data)
            
            if before_nat_filter != after_nat_filter:
                logger.warning(f"ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ì¤‘ ë‚ ì§œê°€ ì—†ëŠ” {before_nat_filter - after_nat_filter}ê°œ í•­ëª© ì œì™¸")
            
            if filtered_data.empty:
                logger.warning("ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return filtered_data
            
            # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            filtered_data['ë‚ ì§œ_str'] = filtered_data['ë‚ ì§œ'].dt.strftime('%Y-%m-%d')
        else:
            # ì´ë¯¸ ë¬¸ìì—´ íƒ€ì…ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
            filtered_data['ë‚ ì§œ_str'] = filtered_data['ë‚ ì§œ'].astype(str)
            
            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ 'NaT' ë¬¸ìì—´ ì œê±°
            before_filter = len(filtered_data)
            filtered_data = filtered_data[
                (filtered_data['ë‚ ì§œ_str'] != '') & 
                (filtered_data['ë‚ ì§œ_str'] != 'NaT') & 
                (filtered_data['ë‚ ì§œ_str'].notna())
            ]
            after_filter = len(filtered_data)
            
            if before_filter != after_filter:
                logger.warning(f"ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ì¤‘ ìœ íš¨í•˜ì§€ ì•Šì€ ë‚ ì§œ {before_filter - after_filter}ê°œ í•­ëª© ì œì™¸")
            
            if filtered_data.empty:
                logger.warning("ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ í›„ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return filtered_data
        
        # ì•ˆì „í•œ item_key ìƒì„± (ë¬¸ìì—´ ë‚ ì§œ ì‚¬ìš©)
        filtered_data['item_key'] = filtered_data.apply(
            lambda row: f"{row['ë‚ ì§œ_str']}_{str(row['ë¶€ì„œëª…'])}_{str(row['ë¬¼í’ˆì½”ë“œ'])}", 
            axis=1
        )
        
        # ì™„ë£Œëœ í•­ëª© ì œì™¸
        before_completion_filter = len(filtered_data)
        filtered_data = filtered_data[~filtered_data['item_key'].isin(completed_items)]
        after_completion_filter = len(filtered_data)
        
        logger.info(f"ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ê²°ê³¼: {before_completion_filter}ê°œ â†’ {after_completion_filter}ê°œ (ì œì™¸ëœ í•­ëª©: {before_completion_filter - after_completion_filter}ê°œ)")
        
        # ë””ë²„ê¹…: ì²« 5ê°œ í•­ëª©ì˜ í‚¤ ë¹„êµ
        if before_completion_filter > 0:
            sample_keys = filtered_data['item_key'].head(5).tolist()
            logger.info(f"ìƒ˜í”Œ ë°ì´í„° í‚¤: {sample_keys}")
            sample_completed = list(completed_items)[:5]
            logger.info(f"ìƒ˜í”Œ ì™„ë£Œ í‚¤: {sample_completed}")
        
        # ì„ì‹œ ì»¬ëŸ¼ ì œê±°
        filtered_data = filtered_data.drop(['item_key', 'ë‚ ì§œ_str'], axis=1)
        
        return filtered_data
        
    except Exception as e:
        logger.error(f"ì™„ë£Œ í•­ëª© í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        return mismatch_data

# ì‚­ì œëœ ì¤‘ë³µ í•¨ìˆ˜
# ----------------------------------------------------

# í•œê¸€ í°íŠ¸ ì„¤ì • í•¨ìˆ˜
def set_korean_font():
    try:
        # ì‹œìŠ¤í…œ í°íŠ¸ ê²€ìƒ‰
        font_path = None
        font_files = fm.findSystemFonts(fontpaths=None, fontext='ttf')
        
        # Windows: Malgun Gothic
        if os.name == 'nt':
            for fpath in font_files:
                if 'malgun' in fpath.lower():
                    font_path = fpath
                    break
        # macOS: AppleGothic
        elif os.name == 'posix':
            for fpath in font_files:
                if 'applegothic' in fpath.lower():
                    font_path = fpath
                    break
        # Linux: NanumGothic (ì„¤ì¹˜ í•„ìš”)
        else:
            for fpath in font_files:
                if 'nanumgothic' in fpath.lower():
                    font_path = fpath
                    break

        if font_path:
            plt.rc('font', family=fm.FontProperties(fname=font_path).get_name())
            plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
        else:
            logger.warning("ì ì ˆí•œ í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì— í°íŠ¸ ì„¤ì¹˜ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")
            # ê¸°ë³¸ í°íŠ¸ë¡œ ì§„í–‰
            plt.rc('font', family='sans-serif')
            plt.rcParams['axes.unicode_minus'] = False
            
    except Exception as e:
        logger.error(f"í•œê¸€ í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ í°íŠ¸ë¡œ ì§„í–‰
        plt.rc('font', family='sans-serif')
        plt.rcParams['axes.unicode_minus'] = False


# ì•± ìŠ¤íƒ€ì¼
st.markdown("""
<style>
    /* ì½˜í…ì¸  ì˜ì—­ ê¸°ë³¸ í…ìŠ¤íŠ¸ í¬ê¸° */
    .main .block-container div[data-testid=\"stMarkdownContainer\"],
    .main .block-container div[data-testid=\"stText\"] {
        font-size: 18px !important;
    }
    /* ë²„íŠ¼ í°íŠ¸ í¬ê¸° */
    button {
        font-size: 17px !important; /* ë²„íŠ¼ì€ ì•½ê°„ ì‘ê²Œ */
    }
    /* st.dataframe ë‚´ë¶€ í…Œì´ë¸” í°íŠ¸ í¬ê¸° */
    .stDataFrame table th,
    .stDataFrame table td {
        font-size: 18px !important; /* ë‹¤ë¥¸ ì½˜í…ì¸ ì™€ ë™ì¼í•˜ê²Œ */
    }
    h1, h2, h3, h4, h5, h6 {
        /* í—¤ë” í°íŠ¸ í¬ê¸°ëŠ” ê¸°ë³¸ê°’ì„ ìœ ì§€í•˜ê±°ë‚˜ í•„ìš”ì‹œ ë³„ë„ ì¡°ì • */
    }
    .main .block-container {
        padding-top: 2rem;
        padding-bottom: 2rem;
    }
    .stProgress .st-eb {
        background-color: #4CAF50;
    }
    .stTabs [data-baseweb=\"tab-list\"] {
        gap: 1px;
    }
    .stTabs [data-baseweb=\"tab\"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: #F0F2F6;
        border-radius: 4px 4px 0 0;
        padding-left: 1rem;
        padding-right: 1rem;
        /* íƒ­ ì œëª© í¬ê¸° ëª…ì‹œì  ì„¤ì • */
        font-size: 17px !important; /* íƒ­ ì œëª© í¬ê¸° ìœ ì§€ */
    }
    .stTabs [aria-selected=\"true\"] {
        background-color: #E0E0E0;
    }
    .department-card {
        padding: 1rem;
        border-radius: 0.5rem;
        background-color: #f8f9fa;
        margin-bottom: 0.5rem;
    }
    .mismatch-badge {
        background-color: #ff4b4b;
        color: white;
        padding: 0.2rem 0.5rem;
        border-radius: 1rem;
        font-size: 0.8rem;
        margin-left: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ìˆ˜ì • ë° ì¶”ê°€)
if 'ocr_results_by_date' not in st.session_state:
    st.session_state.ocr_results_by_date = {} # ë‚ ì§œë³„ OCR ê²°ê³¼ ì €ì¥
if 'pdf_paths_by_date' not in st.session_state:
    st.session_state.pdf_paths_by_date = {} # ë‚ ì§œë³„ ì›ë³¸ PDF ê²½ë¡œ ì €ì¥
if 'processed_pdfs_by_date' not in st.session_state:
    st.session_state.processed_pdfs_by_date = {} # ë‚ ì§œë³„ ì²˜ë¦¬ëœ PDF ê²½ë¡œ ì €ì¥ (fitz ê°ì²´ ëŒ€ì‹  ê²½ë¡œ ì €ì¥ ê¶Œì¥)
if 'dept_page_tuples_by_date' not in st.session_state:
    st.session_state.dept_page_tuples_by_date = {} # ë‚ ì§œë³„ ë¶€ì„œ-í˜ì´ì§€ íŠœí”Œ ëª©ë¡ ì €ì¥

if 'excel_dates' not in st.session_state:
    st.session_state.excel_dates = [] # ì›ë³¸ ì—‘ì…€ ë‚ ì§œ (ì‹œíŠ¸ëª…) - í˜„ì¬ ì‚¬ìš© ì•ˆí•¨
if 'standardized_excel_dates' not in st.session_state:
    st.session_state.standardized_excel_dates = [] # í‘œì¤€í™”ëœ ì—‘ì…€ ë‚ ì§œ
if 'pdf_dates' not in st.session_state:
    st.session_state.pdf_dates = []        # í‘œì¤€í™”ëœ PDF ë‚ ì§œ
if 'available_dates' not in st.session_state:
     st.session_state.available_dates = [] # í†µí•© ë‚ ì§œ ëª©ë¡
if 'selected_date' not in st.session_state:
    st.session_state.selected_date = None
if 'item_db' not in st.session_state:
    st.session_state.item_db = {}  # ë¬¼í’ˆ ì½”ë“œ-ì´ë¦„ ë§¤í•‘ DB
if 'excel_data' not in st.session_state:
    st.session_state.excel_data = pd.DataFrame()  # ì—‘ì…€ ë°ì´í„°
if 'mismatch_data' not in st.session_state:
    st.session_state.mismatch_data = pd.DataFrame()  # ë¶ˆì¼ì¹˜ ë°ì´í„°
if 'missing_items' not in st.session_state:
    st.session_state.missing_items = pd.DataFrame()  # ëˆ„ë½ í’ˆëª© ë°ì´í„°
if 'receipt_status' not in st.session_state:
    st.session_state.receipt_status = {} # ë‚ ì§œ-ë¶€ì„œë³„ ì¸ìˆ˜ì¦ ìƒíƒœ ì €ì¥ ('ì¸ìˆ˜ì¦ ì—†ìŒ')
if 'missing_receipt_info' not in st.session_state:
    st.session_state.missing_receipt_info = {}  # ë¶€ì„œë³„ ë‚ ì§œ ëª©ë¡ì„ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬
# ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©ì„ ì„¸ì…˜ì— ì €ì¥í•˜ëŠ” ë³€ìˆ˜ ì¶”ê°€
if 'completion_logs' not in st.session_state:
    st.session_state.completion_logs = []  # ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸


# PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ í•¨ìˆ˜
@st.cache_data(ttl=3600, max_entries=100)
def extract_pdf_preview(pdf_path_or_bytes, page_num=0, dpi=120, thumbnail_size=(400, 600)):
    """
    PDF íŒŒì¼ì˜ íŠ¹ì • í˜ì´ì§€ë¥¼ ì¸ë„¤ì¼(ë¯¸ë¦¬ë³´ê¸°) ì´ë¯¸ì§€ë¡œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜ (PIL.Image)
    Args:
        pdf_path_or_bytes: PDF íŒŒì¼ ê²½ë¡œ(str) ë˜ëŠ” bytes (io.BytesIO ê°€ëŠ¥)
        page_num: í˜ì´ì§€ ë²ˆí˜¸ (0ë¶€í„° ì‹œì‘)
        dpi: ì›ë³¸ ì´ë¯¸ì§€ í•´ìƒë„ (ë‚®ìœ¼ë©´ ì†ë„/ìš©ëŸ‰â†“)
        thumbnail_size: (width, height) ìµœëŒ€ í¬ê¸°(ë¹„ìœ¨ìœ ì§€)
    """
    try:
        # íŒŒì¼ ê²½ë¡œ ë˜ëŠ” ë°”ì´íŠ¸/ë²„í¼ êµ¬ë¶„
        if isinstance(pdf_path_or_bytes, str):
            doc = fitz.open(pdf_path_or_bytes)
        else:
            # io.BytesIO ë˜ëŠ” bytes
            if isinstance(pdf_path_or_bytes, bytes):
                pdf_path_or_bytes = io.BytesIO(pdf_path_or_bytes)
            doc = fitz.open(stream=pdf_path_or_bytes, filetype="pdf")

        if not doc or page_num < 0 or page_num >= len(doc):
            return None
        
        page = doc.load_page(page_num)
        zoom = dpi / 72  # DPI ì„¤ì •
        mat = fitz.Matrix(zoom, zoom)
        pix = page.get_pixmap(matrix=mat)

        img = Image.frombytes("RGB", [pix.width, pix.height], pix.samples)
        # --- ì¸ë„¤ì¼ ë³€í™˜ ---
        img.thumbnail(thumbnail_size)

        doc.close()
        return img
    except Exception as e:
        logger.error(f"PDF ë¯¸ë¦¬ë³´ê¸° ìƒì„± ì˜¤ë¥˜ ({pdf_path_or_bytes}, í˜ì´ì§€ {page_num}): {e}")
        return None


# íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„°ë¥¼ S3ì—ì„œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
@st.cache_data(ttl=3600) # ìºì‹œ ì¶”ê°€: 1ì‹œê°„ ë™ì•ˆ ê²°ê³¼ ìœ ì§€
def load_data_for_date(date_str):
    """íŠ¹ì • ë‚ ì§œì˜ ë©”íƒ€ë°ì´í„°, PDF ê²½ë¡œ, OCR ê²°ê³¼ ë“±ì„ S3ì—ì„œ ë¡œë“œí•˜ì—¬ ì„¸ì…˜ ìƒíƒœì— ì €ì¥""" 
    s3_handler = S3Handler()
    data_loaded = False
    metadata = None # ë©”íƒ€ë°ì´í„° ë³€ìˆ˜ ì´ˆê¸°í™”
    
    # 1. ë©”íƒ€ë°ì´í„° ë¡œë“œ (PDF, OCR ê²°ê³¼ ë“±)
    metadata_result = s3_handler.load_metadata(date_str)
    if metadata_result["status"] == "success":
        metadata = metadata_result["data"]
        # PDF í‚¤ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        if "pdf_key" in metadata:
            st.session_state.pdf_paths_by_date[date_str] = metadata["pdf_key"]

        else:
            logger.warning(f"****** DEBUG: ë©”íƒ€ë°ì´í„°ì— PDF í‚¤ ì—†ìŒ")
        
        # ë¶€ì„œ-í˜ì´ì§€ íŠœí”Œ ëª©ë¡ ë¡œë“œ
        if "departments_with_pages" in metadata:
            dept_page_tuples = metadata["departments_with_pages"]
            st.session_state.dept_page_tuples_by_date[date_str] = dept_page_tuples
            st.session_state.departments_with_pages_by_date[date_str] = dept_page_tuples
            logger.info(f"****** DEBUG: ë‚ ì§œ {date_str}ì˜ ë¶€ì„œ-í˜ì´ì§€ ì •ë³´ ë¡œë“œ ì„±ê³µ: {len(dept_page_tuples)}ê°œ í•­ëª©")
        else:
            dept_page_tuples = [] # ì—†ì„ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”
            logger.warning(f"****** DEBUG: ë©”íƒ€ë°ì´í„°ì— ë¶€ì„œ-í˜ì´ì§€ ì •ë³´ ì—†ìŒ")
        
        # OCR ê²°ê³¼ ë¡œë“œ
        ocr_text_result = s3_handler.load_ocr_text(date_str)
        if ocr_text_result["status"] == "success":
            ocr_text_list = ocr_text_result["data"]
            ocr_result = {
                "status": "success",
                "ocr_text": ocr_text_list,
                "departments_with_pages": dept_page_tuples
            }
            st.session_state.ocr_results_by_date[date_str] = ocr_result

            
            # ë¶€ì„œë³„ OCR ì½”ë“œ ì§‘ê³„ í›„ ì„¸ì…˜ì— ì €ì¥
            logger.debug(f"****** DEBUG: ë¶€ì„œë³„ OCR ì½”ë“œ ì§‘ê³„ ì‹œì‘ (í˜ì´ì§€ íŠœí”Œ ìˆ˜: {len(dept_page_tuples)})")
            try:
                codes_map = data_analyzer.aggregate_ocr_results_by_department(
                    ocr_text_list, dept_page_tuples
                )
                logger.debug(f"****** DEBUG: OCR ì½”ë“œ ì§‘ê³„ ì‹œë„ ê²°ê³¼: {codes_map.get('status', 'N/A')}")
                if codes_map.get('status') == 'success':
                    if 'aggregated_ocr_items_by_date' not in st.session_state:
                        st.session_state['aggregated_ocr_items_by_date'] = {}
                    items_by_dept = {dept: data['items'] for dept, data in codes_map.get('data', {}).items()}
                    st.session_state['aggregated_ocr_items_by_date'][date_str] = items_by_dept
                    logger.debug(f"****** DEBUG: ë¶€ì„œë³„ OCR ì½”ë“œ ì§‘ê³„ ì €ì¥ ì„±ê³µ: {len(items_by_dept)}ê°œ ë¶€ì„œ")
                else:
                    logger.error(f"****** DEBUG: ë¶€ì„œë³„ OCR ì½”ë“œ ì§‘ê³„ ì‹¤íŒ¨: {codes_map.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
            except Exception as agg_e:
                logger.error(f"****** DEBUG: ë¶€ì„œë³„ OCR ì½”ë“œ ì§‘ê³„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {agg_e}", exc_info=True)
        else:
            logger.warning(f"****** DEBUG: OCR í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")

        data_loaded = True # ë©”íƒ€ë°ì´í„° ë¡œë“œ ì„±ê³µ ì‹œ Trueë¡œ ì„¤ì •
        logger.debug(f"****** DEBUG: ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ë¡œë“œ ì„±ê³µ")
        
        # PDF ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹œë„ (S3ì—ì„œ)
        if "pdf_key" in metadata:
            logger.debug(f"****** DEBUG: PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„ (í‚¤: {metadata['pdf_key']})")
            try:
                pdf_result = s3_handler.download_file(metadata["pdf_key"])
                logger.debug(f"****** DEBUG: PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²°ê³¼: {pdf_result['status']}")
                if pdf_result["status"] == "success":
                    # ê²½ë¡œê°€ ì´ë¯¸ ì €ì¥ë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸ ë¶ˆí•„ìš” (ìœ„ì—ì„œ ì´ë¯¸ ì €ì¥ë¨)
                    logger.debug(f"****** DEBUG: PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë³„ë„ ì²˜ë¦¬ ì—†ìŒ (ê²½ê³ ë§Œ ë¡œê¹…ë¨)
            except Exception as pdf_download_e:
                logger.error(f"****** DEBUG: PDF íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {pdf_download_e}", exc_info=True)
    else:
        logger.warning(f"****** DEBUG: ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ë˜ëŠ” ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    # 3. ì—‘ì…€ ë°ì´í„° ë¡œë“œ (ì„¸ì…˜ì— ì—†ê±°ë‚˜ ë¹„ì–´ìˆëŠ” ê²½ìš°)
    if 'excel_data' not in st.session_state or st.session_state.excel_data is None or st.session_state.excel_data.empty:
        logger.debug(f"****** DEBUG: ì„¸ì…˜ì— ì—‘ì…€ ë°ì´í„° ì—†ìŒ. ë©”íƒ€ë°ì´í„°ì—ì„œ ë¡œë“œ ì‹œë„")
        # ë©”íƒ€ë°ì´í„°ê°€ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆê³ , excel_keyê°€ ìˆëŠ”ì§€ í™•ì¸
        if metadata and "excel_key" in metadata:
            excel_key = metadata["excel_key"]
            logger.debug(f"****** DEBUG: ë©”íƒ€ë°ì´í„°ì—ì„œ ì—‘ì…€ í‚¤ '{excel_key}' ë°œê²¬. ë‹¤ìš´ë¡œë“œ ì‹œë„")
            excel_result = s3_handler.download_file(excel_key)
            logger.debug(f"****** DEBUG: ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ê²°ê³¼: {excel_result['status']}")
            if excel_result["status"] == "success":
                try:
                    excel_buffer_pd = io.BytesIO(excel_result["data"])
                    excel_buffer_pd.seek(0)
                    is_cumulative = "latest/cumulative_excel.xlsx" in excel_key
                    logger.debug(f"****** DEBUG: data_analyzer.load_excel_data í˜¸ì¶œ (ëˆ„ì : {is_cumulative})")
                    excel_data_result = data_analyzer.load_excel_data(excel_buffer_pd, is_cumulative_flag=is_cumulative)
                    logger.debug(f"****** DEBUG: load_excel_data ê²°ê³¼: {excel_data_result['status']}")
                    if excel_data_result["status"] == "success":
                        st.session_state.excel_data = excel_data_result["data"]
                        st.session_state.standardized_excel_dates = sorted(
                            st.session_state.excel_data['ë‚ ì§œ'].astype(str).unique()
                        )
                        logger.debug(f"****** DEBUG: S3ì—ì„œ ì—‘ì…€ ë°ì´í„° ë¡œë“œ ë° íŒŒì‹± ì„±ê³µ ({len(st.session_state.excel_data)} í–‰)")
                        data_loaded = True # ì—‘ì…€ ë¡œë“œ ì„±ê³µ ì‹œ True ë³´ì¥
                    else:
                        logger.error(f"****** DEBUG: ì—‘ì…€ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {excel_data_result.get('message', 'N/A')}")
                except Exception as excel_proc_e:
                    logger.error(f"****** DEBUG: ì—‘ì…€ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {excel_proc_e}", exc_info=True)
            else:
                logger.error(f"****** DEBUG: S3 ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {excel_result.get('message', 'N/A')}")
        else:
            logger.warning(f"****** DEBUG: ë©”íƒ€ë°ì´í„°ê°€ ì—†ê±°ë‚˜ 'excel_key'ê°€ ì—†ì–´ S3 ì—‘ì…€ ë¡œë“œ ë¶ˆê°€")
    else:
        logger.debug(f"****** DEBUG: ì„¸ì…˜ì— ì´ë¯¸ ì—‘ì…€ ë°ì´í„° ì¡´ì¬")
        data_loaded = True # ì„¸ì…˜ì— ì´ë¯¸ ìˆìœ¼ë©´ ë¡œë“œëœ ê²ƒìœ¼ë¡œ ê°„ì£¼

    logger.debug(f"****** DEBUG: load_data_for_date ì¢…ë£Œ (ìµœì¢… data_loaded: {data_loaded})")
    # ë°˜í™˜ í˜•ì‹ ë³€ê²½: ë¶ˆë¦¬ì–¸ ëŒ€ì‹  ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    if data_loaded:
        # ì—‘ì…€ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ ë‹¤ì‹œ í™•ì¸ í›„ ë°˜í™˜
        final_excel_data = st.session_state.get('excel_data', pd.DataFrame())
        return {"status": "success", "data": final_excel_data}
    else:
        return {"status": "error", "message": f"ë‚ ì§œ {date_str}ì˜ ë°ì´í„°ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}

# PDF íŒŒì¼ì„ í‘œì‹œí•˜ëŠ” í•¨ìˆ˜
def display_pdf(file_path):
    try:
        with open(file_path, "rb") as f:
            base64_pdf = base64.b64encode(f.read()).decode('utf-8')
        
        pdf_display = f'<iframe src="data:application/pdf;base64,{base64_pdf}" width="100%" height="600" type="application/pdf"></iframe>'
        st.markdown(pdf_display, unsafe_allow_html=True)
    except Exception as e:
        st.error(f"PDF í‘œì‹œ ì˜¤ë¥˜: {e}")

# PDF OCR ì§„í–‰ë¥  ì½œë°±
def progress_callback(current, total):
    progress_bar = st.session_state.get('progress_bar')
    if progress_bar is not None:
        progress_bar.progress(current / total)
        
        # í˜„ì¬ ì²˜ë¦¬ ì¤‘ì¸ í˜ì´ì§€ ì •ë³´ ì—…ë°ì´íŠ¸
        progress_text = st.session_state.get('progress_text')
        if progress_text is not None:
            progress_text.text(f"í˜ì´ì§€ ì²˜ë¦¬ ì¤‘... {current}/{total}")



# --- S3 ì—°ê²° í™•ì¸ í•¨ìˆ˜ --- 
def check_s3_connection():
    """S3 ì—°ê²° ìƒíƒœ í™•ì¸"""
    try:
        s3_client = get_s3_client()
        if s3_client is None:
            return False
        
        # ë²„í‚· ì ‘ê·¼ í…ŒìŠ¤íŠ¸
        s3_client.head_bucket(Bucket=S3_BUCKET)
        logger.info(f"S3 ë²„í‚· '{S3_BUCKET}' ì—°ê²° ì„±ê³µ")
        return True
    except Exception as e:
        logger.error(f"S3 ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
        return False

# --- ë¬¼í’ˆ DB ë¡œë“œ í•¨ìˆ˜ (S3) --- 
@st.cache_data(ttl=3600)  # 1ì‹œê°„ ìºì‹œ
def load_item_db_from_s3():
    """S3ì—ì„œ ë¬¼í’ˆ DB íŒŒì¼ ë¡œë“œ"""
    try:
        s3_client = get_s3_client()
        db_key = f"{S3_DIRS['DB']}db.xlsx"
        
        try:
            # S3ì—ì„œ DB íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
            response = s3_client.get_object(Bucket=S3_BUCKET, Key=db_key)
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_file:
                temp_file.write(response['Body'].read())
                temp_path = temp_file.name
            
            # DB ë¡œë“œ
            item_db = data_analyzer.load_item_db(temp_path)
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(temp_path)
            
            logger.info("S3ì—ì„œ ë¬¼í’ˆ DB íŒŒì¼ ë¡œë“œ ì„±ê³µ")
            return item_db
            
        except ClientError as e:
            if e.response['Error']['Code'] == 'NoSuchKey':
                logger.warning("S3ì— ë¬¼í’ˆ DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None
            raise e
            
    except Exception as e:
        logger.error(f"ë¬¼í’ˆ DB ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None

# --- ë¬¼í’ˆ DB ì—…ë¡œë“œ í•¨ìˆ˜ (S3) --- 
def upload_db_to_s3(file):
    """ë¬¼í’ˆ DB íŒŒì¼ì„ S3ì— ì—…ë¡œë“œ"""
    try:
        s3_client = get_s3_client()
        db_key = f"{S3_DIRS['DB']}db.xlsx"
        
        # íŒŒì¼ ì—…ë¡œë“œ
        s3_client.upload_fileobj(file, S3_BUCKET, db_key)
        logger.info("ë¬¼í’ˆ DB íŒŒì¼ S3 ì—…ë¡œë“œ ì„±ê³µ")
        return True
        
    except Exception as e:
        logger.error(f"ë¬¼í’ˆ DB íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

# pdf ì²˜ë¦¬ í•µì‹¬ í•¨ìˆ˜
def display_pdf_section(selected_date, sel_dept, tab_prefix="pdf_tab"):
    """
    ë¶€ì„œë³„ PDF ì„¹ì…˜: ëª¨ë“  í˜ì´ì§€ ì¸ë„¤ì¼ì„ í•œ ë²ˆì— í‘œì‹œ, ì²´í¬ë°•ìŠ¤ë¡œ ì„ íƒ, ì„ íƒí•œ ì´ë¯¸ì§€ë§Œ S3+ì—‘ì…€ ì €ì¥
    """
    try:
        # S3ì—ì„œ PDF ì›ë³¸ ë‹¤ìš´ë¡œë“œ
        s3_handler = S3Handler()
        pdf_key = st.session_state.pdf_paths_by_date.get(selected_date)
        if not pdf_key:
            st.warning(f"ì„ íƒëœ ë‚ ì§œ({selected_date})ì˜ PDF íŒŒì¼ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        pdf_result = s3_handler.download_file(pdf_key)
        if pdf_result["status"] != "success":
            st.error("PDF ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨.")
            return
        pdf_bytes = pdf_result["data"]
        dept_pages = get_department_pages(selected_date, sel_dept)
        if not dept_pages:
            st.info(f"'{sel_dept}' ë¶€ì„œì˜ PDF í˜ì´ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        st.subheader(f"{selected_date} {sel_dept} ë¯¸ë¦¬ë³´ê¸° (ì¸ë„¤ì¼, ë‹¤ì¤‘ ì„ íƒ)")
        
        # Formì„ ì‚¬ìš©í•˜ì—¬ ì²´í¬ë°•ìŠ¤ ìƒíƒœ ë³€ê²½ ì‹œ ìƒˆë¡œê³ ì¹¨ ë°©ì§€
        with st.form(key=f"{tab_prefix}_image_selection_form"):
            cols = st.columns(2)
            page_checkbox_keys = []
            page_img_objs = []

            for idx, page_num in enumerate(sorted(dept_pages)):
                with cols[idx % 2]:
                    img = extract_pdf_preview(io.BytesIO(pdf_bytes), page_num-1, dpi=120, thumbnail_size=(700, 1000))
                    if img is not None:
                        st.image(img, caption=f"p.{page_num}", width=650)
                        cb_key = f"{tab_prefix}_{selected_date}_{sel_dept}_{page_num}"
                        
                        # ì²´í¬ë°•ìŠ¤ í‘œì‹œ (Form ë‚´ë¶€ì—ì„œ ìƒˆë¡œê³ ì¹¨ ì—†ì´ ë™ì‘)
                        checked = st.checkbox(
                            f"í˜ì´ì§€ {page_num} ì„ íƒ",
                            key=cb_key,
                            value=False
                        )

                        page_checkbox_keys.append((cb_key, page_num, img, checked))

            # Form ì œì¶œ ë²„íŠ¼
            submitted = st.form_submit_button("ì„ íƒí•œ ì´ë¯¸ì§€ë¥¼ ì—‘ì…€ë¡œ ì €ì¥")
            
            if submitted:
                # --- ì„ íƒëœ ì´ë¯¸ì§€ë§Œ ì¶”ë¦¼ (Form ì œì¶œ ì‹œì ì˜ ìƒíƒœ ì‚¬ìš©) ---
                selected_imgs = []
                checkbox_status = {}
                
                for cb_key, pg, img, is_checked in page_checkbox_keys:
                    checkbox_status[f"í˜ì´ì§€ {pg}"] = is_checked
                    if is_checked:
                        selected_imgs.append((pg, img))

                # ë””ë²„ê¹… ì •ë³´ í‘œì‹œ
                st.write(f"**ì„ íƒ ê²°ê³¼:**")
                st.write(f"- ì´ í˜ì´ì§€ ìˆ˜: {len(page_checkbox_keys)}")
                st.write(f"- ì„ íƒëœ ì´ë¯¸ì§€ ìˆ˜: {len(selected_imgs)}")
                st.write(f"- ì²´í¬ë°•ìŠ¤ ìƒíƒœ: {checkbox_status}")

                if not selected_imgs:
                    st.warning("ì €ì¥í•  ì´ë¯¸ì§€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”. ì²´í¬ë°•ìŠ¤ë¥¼ í´ë¦­í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•œ í›„ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                else:
                    saved_count = 0
                    error_count = 0
                    
                    with st.spinner(f"{len(selected_imgs)}ê°œ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•˜ëŠ” ì¤‘..."):
                        for page_num, img_obj in selected_imgs:
                            try:
                                save_result = s3_handler.save_pdf_preview_image(
                                    selected_date, sel_dept, page_num, img_obj
                                )
                                if save_result.get("status") == "success":
                                    saved_count += 1
                                    logger.info(f"ì´ë¯¸ì§€ ì €ì¥ ì„±ê³µ: {sel_dept} í˜ì´ì§€ {page_num}")
                                else:
                                    error_count += 1
                                    logger.error(f"ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {sel_dept} í˜ì´ì§€ {page_num} - {save_result.get('message')}")
                            except Exception as e:
                                error_count += 1
                                logger.error(f"ì´ë¯¸ì§€ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {sel_dept} í˜ì´ì§€ {page_num} - {e}")
                    
                    # ê²°ê³¼ ë©”ì‹œì§€
                    if saved_count > 0:
                        st.success(f"âœ… {saved_count}ê°œ ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ë¥¼ S3ì— ì €ì¥ ì™„ë£Œ! ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì‹œ ìë™ ì‚½ì…ë©ë‹ˆë‹¤.")
                        if error_count > 0:
                            st.warning(f"âš ï¸ {error_count}ê°œ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨")
                    else:
                        if error_count > 0:
                            st.error(f"âŒ ëª¨ë“  ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨ ({error_count}ê°œ)")
                        else:
                            st.warning("ì €ì¥í•  ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì²´í¬ë°•ìŠ¤ë¥¼ ì„ íƒí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

    except Exception as e:
        logger.error(f"PDF ì„¹ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        st.error("PDF ì„¹ì…˜ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


# --- í—¤ë” í–‰ ì°¾ê¸° í•¨ìˆ˜ --- 
def find_header_row(df):
    """
    1í–‰ë¶€í„° 10í–‰ ì‚¬ì´ì—ì„œ ë¶€ì„œëª…, ë¬¼í’ˆì½”ë“œ, ì²­êµ¬ëŸ‰, ìˆ˜ë ¹ëŸ‰ì´ ëª¨ë‘ í¬í•¨ëœ í–‰ì„ ì°¾ì•„ í—¤ë” í–‰ìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # í™•ì¸í•  í‚¤ì›Œë“œë“¤ (ë¬¼í’ˆì½”ë“œ ì¶”ê°€)
    keywords = ['ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ', 'ì²­êµ¬ëŸ‰', 'ìˆ˜ë ¹ëŸ‰']

    # ì²˜ìŒ 10í–‰ë§Œ ê²€ì‚¬ (ë” ì ì€ í–‰ì„ ê°€ì§„ ê²½ìš° ëª¨ë“  í–‰ ê²€ì‚¬)
    max_rows = min(10, len(df))

    for i in range(max_rows):
        try:
            row = df.iloc[i].astype(str)
            row_values = [str(val).lower().strip() for val in row.values] # ê³µë°± ì œê±° ì¶”ê°€

            # ëª¨ë“  í‚¤ì›Œë“œê°€ ì´ í–‰ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
            if all(any(keyword in val for val in row_values) for keyword in keywords):
                return i
        except IndexError:
            # í–‰ ì¸ë±ìŠ¤ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ì¤‘ë‹¨
            break

    # ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ê°’ 0 ë°˜í™˜ (ë˜ëŠ” ì—ëŸ¬ ì²˜ë¦¬)
    # logger.warning("í—¤ë” í–‰ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ 0ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    return 0 # í˜„ì¬ ë¡œì§ ìœ ì§€ ì‹œ 0 ë°˜í™˜, í˜¹ì€ ì—ëŸ¬ ë°˜í™˜ ê³ ë ¤

# --- PDF ë¯¸ë¦¬ë³´ê¸° ì—‘ì…€ ì €ì¥ í•¨ìˆ˜ --- 
def save_pdf_preview_to_excel(selected_date, sel_dept, page_num, img: Image.Image, excel_path=None):
    """PDF ë¯¸ë¦¬ë³´ê¸° ì´ë¯¸ì§€ë¥¼ S3ì— ì €ì¥í•˜ê³  ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡í•©ë‹ˆë‹¤."""
    try:
        s3_handler = S3Handler()
        
        # PIL Image ê°ì²´ë¥¼ ì§ì ‘ ì „ë‹¬
        result = s3_handler.save_pdf_preview_image(selected_date, sel_dept, page_num, img)

        if result["status"] == "success":
            return {"status": "success", "message": f"ì´ë¯¸ì§€ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì‹œ '{sel_dept}' ì‹œíŠ¸ì— ì‚½ì…ë©ë‹ˆë‹¤."}
        else:
            return {"status": "error", "message": result.get("message", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")}
    
    except Exception as e:
        logger.error(f"ì´ë¯¸ì§€ ì €ì¥ ë° ë©”íƒ€ë°ì´í„° ê¸°ë¡ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        return {"status": "error", "message": f"ì´ë¯¸ì§€ ì €ì¥ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}

# --- ë¶€ì„œë³„ ì—‘ì…€ ë‹¤ìš´ë¡œë“œ í•¨ìˆ˜ (Openpyxl ë‹¨ë… ì‚¬ìš©ìœ¼ë¡œ ìˆ˜ì •) --- 
def download_department_excel(selected_dates):
    """
    ì„ íƒí•œ ì—¬ëŸ¬ ë‚ ì§œì˜ ë°ì´í„°ë¥¼ í•˜ë‚˜ë¡œ í•©ì³
    ê° ë¶€ì„œë³„ë¡œ ì‹œíŠ¸(ë°ì´í„°+ì´ë¯¸ì§€)ë¥¼ ìƒì„±í•˜ì—¬ ì—‘ì…€ë¡œ ë°˜í™˜
    """
    try:
        s3_handler = S3Handler()

        # 1. ê¸°ì¡´ í†µí•© mismatches_full.json ë¡œë“œ (í†µí•© ì‘ì—… ì—†ì´)
        df_full = s3_handler.load_full_mismatches()
        
        if df_full is None or df_full.empty:
            wb = Workbook()
            ws = wb.active
            ws.title = "ë°ì´í„° ì—†ìŒ"
            ws.cell(row=1, column=1, value="ì„ íƒí•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì—†ìŒ")
            buffer = io.BytesIO()
            wb.save(buffer)
            buffer.seek(0)
            return buffer.getvalue(), "ë¶€ì„œë³„_í†µê³„_ë°ì´í„°ì—†ìŒ.xlsx"

        # 2. ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª© í•„í„°ë§ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
        try:
            # ì„¸ì…˜ ìƒíƒœì—ì„œ ì™„ë£Œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸° (S3 ë¡œë”© ì—†ìŒ)
            completion_logs = st.session_state.get('completion_logs', [])
            filtered_df = filter_completed_items(df_full, completion_logs) if completion_logs else df_full
        except Exception as e:
            logger.warning(f"ì™„ë£Œì²˜ë¦¬ ê¸°ë¡ í•„í„°ë§ ì˜¤ë¥˜: {e}")
            filtered_df = df_full

        # 3. ì„ íƒí•œ ë‚ ì§œë¡œ í•„í„°ë§
        all_excels = []
        for dt in selected_dates:
            # ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜ ë° í•„í„°ë§
            if not pd.api.types.is_datetime64_any_dtype(filtered_df['ë‚ ì§œ']):
                filtered_df['ë‚ ì§œ'] = pd.to_datetime(filtered_df['ë‚ ì§œ'], errors='coerce')
            
            df = filtered_df[filtered_df['ë‚ ì§œ'].dt.strftime('%Y-%m-%d') == dt].copy()
            if not df.empty:
                all_excels.append(df)
                
        if not all_excels:
            wb = Workbook()
            ws = wb.active
            ws.title = "ë°ì´í„° ì—†ìŒ"
            ws.cell(row=1, column=1, value="ì„ íƒí•œ ë‚ ì§œì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì—†ìŒ")
            buffer = io.BytesIO()
            wb.save(buffer)
            buffer.seek(0)
            return buffer.getvalue(), "ë¶€ì„œë³„_í†µê³„_ë°ì´í„°ì—†ìŒ.xlsx"

        excel_df = pd.concat(all_excels, ignore_index=True)

        # 4. ì„ íƒ ë‚ ì§œì˜ ëª¨ë“  ì´ë¯¸ì§€ ì·¨í•© (ë©”íƒ€ë°ì´í„° ê¸°ì¤€)
        dept_images = {}
        missing_depts_with_images = set()  # ëˆ„ë½ëœ ë¶€ì„œ ì¶”ì 
        
        for dt in selected_dates:
            metadata_result = s3_handler.load_metadata(dt)
            metadata = metadata_result.get("data", {}) if metadata_result.get("status") == "success" else {}
            preview_images = metadata.get("preview_images", [])
            
            # í•´ë‹¹ ë‚ ì§œì˜ ì—‘ì…€ ë¶€ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            excel_depts_for_date = set()
            if not excel_df.empty:
                date_filtered_excel = excel_df[excel_df['ë‚ ì§œ'].dt.strftime('%Y-%m-%d') == dt]
                if not date_filtered_excel.empty and 'ë¶€ì„œëª…' in date_filtered_excel.columns:
                    excel_depts_for_date = set(date_filtered_excel['ë¶€ì„œëª…'].unique())
            
            # PDF ë¶€ì„œ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° (departments_with_pages_by_dateì—ì„œ)
            pdf_depts_for_date = set()
            if dt in st.session_state.get('departments_with_pages_by_date', {}):
                dept_page_tuples = st.session_state.departments_with_pages_by_date[dt]
                pdf_depts_for_date = {dept for dept, page in dept_page_tuples if dept}
            
            # ëˆ„ë½ëœ ë¶€ì„œ ì°¾ê¸° (PDFì—ë§Œ ìˆê³  ì—‘ì…€ì— ì—†ëŠ” ë¶€ì„œ)
            missing_depts_for_date = pdf_depts_for_date - excel_depts_for_date
            missing_depts_with_images.update(missing_depts_for_date)
            
            # ëª¨ë“  ì´ë¯¸ì§€ ì •ë³´ ìˆ˜ì§‘
            for img_info in preview_images:
                dept = img_info.get("dept")
                if dept: 
                    # ë‚ ì§œ ì •ë³´ ì¶”ê°€
                    img_info_with_date = img_info.copy()
                    img_info_with_date['date'] = dt
                    dept_images.setdefault(dept, []).append(img_info_with_date)

        # 5. ë¶€ì„œë³„ë¡œ ì‹œíŠ¸ ìƒì„± (ì—‘ì…€ ë¶€ì„œ + ì´ë¯¸ì§€ê°€ ìˆëŠ” ëª¨ë“  ë¶€ì„œ í¬í•¨)
        # ì—‘ì…€ì— ìˆëŠ” ë¶€ì„œ ëª©ë¡
        excel_depts = set()
        if not excel_df.empty and 'ë¶€ì„œëª…' in excel_df.columns:
            excel_depts = set(excel_df['ë¶€ì„œëª…'].unique())
        
        # ì´ë¯¸ì§€ê°€ ìˆëŠ” ëª¨ë“  ë¶€ì„œ (PDFì—ì„œ ì¶”ì¶œëœ ë¶€ì„œ í¬í•¨)
        image_depts = set(dept_images.keys())
        
        # ì „ì²´ ë¶€ì„œ ëª©ë¡ (ì—‘ì…€ ë¶€ì„œ + ì´ë¯¸ì§€ ë¶€ì„œ)
        all_depts = excel_depts | image_depts
        
        # ëˆ„ë½ëœ ë¶€ì„œ í™•ì¸ ë° ë¡œê¹…
        missing_depts_final = missing_depts_with_images & image_depts
        existing_depts = excel_depts & image_depts
        
        logger.info(f"ì—‘ì…€ ì‹œíŠ¸ ìƒì„± - ì´ ë¶€ì„œ ìˆ˜: {len(all_depts)}")
        logger.info(f"  - ì—‘ì…€ì— ìˆëŠ” ë¶€ì„œ: {len(excel_depts)}ê°œ")
        logger.info(f"  - ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¶€ì„œ: {len(image_depts)}ê°œ")
        logger.info(f"  - ëˆ„ë½ëœ ë¶€ì„œ (ìƒˆ ì‹œíŠ¸ ìƒì„±): {len(missing_depts_final)}ê°œ")
        if missing_depts_final:
            logger.info(f"  - ëˆ„ë½ëœ ë¶€ì„œ ëª©ë¡: {', '.join(sorted(missing_depts_final))}")
        
        wb = Workbook()
        if "Sheet" in wb.sheetnames:
            wb.remove(wb["Sheet"])  # ê¸°ë³¸ ì‹œíŠ¸ ì œê±°

        headers = ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰', 'ìˆ˜ë ¹ëŸ‰', 'ì°¨ì´', 'ëˆ„ë½']

        for dept in sorted(list(all_depts)):
            # ì•ˆì „í•œ ì‹œíŠ¸ëª… ìƒì„± (ì—‘ì…€ ì‹œíŠ¸ëª… ì œí•œì‚¬í•­ ê³ ë ¤)
            safe_sheet_name = re.sub(r'[\\/*?:\[\]]', '_', str(dept))[:31]
            
            # ì‹œíŠ¸ëª… ì¤‘ë³µ ë°©ì§€ (ê°™ì€ ì´ë¦„ì˜ ì‹œíŠ¸ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸)
            original_name = safe_sheet_name
            counter = 1
            while safe_sheet_name in [ws.title for ws in wb.worksheets]:
                safe_sheet_name = f"{original_name[:28]}_{counter}"
                counter += 1
            
            # ìƒˆ ì‹œíŠ¸ ìƒì„±
            ws = wb.create_sheet(safe_sheet_name)
            
            # ëˆ„ë½ëœ ë¶€ì„œì¸ì§€ í™•ì¸
            is_missing_dept = dept in missing_depts_with_images
            has_excel_data = dept in excel_depts
            
            # ì‹œíŠ¸ ìƒì„± ë¡œê¹…
            if is_missing_dept:
                logger.info(f"ëˆ„ë½ëœ ë¶€ì„œ '{dept}' ì‹œíŠ¸ ìƒì„±: {safe_sheet_name}")
            else:
                logger.debug(f"ì¼ë°˜ ë¶€ì„œ '{dept}' ì‹œíŠ¸ ìƒì„±: {safe_sheet_name}")
            
            # ë°ì´í„°: ì„ íƒí•œ ëª¨ë“  ë‚ ì§œì˜ í•´ë‹¹ ë¶€ì„œ ë°ì´í„°ë§Œ ì¶”ì¶œ
            dept_df_export = pd.DataFrame(columns=headers)
            if has_excel_data and not excel_df.empty:
                dept_df_filtered = excel_df[excel_df['ë¶€ì„œëª…'] == dept].copy()
                for col in headers:
                    if col not in dept_df_filtered.columns:
                        if col == 'ì°¨ì´':
                            dept_df_filtered[col] = dept_df_filtered.get('ìˆ˜ë ¹ëŸ‰', 0) - dept_df_filtered.get('ì²­êµ¬ëŸ‰', 0)
                        else:
                            dept_df_filtered[col] = ''
                mask = (dept_df_filtered['ì°¨ì´'] == 1) & (dept_df_filtered['ì²­êµ¬ëŸ‰'] == 0) & (dept_df_filtered['ìˆ˜ë ¹ëŸ‰'] == 1)
                dept_df_filtered.loc[mask, 'ëˆ„ë½'] = 'ëˆ„ë½'
                dept_df_filtered['ëˆ„ë½'] = dept_df_filtered['ëˆ„ë½'].fillna('')
                # ë‚ ì§œ ì¹¼ëŸ¼ í¬ë§·
                if 'ë‚ ì§œ' in dept_df_filtered.columns:
                    dept_df_filtered['ë‚ ì§œ'] = pd.to_datetime(dept_df_filtered['ë‚ ì§œ'], errors='coerce').dt.strftime('%Y-%m-%d')
                dept_df_export = dept_df_filtered[headers]
                logger.debug(f"ë¶€ì„œ '{dept}' ì—‘ì…€ ë°ì´í„°: {len(dept_df_export)}í–‰")
            elif is_missing_dept:
                logger.info(f"ëˆ„ë½ëœ ë¶€ì„œ '{dept}': ì—‘ì…€ ë°ì´í„° ì—†ìŒ, ì´ë¯¸ì§€ë§Œ í¬í•¨")
            
            # í—¤ë” ì‘ì„±
            ws.append(headers)
            
            # ë°ì´í„° ì‘ì„±
            if not dept_df_export.empty:
                for r in dataframe_to_rows(dept_df_export, index=False, header=False):
                    ws.append(r)
            else:
                # ëˆ„ë½ëœ ë¶€ì„œì˜ ê²½ìš° íŠ¹ë³„í•œ ë©”ì‹œì§€ í‘œì‹œ
                if is_missing_dept:
                    ws.cell(row=2, column=1, value="âš ï¸ ëˆ„ë½ëœ ë¶€ì„œ")
                    ws.cell(row=2, column=2, value=dept)
                    ws.cell(row=3, column=1, value="ìƒíƒœ")
                    ws.cell(row=3, column=2, value="PDF ì¸ìˆ˜ì¦ì—ë§Œ ìˆê³  ì—‘ì…€ ë°ì´í„°ì—ëŠ” ì—†ëŠ” ë¶€ì„œ")
                    ws.cell(row=4, column=1, value="ë°œê²¬ ë‚ ì§œ")
                    # í•´ë‹¹ ë¶€ì„œê°€ ë°œê²¬ëœ ë‚ ì§œë“¤ í‘œì‹œ
                    dept_dates = []
                    for img_info in dept_images.get(dept, []):
                        date_info = img_info.get('date', '')
                        if date_info and date_info not in dept_dates:
                            dept_dates.append(date_info)
                    ws.cell(row=4, column=2, value=", ".join(sorted(dept_dates)) if dept_dates else "ì•Œ ìˆ˜ ì—†ìŒ")
                    ws.cell(row=5, column=1, value="ì¡°ì¹˜ í•„ìš”")
                    ws.cell(row=5, column=2, value="ì•„ë˜ ì¸ìˆ˜ì¦ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ì—¬ ëˆ„ë½ëœ ë°ì´í„°ë¥¼ ì—‘ì…€ì— ì¶”ê°€í•˜ì„¸ìš”.")
                    ws.cell(row=6, column=1, value="ì´ë¯¸ì§€ ìˆ˜")
                    ws.cell(row=6, column=2, value=f"{len(dept_images.get(dept, []))}ê°œ")
                else:
                    ws.cell(row=2, column=1, value=f"ë°ì´í„° ì—†ìŒ")
                    ws.cell(row=2, column=2, value="ì´ ë¶€ì„œëŠ” ì„ íƒí•œ ê¸°ê°„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            # ì´ë¯¸ì§€ ì‚½ì…
            images = dept_images.get(dept, [])
            if images:
                last_data_col = len(headers)
                image_col_start = last_data_col + 2
                max_images_per_row = 2
                
                # ëˆ„ë½ëœ ë¶€ì„œì˜ ê²½ìš° íŠ¹ë³„í•œ í—¤ë” í‘œì‹œ
                if is_missing_dept:
                    ws.cell(row=1, column=image_col_start, value="âš ï¸ ëˆ„ë½ ë¶€ì„œ ì¸ìˆ˜ì¦ ì´ë¯¸ì§€")
                    # ëˆ„ë½ëœ ë¶€ì„œ ì„¤ëª… ì¶”ê°€
                    ws.cell(row=2, column=image_col_start, value="ì´ ë¶€ì„œëŠ” PDFì—ë§Œ ìˆê³ ")
                    ws.cell(row=3, column=image_col_start, value="ì—‘ì…€ ë°ì´í„°ì—ëŠ” ì—†ìŠµë‹ˆë‹¤.")
                    ws.cell(row=4, column=image_col_start, value="ì•„ë˜ ì´ë¯¸ì§€ë¥¼ í™•ì¸í•˜ì—¬")
                    ws.cell(row=5, column=image_col_start, value="ëˆ„ë½ëœ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ì„¸ìš”.")
                    ws.cell(row=6, column=image_col_start, value="ìë™ ìƒì„±ëœ ì‹œíŠ¸ì…ë‹ˆë‹¤.")
                    image_start_row = 8  # ë” ë§ì€ ì •ë³´ê°€ ì¶”ê°€ë˜ì—ˆìœ¼ë¯€ë¡œ ì‹œì‘ í–‰ ì¡°ì •
                else:
                    ws.cell(row=1, column=image_col_start, value="ì¸ìˆ˜ì¦ ì´ë¯¸ì§€")
                    image_start_row = 2
                
                for i, img_info in enumerate(images):
                    try:
                        img_bytes = get_pdf_preview_image_from_s3(img_info["file_key"])
                        if not img_bytes:
                            continue  # ì´ë¯¸ì§€ê°€ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
                        xl_img = XLImage(io.BytesIO(img_bytes))
                        xl_img.width = 350
                        xl_img.height = 500
                        row_idx = i // max_images_per_row
                        col_idx = i % max_images_per_row
                        col_pos = image_col_start + (col_idx * 4)
                        current_row_for_images = image_start_row + (row_idx * 15)
                        
                        # ì´ë¯¸ì§€ ì •ë³´ (ëˆ„ë½ëœ ë¶€ì„œì˜ ê²½ìš° ë” ìì„¸í•œ ì •ë³´ í‘œì‹œ)
                        date_info = img_info.get('date', '')
                        page_info = img_info.get('page', '?')
                        if is_missing_dept:
                            ws.cell(row=current_row_for_images, column=col_pos, 
                                   value=f"âš ï¸ ëˆ„ë½ë¶€ì„œ: {dept}")
                            ws.cell(row=current_row_for_images+1, column=col_pos, 
                                   value=f"ë‚ ì§œ: {date_info}, í˜ì´ì§€: {page_info}")
                            img_row = current_row_for_images + 2
                        else:
                            ws.cell(row=current_row_for_images, column=col_pos, 
                                   value=f"ë‚ ì§œ:{date_info}, í˜ì´ì§€:{page_info}")
                            img_row = current_row_for_images + 1
                        
                        ws.add_image(xl_img, f"{ws.cell(row=img_row, column=col_pos).coordinate}")
                    except Exception as e:
                        logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬/ì‚½ì… ì¤‘ ì˜¤ë¥˜ ({dept}, {img_info.get('page')}): {e}")
                        continue

        # ì—´ ë„ˆë¹„
        for ws in wb.worksheets:
            std_widths = {
                'A': 12, 'B': 20, 'C': 12, 'D': 30, 'E': 10, 'F': 10, 'G': 10, 'H': 10,
            }
            for col, width in std_widths.items():
                if col in ws.column_dimensions:
                    ws.column_dimensions[col].width = width

        # ìµœì¢… ì—‘ì…€ íŒŒì¼ ë²„í¼ì— ì €ì¥
        excel_buffer_final = io.BytesIO()
        wb.save(excel_buffer_final)
        excel_buffer_final.seek(0)
        
        # íŒŒì¼ëª… ìƒì„± (ëˆ„ë½ëœ ë¶€ì„œ ì •ë³´ í¬í•¨)
        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
        if missing_depts_with_images:
            missing_count = len(missing_depts_with_images)
            file_name = f"ë¶€ì„œë³„_í†µê³„_ëˆ„ë½ë¶€ì„œ{missing_count}ê°œí¬í•¨_{current_time}.xlsx"
            logger.info(f"âœ… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ì´ {len(all_depts)}ê°œ ë¶€ì„œ ì‹œíŠ¸ ìƒì„±")
            logger.info(f"   - ì¼ë°˜ ë¶€ì„œ: {len(excel_depts)}ê°œ")
            logger.info(f"   - ëˆ„ë½ ë¶€ì„œ (ìë™ ìƒì„±): {missing_count}ê°œ")
            logger.info(f"   - ëˆ„ë½ëœ ë¶€ì„œ ëª©ë¡: {', '.join(sorted(missing_depts_with_images))}")
            logger.info(f"   - íŒŒì¼ëª…: {file_name}")
        else:
            file_name = f"ë¶€ì„œë³„_í†µê³„_{current_time}.xlsx"
            logger.info(f"âœ… ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ì´ {len(all_depts)}ê°œ ë¶€ì„œ ì‹œíŠ¸ ìƒì„± (ëˆ„ë½ ë¶€ì„œ ì—†ìŒ)")
            logger.info(f"   - íŒŒì¼ëª…: {file_name}")
        
        return excel_buffer_final.getvalue(), file_name

    except Exception as e:
        logger.error(f"ì—‘ì…€ ë‹¤ìš´ë¡œë“œ(download_department_excel) ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"ì—‘ì…€ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return None, None


# --- ë‚ ì§œ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸° í•¨ìˆ˜ --- 
def get_date_options():
    """ì²˜ë¦¬ëœ ë‚ ì§œ ëª©ë¡ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    s3_handler = S3Handler()
    result = s3_handler.list_processed_dates()
    
    # ê²°ê³¼ê°€ ë”•ì…”ë„ˆë¦¬ì´ê³  'status'ê°€ 'success'ì¸ ê²½ìš° 'dates' í‚¤ì—ì„œ ë‚ ì§œ ëª©ë¡ì„ ê°€ì ¸ì˜´
    if isinstance(result, dict):
        if result.get('status') == 'success' and 'dates' in result:
            return sorted(result['dates'], reverse=True)
        elif result.get('status') == 'error':
            logger.error(f"ë‚ ì§œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {result.get('message')}")
            return []
    
    # ê²°ê³¼ê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ë°˜í™˜
    if isinstance(result, list):
        return sorted(result, reverse=True)
    
    # ê·¸ ì™¸ì˜ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    logger.warning(f"ì˜ˆìƒì¹˜ ëª»í•œ ë‚ ì§œ ëª©ë¡ í˜•ì‹: {type(result)}")
    return []


# --- ì•± ì„¤ì •, ìŠ¤íƒ€ì¼, ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë“± --- 
# ... (ê¸°ì¡´ ì•± ì„¤ì • ë° ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ì½”ë“œ) ...

# ë©”ì¸ í•¨ìˆ˜
def main():
    # S3 ì—°ê²° í™•ì¸
    if not check_s3_connection():
        st.error("S3 ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        return
    
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if 'pdf_paths_by_date' not in st.session_state:
        st.session_state.pdf_paths_by_date = {}
    if 'dept_page_tuples_by_date' not in st.session_state:
        st.session_state.dept_page_tuples_by_date = {}
    if 'ocr_results_by_date' not in st.session_state:
        st.session_state.ocr_results_by_date = {}
    if 'aggregated_ocr_items_by_date' not in st.session_state:
        st.session_state.aggregated_ocr_items_by_date = {}
    if 'departments_with_pages_by_date' not in st.session_state:
        st.session_state.departments_with_pages_by_date = {}
    if 'loaded_dates' not in st.session_state:
        st.session_state.loaded_dates = set()  # ì´ë¯¸ ë¡œë“œëœ ë‚ ì§œë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ì„¸íŠ¸
    
    # ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ (ì•± ì‹œì‘ ì‹œ)
    if 'completion_logs' not in st.session_state:
        try:
            s3_handler = S3Handler()
            completion_logs_result = s3_handler.load_completion_logs()
            
            if completion_logs_result["status"] == "success":
                st.session_state.completion_logs = completion_logs_result["data"]
                logger.info(f"ì•± ì‹œì‘ ì‹œ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ {len(st.session_state.completion_logs)}ê°œ ë¡œë“œ ì„±ê³µ.")
            elif completion_logs_result["status"] == "not_found":
                st.session_state.completion_logs = []
                logger.info("ì•± ì‹œì‘ ì‹œ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šì•„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            else: # "error" ë˜ëŠ” ê¸°íƒ€ ìƒíƒœ
                st.session_state.completion_logs = []
                logger.error(f"ì•± ì‹œì‘ ì‹œ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ ì‹¤íŒ¨: {completion_logs_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}. ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
        except Exception as e:
            st.session_state.completion_logs = []
            logger.error(f"ì•± ì‹œì‘ ì‹œ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜ˆì™¸ ë°œìƒ: {e}", exc_info=True)
    
    # í•œê¸€ í°íŠ¸ ì„¤ì •
    set_korean_font()
    
    st.title("ìƒê³„ë°±ë³‘ì› ì¸ìˆ˜ì¦ & ì—‘ì…€ ë°ì´í„° ë¹„êµ ì‹œìŠ¤í…œ")
    
    s3_handler = S3Handler()

    # --- ì•± ì‹œì‘ ì‹œ ë°ì´í„° ë¡œë“œ ìµœì í™” (í†µí•© ì‘ì—… ì œê±°) ---
    if 'mismatch_data' not in st.session_state or st.session_state.mismatch_data.empty:
        logger.info("ì„¸ì…˜ì— ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆì–´ S3ì—ì„œ ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
        try:
            # ê¸°ì¡´ í†µí•© íŒŒì¼ë§Œ ë¡œë“œ (í†µí•© ì‘ì—…ì€ í•˜ì§€ ì•ŠìŒ)
            full_mismatches = s3_handler.load_full_mismatches()
            if not full_mismatches.empty:
                st.session_state.mismatch_data = full_mismatches
                logger.info(f"ê¸°ì¡´ í†µí•© mismatches_full.json ë¡œë“œ ì™„ë£Œ: {len(full_mismatches)}ê°œ í•­ëª©")
            else:
                # í†µí•© íŒŒì¼ì´ ì—†ì–´ë„ ì•± ì‹œì‘ ì‹œì—ëŠ” í†µí•© ì‘ì—…í•˜ì§€ ì•ŠìŒ
                st.session_state.mismatch_data = pd.DataFrame()
                logger.info("ê¸°ì¡´ í†µí•© íŒŒì¼ì´ ì—†ì–´ ë¹ˆ DataFrameìœ¼ë¡œ ì´ˆê¸°í™”")
        except Exception as e:
            logger.error(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜: {e}")
            st.session_state.mismatch_data = pd.DataFrame()
    else:
        logger.info("ì„¸ì…˜ì— ì´ë¯¸ ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. S3 ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")

    if 'excel_data' not in st.session_state or st.session_state.excel_data.empty:
        logger.info("ì„¸ì…˜ì— ì—‘ì…€ ë°ì´í„°ê°€ ì—†ì–´ ê°•ì œ ë¦¬ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤.")
        force_reload_excel_data(s3_handler) # ì•± ì‹œì‘ ì‹œ í•­ìƒ ìµœì‹  ì—‘ì…€ ë°ì´í„° ë¡œë“œ

    # ë¶ˆì¼ì¹˜ ë°ì´í„° ì¬ê³„ì‚°ì€ í•„ìš”í•œ ê²½ìš°ì—ë§Œ ìˆ˜í–‰ (ì˜ˆ: íŒŒì¼ ì—…ë¡œë“œ í›„)
    # process_files í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì¬ê³„ì‚° ë¡œì§ í˜¸ì¶œ
    
    # PDF í‚¤ ëˆ„ë½ëœ ë©”íƒ€ë°ì´í„° ìˆ˜ì •
    if 'available_dates' in st.session_state:
        for date in st.session_state.available_dates:
            update_metadata_with_pdf(s3_handler, date)
            
    # ê°•ì œ ë¦¬ë¡œë“œ í”Œë˜ê·¸ ì²˜ë¦¬
    if st.session_state.get('force_reload_mismatch', False):
        logger.info("force_reload_mismatch í”Œë˜ê·¸ ê°ì§€. ë¶ˆì¼ì¹˜ ë°ì´í„° ì¬ê³„ì‚° ë° UI ìƒˆë¡œê³ ì¹¨")
        recalculate_mismatches(s3_handler) # ì¬ê³„ì‚° ìˆ˜í–‰
        del st.session_state.force_reload_mismatch
        st.rerun() # UI ì¦‰ì‹œ ì—…ë°ì´íŠ¸

    # S3 ì €ì¥ì†Œì—ì„œ ì²˜ë¦¬ëœ ë‚ ì§œ ëª©ë¡ ë¡œë“œ
    processed_dates_result = s3_handler.list_processed_dates()
    if processed_dates_result["status"] == "success" and processed_dates_result["dates"]:
        st.session_state.dates = sorted(processed_dates_result["dates"])  # ì „ì²´ ë‚ ì§œë¥¼ ì €ì¥

        # --- ì‘ì—… ê¸°ê°„ ì„ íƒ UI ì¶”ê°€ ---
        st.sidebar.header("ì‘ì—… ê¸°ê°„ ì„ íƒ")
        col1, col2 = st.sidebar.columns(2)
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        default_start = datetime.today() - timedelta(days=30)
        default_end = datetime.today()
        
        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” (ìœ„ì ¯ ìƒì„± ì „ì—)
        if 'work_start_date' not in st.session_state:
            st.session_state.work_start_date = default_start.date()
        if 'work_end_date' not in st.session_state:
            st.session_state.work_end_date = default_end.date()
        
        start_date = col1.date_input(
            "ì‹œì‘ì¼", 
            value=st.session_state.work_start_date, 
            key="date_input_start"
        )
        end_date = col2.date_input(
            "ì¢…ë£Œì¼", 
            value=st.session_state.work_end_date, 
            key="date_input_end"
        )

        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.work_start_date = start_date
        st.session_state.work_end_date = end_date

        if st.session_state.work_start_date > st.session_state.work_end_date:
            st.sidebar.error("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ì•ì„œì•¼ í•©ë‹ˆë‹¤.")
            st.stop()

        st.session_state.available_dates = [
            d for d in st.session_state.dates
            if st.session_state.work_start_date <= datetime.strptime(d, "%Y-%m-%d").date() <= st.session_state.work_end_date
        ]

        if not st.session_state.available_dates:
            st.sidebar.warning("ì„ íƒí•œ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.stop()


    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("íŒŒì¼ ì—…ë¡œë“œ")
        
        # --- ë‹¤ì¤‘ ì—‘ì…€ ì—…ë¡œë“œ í—ˆìš© ---
        excel_files = st.file_uploader(
            "ì—‘ì…€ íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)", 
            type=["xlsx", "xls"],
            accept_multiple_files=True # ë‹¤ì¤‘ íŒŒì¼ í—ˆìš©
        )
        # -------------------------
        
        # --- ë‹¤ì¤‘ PDF ì—…ë¡œë“œ í—ˆìš© ---
        pdf_files = st.file_uploader(
            "PDF íŒŒì¼ ì—…ë¡œë“œ (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥)",
            type=["pdf"],
            accept_multiple_files=True # ë‹¤ì¤‘ íŒŒì¼ í—ˆìš©
        )
        # -------------------------
        
        # ì²˜ë¦¬ ë²„íŠ¼
        if excel_files or pdf_files: # ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì²˜ë¦¬ ê°€ëŠ¥
            if st.button("ì²˜ë¦¬ ì‹œì‘", key="process_button"):
                process_files(excel_files, pdf_files) # excel_files ë¦¬ìŠ¤íŠ¸ ì „ë‹¬
        

        
        # ë¦¬ì…‹ ë²„íŠ¼
        if st.button("ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”", key="reset_button"):
            for key in list(st.session_state.keys()):
                del st.session_state[key]
            st.success("ëª¨ë“  ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        st.markdown("---")
        st.markdown("""
        ### ì‚¬ìš© ë°©ë²•
        1. ì—‘ì…€ íŒŒì¼ê³¼ PDF íŒŒì¼(ë“¤)ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.
        2. 'ì²˜ë¦¬ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.
        3. **ì‘ì—… ê¸°ê°„**ì„ ì„¤ì •í•˜ì„¸ìš”.
        4. ê° íƒ­ì—ì„œ ì„¸ë¶€ ë‚ ì§œë¥¼ ì„ íƒí•˜ì—¬ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.
           (PDF ê´€ë ¨ ì •ë³´ëŠ” í•´ë‹¹ ë‚ ì§œì˜ PDFê°€ ìˆì„ ë•Œë§Œ í‘œì‹œë©ë‹ˆë‹¤.)
        """)

        # ë¬¼í’ˆ DB ë¡œë“œ (S3 ìš°ì„ , ì—†ìœ¼ë©´ ì—…ë¡œë“œ í—ˆìš©)
        if not st.session_state.item_db:
            # S3ì—ì„œ DB íŒŒì¼ ë¡œë“œ ì‹œë„
            s3_db = load_item_db_from_s3()
            if s3_db:
                st.session_state.item_db = s3_db
                st.info("S3ì—ì„œ ë¬¼í’ˆ DB íŒŒì¼ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            else:
                # S3ì— ì—†ìœ¼ë©´ ì—…ë¡œë“œ í—ˆìš©
                db_file = st.file_uploader("ë¬¼í’ˆ DB íŒŒì¼ ì—…ë¡œë“œ", type=["xlsx"], key="db_file")
                if db_file:
                    # ì—…ë¡œë“œëœ íŒŒì¼ì„ S3ì— ì €ì¥
                    if upload_db_to_s3(db_file):
                        # íŒŒì¼ í¬ì¸í„° ìœ„ì¹˜ ë¦¬ì…‹
                        db_file.seek(0)
                        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•˜ê³  ë¡œë“œ
                        with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as temp_file:
                            temp_file.write(db_file.read())
                            temp_path = temp_file.name
                        st.session_state.item_db = data_analyzer.load_item_db(temp_path)
                        os.unlink(temp_path)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                        st.success("ë¬¼í’ˆ DB íŒŒì¼ì´ ì—…ë¡œë“œë˜ê³  ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    # íƒ­ ìƒì„±
    tabs = st.tabs(["ë‚ ì§œë³„ ì‘ì—…", "ë¶€ì„œë³„ í†µê³„", "ì™„ë£Œ í•­ëª© ê´€ë¦¬"])
    
    # ì„ íƒëœ ë‚ ì§œ í™•ì¸
    if 'selected_date' not in st.session_state or not st.session_state.selected_date:
        # ì‚¬ì´ë“œë°”ì—ì„œ ë‚ ì§œë¥¼ ì„ íƒí•˜ì§€ ì•Šì•˜ê±°ë‚˜, available_datesê°€ ì—†ì„ ìˆ˜ ìˆìŒ
        # ì´ ê²½ìš°, ë‚ ì§œë³„ ì‘ì—… íƒ­ì—ì„œ ë‚ ì§œ ì„ íƒì„ ìœ ë„í•˜ê±°ë‚˜, ê¸°ê°„ ë‚´ ì²« ë‚ ì§œë¥¼ ê¸°ë³¸ìœ¼ë¡œ í•  ìˆ˜ ìˆìŒ
        # ì—¬ê¸°ì„œëŠ” ë‚ ì§œë³„ ì‘ì—… íƒ­ì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•˜ê³ , mainì—ì„œëŠ” blockingí•˜ì§€ ì•ŠìŒ
        pass # display_mismatch_tabì—ì„œ ì²˜ë¦¬

    selected_date = st.session_state.selected_date
    
    # íƒ­ 1: ë‚ ì§œë³„ ì‘ì—…
    with tabs[0]:
        display_mismatch_tab() # ì¸ì ì—†ì´ í˜¸ì¶œ

    # íƒ­ 2: ë¶€ì„œë³„ í†µê³„
    with tabs[1]:
        display_filter_tab()
           
    # íƒ­ 3: ì™„ë£Œ í•­ëª© ê´€ë¦¬
    with tabs[2]:
        display_completed_items_tab() # ìƒˆë¡œ ì¶”ê°€í•  í•¨ìˆ˜ í˜¸ì¶œ


# íŒŒì¼ ì²˜ë¦¬ í•¨ìˆ˜ (ë‹¤ì¤‘ PDF ì²˜ë¦¬)

def process_files(excel_files, pdf_files):
    try:
        s3_handler = S3Handler()
        processed_dates = set() # ë‚ ì§œ ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ set ì‚¬ìš©
        current_excel_data = pd.DataFrame()
        cumulative_excel_key = f"{S3_DIRS['EXCEL']}latest/cumulative_excel.xlsx"

        # --- 1. ê¸°ì¡´ ëˆ„ì  ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì‹œë„ --- 
        st.write("ê¸°ì¡´ ëˆ„ì  ì—‘ì…€ ë°ì´í„° ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        try:
            excel_download_result = s3_handler.download_file(cumulative_excel_key)
            if excel_download_result["status"] == "success":
                excel_buffer = io.BytesIO(excel_download_result["data"])
                # ëˆ„ì  íŒŒì¼ì´ë¯€ë¡œ is_cumulative_flag=True ì „ë‹¬
                load_result = data_analyzer.load_excel_data(excel_buffer, is_cumulative_flag=True)
                if load_result["status"] == "success":
                    current_excel_data = load_result["data"]
                    logger.info(f"S3ì—ì„œ ê¸°ì¡´ ëˆ„ì  ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì„±ê³µ: {len(current_excel_data)}ê°œ í–‰")
                    # ê¸°ì¡´ ë°ì´í„°ì˜ ë‚ ì§œë„ processed_datesì— ì¶”ê°€
                    if 'ë‚ ì§œ' in current_excel_data.columns:
                        processed_dates.update(current_excel_data['ë‚ ì§œ'].astype(str).unique())
                else:
                    logger.warning(f"S3ì—ì„œ ë‹¤ìš´ë¡œë“œí•œ ëˆ„ì  ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {load_result['message']}")
            elif excel_download_result["status"] == "not_found":
                logger.info("S3ì— ê¸°ì¡´ ëˆ„ì  ì—‘ì…€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            else:
                logger.error(f"S3ì—ì„œ ëˆ„ì  ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {excel_download_result['message']}")
        except Exception as e:
            logger.error(f"ê¸°ì¡´ ëˆ„ì  ì—‘ì…€ ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
            st.warning("ê¸°ì¡´ ëˆ„ì  ì—‘ì…€ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        # -------------------------------------
        
        newly_processed_excel_files = [] # ìƒˆë¡œ ì²˜ë¦¬ëœ ì—‘ì…€ íŒŒì¼ëª… ì €ì¥
        
        # --- 2. ìƒˆë¡œ ì—…ë¡œë“œëœ ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ --- 
        if excel_files:
    
            progress_bar_excel = st.progress(0)
            status_text_excel = st.empty()
            
            for i, uploaded_excel_file in enumerate(excel_files, 1):
                status_text_excel.text(f"ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ({i}/{len(excel_files)}): {uploaded_excel_file.name}")
                try:
                    # íŒŒì¼ ì½ê¸°
                    uploaded_excel_file.seek(0)
                    excel_buffer_new = io.BytesIO(uploaded_excel_file.read())
                    uploaded_excel_file.seek(0) # ë‹¤ìŒ ì‚¬ìš© ìœ„í•´ í¬ì¸í„° ë¦¬ì…‹
                    
                    # ë°ì´í„° ë¡œë“œ (ì¼ë°˜ íŒŒì¼ì´ë¯€ë¡œ is_cumulative_flag=False ëª…ì‹œ)
                    logger.info(f"'{uploaded_excel_file.name}' ë¡œë“œ ì‹œë„ (is_cumulative=False)")
                    new_data_result = data_analyzer.load_excel_data(excel_buffer_new, is_cumulative_flag=False)
                    
                    if new_data_result["status"] == "success":
                        new_data_df = new_data_result["data"]
                        logger.info(f"ì—‘ì…€ íŒŒì¼ '{uploaded_excel_file.name}' ë¡œë“œ ì„±ê³µ: {len(new_data_df)}ê°œ í–‰")
                        
                        # ê¸°ì¡´ ë°ì´í„°ì™€ ë³‘í•©
                        current_excel_data = pd.concat([current_excel_data, new_data_df], ignore_index=True)
                        logger.info(f"'{uploaded_excel_file.name}' ë°ì´í„° ë³‘í•© í›„ ì´ {len(current_excel_data)}ê°œ í–‰")
                        
                        # ìƒˆë¡œ ì²˜ë¦¬ëœ ë‚ ì§œ ì¶”ê°€
                        if 'ë‚ ì§œ' in new_data_df.columns:
                            processed_dates.update(new_data_df['ë‚ ì§œ'].astype(str).unique())
                        
                        newly_processed_excel_files.append(uploaded_excel_file.name)
                    else:
                        st.warning(f"ì—‘ì…€ íŒŒì¼ '{uploaded_excel_file.name}' ë¡œë“œ ì‹¤íŒ¨: {new_data_result['message']}")
                        logger.warning(f"ì—‘ì…€ íŒŒì¼ '{uploaded_excel_file.name}' ë¡œë“œ ì‹¤íŒ¨, ë³‘í•© ê±´ë„ˆ<0xEB><0x9B><0x84>: {new_data_result['message']}")
                except Exception as e:
                    logger.error(f"ì—‘ì…€ íŒŒì¼ '{uploaded_excel_file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    st.error(f"ì—‘ì…€ íŒŒì¼ '{uploaded_excel_file.name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                progress_bar_excel.progress(i / len(excel_files))
            
            status_text_excel.text("ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ. ì¤‘ë³µ ì œê±° ì¤‘...")
            
            # --- 3. ì¤‘ë³µ ì œê±° --- 
            key_columns = ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ']
            if all(col in current_excel_data.columns for col in key_columns):
                initial_rows = len(current_excel_data)
                current_excel_data = current_excel_data.drop_duplicates(subset=key_columns, keep='last').reset_index(drop=True)
                removed_rows = initial_rows - len(current_excel_data)
                logger.info(f"ì¤‘ë³µ ë°ì´í„° ì œê±° ì™„ë£Œ. {removed_rows}ê°œ í–‰ ì œê±°ë¨. ìµœì¢… {len(current_excel_data)}ê°œ í–‰.")
            else:
                logger.warning(f"ì¤‘ë³µ ì œê±° ìœ„í•œ í‚¤ ì»¬ëŸ¼ ë¶€ì¡±: {key_columns}. ì¤‘ë³µ ì œê±° ê±´ë„ˆëœ€.")
            
            # --- 4. ëˆ„ì  ì—‘ì…€ ë°ì´í„° S3 ì €ì¥ --- 
            if not current_excel_data.empty:
                try:
                    excel_output_buffer = io.BytesIO()
                    current_excel_data.to_excel(excel_output_buffer, index=False)
                    excel_output_buffer.seek(0)
                    
                    # í•´ì‹œ ê³„ì‚° (ì„ íƒì , ë©”íƒ€ë°ì´í„°ìš©)
                    cumulative_excel_hash_result = s3_handler.get_file_hash(excel_output_buffer)
                    cumulative_excel_hash = cumulative_excel_hash_result.get("hash") if cumulative_excel_hash_result["status"] == "success" else None
                    
                    excel_output_buffer.seek(0)
                    upload_result = s3_handler.upload_file(
                        excel_output_buffer, 
                        "latest", # ë‚ ì§œ ëŒ€ì‹  'latest' ì‚¬ìš©
                        "cumulative_excel.xlsx", # ê³ ì • íŒŒì¼ëª… ì‚¬ìš©
                        'EXCEL' # ë””ë ‰í† ë¦¬ íƒ€ì…
                    )
                    if upload_result["status"] == "success":
                        cumulative_excel_key = upload_result["key"] # ì‹¤ì œ ì €ì¥ëœ í‚¤ ì—…ë°ì´íŠ¸
                        logger.info(f"ëˆ„ì  ì—‘ì…€ ë°ì´í„°ë¥¼ S3ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤: {cumulative_excel_key}")
                    else:
                        st.error(f"ëˆ„ì  ì—‘ì…€ ë°ì´í„° S3 ì €ì¥ ì‹¤íŒ¨: {upload_result['message']}")
                except Exception as e:
                    logger.error(f"ëˆ„ì  ì—‘ì…€ ë°ì´í„° S3 ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                    st.error("ëˆ„ì  ì—‘ì…€ ë°ì´í„°ë¥¼ S3ì— ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            else:
                logger.warning("ì €ì¥í•  ëˆ„ì  ì—‘ì…€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
            # --- 5. ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸ --- 
            st.session_state.excel_data = current_excel_data
            if not current_excel_data.empty:
                st.session_state.standardized_excel_dates = sorted(
                    current_excel_data['ë‚ ì§œ'].astype(str).unique()
                )
                # logger.info(f"ì„¸ì…˜ ì—‘ì…€ ë‚ ì§œ ì—…ë°ì´íŠ¸: {len(st.session_state.standardized_excel_dates)}ê°œ") # ì¤‘ë³µ ë¡œê·¸ ì œê±°
            else:
                st.session_state.standardized_excel_dates = []
                
            st.success(f"{len(excel_files)}ê°œ ì—‘ì…€ íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            progress_bar_excel.empty()
            status_text_excel.empty()
        # -------------------------------------

        # --- PDF íŒŒì¼ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§ê³¼ ìœ ì‚¬í•˜ê²Œ ì§„í–‰, processed_dates ì‚¬ìš©) ---
        # ... (ê¸°ì¡´ PDF ì²˜ë¦¬ ë¡œì§, ë‹¨ processed_dates.update(...) ì‚¬ìš©) ...
        if pdf_files:
            st.markdown("---")
            st.subheader("PDF íŒŒì¼ ì²˜ë¦¬")
            total_pdfs = len(pdf_files)

            
            progress_bar_pdf = st.progress(0)
            status_text_pdf = st.empty()

            for i, pdf_file in enumerate(pdf_files, 1):
                status_text_pdf.write(f"PDF íŒŒì¼ ì²˜ë¦¬ ì¤‘ ({i}/{total_pdfs}): {pdf_file.name}")

                # 1. íŒŒì¼ ë‚´ìš© í•´ì‹œ ê³„ì‚°
                pdf_hash_result = s3_handler.get_file_hash(pdf_file)
                if pdf_hash_result["status"] != "success":
                    st.error(f"PDF íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {pdf_hash_result['message']}")
                    continue
                pdf_hash = pdf_hash_result["hash"]

                # 2. PDF íŒŒì¼ëª…ì—ì„œ ë‚ ì§œ ì¶”ì¶œ
                pdf_filename = pdf_file.name
                extracted_date = standardize_date(pdf_filename)
                
                if extracted_date == pdf_filename:
                    st.warning(f"'{pdf_filename}' íŒŒì¼ëª…ì—ì„œ ë‚ ì§œë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ í˜„ì¬ ë‚ ì§œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    pdf_date = datetime.now().strftime('%Y-%m-%d')
                else:
                    pdf_date = extracted_date
                    logger.info(f"PDF íŒŒì¼ëª… '{pdf_filename}'ì—ì„œ ë‚ ì§œ ì¶”ì¶œ: {pdf_date}")

                # 3. í•´ì‹œê°’ìœ¼ë¡œ ì´ë¯¸ ì²˜ë¦¬ëœ PDFì¸ì§€ í™•ì¸
                exists_result = s3_handler.check_file_exists(pdf_date, pdf_hash, "PDF")
                if exists_result["status"] == "success" and exists_result["exists"]:
                    metadata = exists_result["metadata"]
                    st.info(f"'{pdf_file.name}' ({pdf_date}) íŒŒì¼ì€ ì´ë¯¸ ì²˜ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")
                    
                    ocr_text_result = s3_handler.load_ocr_text(pdf_date)
                    if ocr_text_result["status"] == "success":
                        ocr_result = {
                            "status": "success",
                            "ocr_text": ocr_text_result["data"],
                            "departments_with_pages": metadata.get("departments_with_pages", [])
                        }
                        st.session_state.pdf_paths_by_date[pdf_date] = metadata["pdf_key"]
                        st.session_state.ocr_results_by_date[pdf_date] = ocr_result
                        st.session_state.dept_page_tuples_by_date[pdf_date] = metadata.get("departments_with_pages", [])
                        
                        # departments_with_pages_by_date ì„¸ì…˜ ìƒíƒœ ëª…ì‹œì  ì—…ë°ì´íŠ¸ ì¶”ê°€
                        dept_pages = metadata.get("departments_with_pages", [])
                        st.session_state.departments_with_pages_by_date[pdf_date] = dept_pages
                        # logger.info(f"ê¸°ì¡´ PDF ì²˜ë¦¬ ê²°ê³¼ ë¡œë“œ - ë‚ ì§œ {pdf_date}ì˜ ë¶€ì„œ-í˜ì´ì§€ ì •ë³´: {len(dept_pages)}ê°œ í•­ëª©") # ì¤‘ë³µ ë¡œê·¸ ì œê±°
                        
                        processed_dates.add(pdf_date) # ì²˜ë¦¬ëœ ë‚ ì§œ setì— ì¶”ê°€
                        # logger.info(f"ê¸°ì¡´ PDF ì²˜ë¦¬ ê²°ê³¼ ë¡œë“œ ì™„ë£Œ: {pdf_date}") # ì¤‘ë³µ ë¡œê·¸ ì œê±°
                    else:
                        st.warning(f"OCR í…ìŠ¤íŠ¸ë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
                        exists_result["exists"] = False
                
                if not exists_result.get("exists", False):
                    pdf_file.seek(0)
                    pdf_bytes = pdf_file.read()
                    pdf_buffer_s3 = io.BytesIO(pdf_bytes)
                    pdf_buffer_s3.seek(0)
                    pdf_upload_result = s3_handler.upload_file(
                        pdf_buffer_s3,
                        pdf_date,
                        pdf_file.name,
                        'PDF'
                    )
                    if pdf_upload_result["status"] != "success":
                        st.error(f"PDF íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {pdf_upload_result['message']}")
                        continue
                    
                    pdf_buffer_proc = io.BytesIO(pdf_bytes)
                    pdf_buffer_proc.seek(0)

                    ocr_result = pdf3_module.process_pdf(pdf_buffer_proc)
                    
                    if ocr_result["status"] == "success":
                        ocr_text_save_result = s3_handler.save_ocr_text(pdf_date, ocr_result["ocr_text"])
                        if ocr_text_save_result["status"] != "success":
                            st.warning(f"OCR í…ìŠ¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {ocr_text_save_result['message']}")
                        
                        departments_with_pages = ocr_result.get("departments_with_pages", [])
                        metadata = {
                            "pdf_key": pdf_upload_result["key"],
                            "pdf_hash": pdf_hash,
                            "pdf_filename": pdf_file.name,
                            "ocr_pages": len(ocr_result["ocr_text"]),
                            "departments_with_pages": departments_with_pages,
                            "processed_date": datetime.now().isoformat()
                            # ì—‘ì…€ ê´€ë ¨ ì •ë³´ëŠ” ì•„ë˜ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ì—ì„œ ì¶”ê°€
                        }
                        # ë©”íƒ€ë°ì´í„° ì €ì¥ (ì„ì‹œ, ì•„ë˜ì—ì„œ ë®ì–´ì“¸ ìˆ˜ ìˆìŒ)
                        s3_handler.save_metadata(pdf_date, metadata) 

                        st.session_state.pdf_paths_by_date[pdf_date] = pdf_upload_result["key"]
                        st.session_state.ocr_results_by_date[pdf_date] = ocr_result
                        processed_dates.add(pdf_date) # ì²˜ë¦¬ëœ ë‚ ì§œ setì— ì¶”ê°€
                        st.success(f"'{pdf_file.name}' íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    else:
                        st.error(f"'{pdf_file.name}' OCR ì²˜ë¦¬ ì‹¤íŒ¨: {ocr_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                
                        # departments_with_pages_by_date ì„¸ì…˜ ìƒíƒœ ëª…ì‹œì  ì—…ë°ì´íŠ¸ ì¶”ê°€
                        if "departments_with_pages" in metadata:
                            st.session_state.departments_with_pages_by_date[pdf_date] = metadata["departments_with_pages"]
                
                progress_bar_pdf.progress(i / total_pdfs)

            status_text_pdf.empty()
            progress_bar_pdf.empty()
            st.success(f"ì´ {total_pdfs}ê°œì˜ PDF íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        # -------------------------------------
        
        # --- 6. ëˆ„ì  ë¶ˆì¼ì¹˜ ë°ì´í„° ê³„ì‚° ë° ì €ì¥ ---

        if 'excel_data' in st.session_state and not st.session_state.excel_data.empty:
            try:
                # ë¶ˆì¼ì¹˜ ë°ì´í„° ìƒì„±
                new_mismatch_result = data_analyzer.find_mismatches(st.session_state.excel_data)
                if new_mismatch_result["status"] == "success":
                    new_mismatch_data = new_mismatch_result["data"]

                    # ì œì™¸í•  ë¬¼í’ˆì½”ë“œ ì œê±° (í•˜ë“œì½”ë”©)
                    excluded_item_codes = [
                        'L505001', 'L505002', 'L505003', 'L505004', 'L505005', 'L505006', 'L505007', 
                        'L505008', 'L505009', 'L505010', 'L505011', 'L505012', 'L505013', 'L505014',
                        'L605001', 'L605002', 'L605003', 'L605004', 'L605005', 'L605006'
                    ]
                    if not new_mismatch_data.empty and 'ë¬¼í’ˆì½”ë“œ' in new_mismatch_data.columns:
                        new_mismatch_data = new_mismatch_data[
                            ~new_mismatch_data['ë¬¼í’ˆì½”ë“œ'].astype(str).isin(excluded_item_codes)
                        ]

                    # ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ í•„í„°ë§ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
                    completion_logs = st.session_state.get('completion_logs', [])
                    if not new_mismatch_data.empty and completion_logs:
                        new_mismatch_data = filter_completed_items(new_mismatch_data, completion_logs)

                    st.session_state.mismatch_data = new_mismatch_data.reset_index(drop=True)

                    # ğŸš©ğŸš©ğŸš© [ì¤‘ìš”] ë‚ ì§œë³„ë¡œ ê°ê° S3ì— ì €ì¥ (YYYY-MM-DD í´ë”ì—!)
                    save_all_date_mismatches(s3_handler, st.session_state.mismatch_data)

                else:
                    st.session_state.mismatch_data = pd.DataFrame()
                    st.warning(f"ìƒˆ ë¶ˆì¼ì¹˜ ë°ì´í„° ê³„ì‚° ì‹¤íŒ¨: {new_mismatch_result['message']}")
            except Exception as e:
                st.session_state.mismatch_data = pd.DataFrame()
                logger.error(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ê³„ì‚°/ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                st.error("ë¶ˆì¼ì¹˜ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ê±°ë‚˜ ì €ì¥í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        else:
            st.session_state.mismatch_data = pd.DataFrame()
            logger.info("ì—‘ì…€ ë°ì´í„°ê°€ ì—†ì–´ ë¶ˆì¼ì¹˜ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        # -------------------------------------
        
        # --- 7. ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ---
        final_processed_dates = sorted(list(processed_dates))
        for date_str in final_processed_dates:
            try:
                metadata_result = s3_handler.load_metadata(date_str)
                if metadata_result["status"] == "success":
                    metadata = metadata_result["data"]
                else:
                    metadata = {} # ê¸°ì¡´ ë©”íƒ€ë°ì´í„° ì—†ìŒ
                
                # ì—‘ì…€ ì •ë³´ ì—…ë°ì´íŠ¸ (ëˆ„ì  íŒŒì¼ ê¸°ì¤€)
                metadata["excel_key"] = cumulative_excel_key
                metadata["excel_hash"] = cumulative_excel_hash # ìœ„ì—ì„œ ê³„ì‚°í•œ ëˆ„ì  í•´ì‹œ
                metadata["excel_processed_files"] = newly_processed_excel_files # ì´ë²ˆ ì‹¤í–‰ì—ì„œ ì²˜ë¦¬í•œ íŒŒì¼ ëª©ë¡
                
                # PDF ì •ë³´ ì—…ë°ì´íŠ¸ (ì´ë¯¸ ì²˜ë¦¬ëœ ì •ë³´ê°€ ìˆë‹¤ë©´ ìœ ì§€)
                if date_str in st.session_state.pdf_paths_by_date:
                    metadata["pdf_key"] = st.session_state.pdf_paths_by_date[date_str]
                if date_str in st.session_state.ocr_results_by_date:
                    ocr_data = st.session_state.ocr_results_by_date[date_str]
                    metadata["pdf_filename"] = metadata.get("pdf_filename", "N/A") # ì´ì „ ê°’ ìœ ì§€ ì‹œë„
                    metadata["ocr_pages"] = len(ocr_data.get("ocr_text", []))
                    metadata["departments_with_pages"] = ocr_data.get("departments_with_pages", [])
                
                metadata["processed_date"] = datetime.now().isoformat()
                s3_handler.save_metadata(date_str, metadata)
                logger.debug(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ: {date_str}")
            except Exception as e:
                logger.error(f"ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì‹¤íŒ¨ ({date_str}): {e}", exc_info=True)
                st.warning(f"{date_str} ë‚ ì§œì˜ ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ")
        # -------------------------------------
        
        # --- 8. ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡ ì—…ë°ì´íŠ¸ ë° ë§ˆë¬´ë¦¬ --- 
        st.session_state.available_dates = final_processed_dates
        # logger.info(f"ìµœì¢… ì²˜ë¦¬ëœ ë‚ ì§œ ëª©ë¡ ì—…ë°ì´íŠ¸: {len(st.session_state.available_dates)}ê°œ") # ì¤‘ë³µ ë¡œê·¸ ì œê±°

        # PDF ì²˜ë¦¬ ê²°ê³¼ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
        for date in final_processed_dates:
            if date in st.session_state.pdf_paths_by_date:
                logger.info(f"ë‚ ì§œ {date}ì˜ PDF ê²½ë¡œ: {st.session_state.pdf_paths_by_date[date]}")
            if date in st.session_state.departments_with_pages_by_date:
                dept_pages = st.session_state.departments_with_pages_by_date[date]
                # logger.info(f"ë‚ ì§œ {date}ì˜ ë¶€ì„œ-í˜ì´ì§€ ì •ë³´: {len(dept_pages)}ê°œ í•­ëª©") # ì¤‘ë³µ ë¡œê·¸ ì œê±°
            else:
                logger.warning(f"ë‚ ì§œ {date}ì˜ ë¶€ì„œ-í˜ì´ì§€ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

        if final_processed_dates:
            # ê°€ì¥ ìµœê·¼ ë‚ ì§œë¡œ ì„ íƒ ì—…ë°ì´íŠ¸ (ì„ íƒì )
            new_date = max(final_processed_dates)
            st.session_state.selected_date = new_date
            # logger.info(f"ê°€ì¥ ìµœê·¼ ì²˜ë¦¬ ë‚ ì§œë¡œ ì„ íƒ ì—…ë°ì´íŠ¸: {new_date}") # ì¤‘ë³µ ë¡œê·¸ ì œê±°

        st.success("ëª¨ë“  íŒŒì¼ ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        st.session_state.force_reload_mismatch = True
    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        logger.exception("íŒŒì¼ ì²˜ë¦¬ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ")
        # ---------------------------------------------

    except Exception as e:
        st.error(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        logger.exception("íŒŒì¼ ì²˜ë¦¬ ì¤‘ ìµœìƒìœ„ ì˜¤ë¥˜ ë°œìƒ")

@st.cache_data(ttl=3600)
def get_pdf_preview_image_from_s3(file_key):
    s3_handler = S3Handler()
    result = s3_handler.download_file(file_key)
    if result["status"] == "success":
        return result["data"]
    return None

# ë¶ˆì¼ì¹˜ ë¦¬ìŠ¤íŠ¸ íƒ­ í‘œì‹œ í•¨ìˆ˜
def display_mismatch_tab(): # selected_date ì¸ì ì œê±°
    """ë‚ ì§œë³„ ì‘ì—… íƒ­ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    try:
        # --- ë‚ ì§œ ì„ íƒ UI (íƒ­ ë‚´ë¶€) ---
        st.header("ë‚ ì§œë³„ ì‘ì—… ìƒì„¸ ì¡°íšŒ")
        if not st.session_state.get("available_dates"):
            st.warning("ì‚¬ì´ë“œë°”ì—ì„œ ì‘ì—… ê¸°ê°„ì„ ë¨¼ì € ì„¤ì •í•˜ê³  íŒŒì¼ì„ ì²˜ë¦¬í•´ì£¼ì„¸ìš”.")
            return

        selected_date_in_tab = st.selectbox(
            "ì‘ì—…í•  ë‚ ì§œë¥¼ ì„ íƒí•˜ì„¸ìš”:",
            st.session_state.available_dates,
            key="selected_date_in_mismatch_tab"
        )

        if not selected_date_in_tab:
            st.info("ë‚ ì§œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")
            return
        
        # ì„ íƒëœ ë‚ ì§œì— ëŒ€í•œ ë°ì´í„° ë¡œë“œ (mainì—ì„œ ì´ë™)
        if selected_date_in_tab not in st.session_state.get('loaded_dates', set()):
            with st.spinner(f"{selected_date_in_tab} ë‚ ì§œì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘..."):
                result = load_data_for_date(selected_date_in_tab)
                if result.get("status") == "success":
                    if 'loaded_dates' not in st.session_state:
                        st.session_state.loaded_dates = set()
                    st.session_state.loaded_dates.add(selected_date_in_tab)
                    # ì„±ê³µ ë©”ì‹œì§€ ì œê±°í•˜ì—¬ ì¤‘ë³µ ìƒˆë¡œê³ ì¹¨ ë°©ì§€
                else:
                    st.warning(f"{selected_date_in_tab} ë‚ ì§œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {result.get('message')}")
        
        # PDF ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (S3ì—ì„œ ì§ì ‘ í™•ì¸)
        s3_handler = S3Handler()
        
        # 1. ì„¸ì…˜ ìƒíƒœì—ì„œ ë¨¼ì € í™•ì¸
        pdf_exists_in_session = selected_date_in_tab in st.session_state.get('pdf_paths_by_date', {})
        
        # 2. ì„¸ì…˜ì— ì—†ìœ¼ë©´ S3 ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸
        pdf_exists_in_s3 = False
        if not pdf_exists_in_session:
            metadata_result = s3_handler.load_metadata(selected_date_in_tab)
            if metadata_result["status"] == "success":
                metadata = metadata_result["data"]
                if "pdf_key" in metadata:
                    # ì„¸ì…˜ì— PDF ê²½ë¡œ ì €ì¥
                    st.session_state.pdf_paths_by_date[selected_date_in_tab] = metadata["pdf_key"]
                    pdf_exists_in_s3 = True
                    
                    # ë¶€ì„œ-í˜ì´ì§€ ì •ë³´ë„ ì„¸ì…˜ì— ì €ì¥
                    if "departments_with_pages" in metadata:
                        st.session_state.departments_with_pages_by_date[selected_date_in_tab] = metadata["departments_with_pages"]
        
        # PDF ì¡´ì¬ ì—¬ë¶€ í‘œì‹œ
        if pdf_exists_in_session or pdf_exists_in_s3:
            st.success(f"ì„ íƒëœ ë‚ ì§œ({selected_date_in_tab})ì— PDF íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤.")
        else:
            st.warning(f"ì„ íƒëœ ë‚ ì§œ({selected_date_in_tab})ì— PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        # --- ë‚ ì§œ ì„ íƒ UI ë ---
        
        if 'mismatch_data' not in st.session_state or st.session_state.mismatch_data.empty:
            st.info("ì²˜ë¦¬ëœ ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        # 2) ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª© í•„í„°ë§ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
        completion_logs = st.session_state.get('completion_logs', [])
        if completion_logs:
            filtered_mismatch_data = filter_completed_items(st.session_state.mismatch_data, completion_logs)
        else:
            filtered_mismatch_data = st.session_state.mismatch_data
            
        # 3) ë‚ ì§œë³„ í•„í„°ë§
        # ë‚ ì§œ ì»¬ëŸ¼ì´ ë¬¸ìì—´ì¸ ê²½ìš° datetimeìœ¼ë¡œ ë³€í™˜
        if not pd.api.types.is_datetime64_any_dtype(filtered_mismatch_data['ë‚ ì§œ']):
            filtered_mismatch_data['ë‚ ì§œ'] = pd.to_datetime(filtered_mismatch_data['ë‚ ì§œ'], format='%Y-%m-%d', errors='coerce')
            
        df_date = filtered_mismatch_data[
            filtered_mismatch_data['ë‚ ì§œ'].dt.strftime('%Y-%m-%d') == selected_date_in_tab # selected_date_in_tab ì‚¬ìš©
        ].copy()
        
        if df_date.empty:
            st.info(f"ì„ íƒëœ ë‚ ì§œ({selected_date_in_tab})ì— í•´ë‹¹í•˜ëŠ” ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.") # selected_date_in_tab ì‚¬ìš©
            # ì´ ê²½ìš°ì—ë„ íŠ¹ì • ë¶€ì„œ íƒ­ìœ¼ë¡œ ë°”ë¡œ ë„˜ì–´ê°ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì „ì²´ íƒ­ì— ëŒ€í•œ ì²˜ë¦¬ëŠ” ê³„ì† ì§„í–‰
            # return # ì—¬ê¸°ì„œ ë¦¬í„´í•˜ë©´ ì „ì²´ íƒ­ ë° ë¶€ì„œë³„ íƒ­ì´ ì•„ì˜ˆ ì•ˆ ë‚˜ì˜´
            
        dept_result = get_unique_departments(df_date)
        if dept_result["status"] == "error":
            st.error(dept_result["message"])
            return
            
        dept_options = dept_result["data"]
        
        # 5) ë¶€ì„œë³„ ì„œë¸Œíƒ­ ìƒì„±
        dept_tabs = st.tabs(["ì „ì²´"] + dept_options)
        
        # ì „ì²´ íƒ­ (ì¼ê´„ ì²˜ë¦¬ + ë¶€ì„œ ë¹„êµ ì „ìš©)
        with dept_tabs[0]:
            st.subheader("ğŸ“‹ ì„ íƒ í•­ëª© ê´€ë¦¬")
            
            # ì„ íƒ ìƒíƒœ ìš”ì•½ í‘œì‹œ (ìë™ ê°±ì‹ )
            selected_count_by_dept = {}
            total_selected = 0
            
            # ê° ë¶€ì„œë³„ë¡œ ì„ íƒëœ í•­ëª© ìˆ˜ ê³„ì‚° (í†µí•©ëœ í‚¤ ì‚¬ìš©)
            for dept in dept_options:
                count = 0
                dept_data = df_date[df_date['ë¶€ì„œëª…'] == dept]
                for idx, row in dept_data.iterrows():
                    try:
                        date_val = pd.to_datetime(row.get('ë‚ ì§œ', 'N/A')).strftime('%Y-%m-%d')
                    except:
                        date_val = str(row.get('ë‚ ì§œ', 'N/A'))
                    dept_key_val = str(row.get('ë¶€ì„œëª…', 'N/A'))
                    code_key_val = str(row.get('ë¬¼í’ˆì½”ë“œ', 'N/A'))
                    # ë¶€ì„œë³„ íƒ­ê³¼ ë™ì¼í•œ í‚¤ í˜•ì‹ ì‚¬ìš© (ë¶€ì„œ ì ‘ë¯¸ì‚¬ ì œê±°)
                    state_key = f"sel_{date_val}_{dept_key_val}_{code_key_val}"
                    
                    if st.session_state.get(state_key, False):
                        count += 1
                        total_selected += 1
                        
                selected_count_by_dept[dept] = count
            
            # ì„ íƒ ì €ì¥ ìƒíƒœ í™•ì¸
            saved_selections = st.session_state.get('saved_selections', {})
            saved_count = sum(saved_selections.values())
            
            col1, col2, col3 = st.columns([1, 1, 1])
            with col1:
                st.metric("ì´ ì„ íƒ í•­ëª©", f"{total_selected}ê°œ")
            with col2:
                st.metric("ì €ì¥ëœ ì„ íƒ", f"{saved_count}ê°œ")
            with col3:
                if total_selected > 0:
                    if saved_count == 0:
                        st.warning("âš ï¸ ì„ íƒ ì €ì¥ í•„ìš”")
                    elif saved_count < total_selected:
                        st.info("ğŸ’¡ ì¼ë¶€ ì €ì¥ë¨")
                    else:
                        st.success("âœ… ëª¨ë‘ ì €ì¥ë¨")
            
            # ë¶€ì„œë³„ ì„ íƒ ìƒíƒœ í‘œì‹œ
            if total_selected > 0 or saved_count > 0:
                dept_summary = []
                for dept, count in selected_count_by_dept.items():
                    if count > 0:
                        # í•´ë‹¹ ë¶€ì„œì˜ ì €ì¥ ìƒíƒœ í™•ì¸
                        dept_key = f"{selected_date_in_tab}_{dept}"
                        saved_for_dept = saved_selections.get(dept_key, 0)
                        if saved_for_dept > 0:
                            dept_summary.append(f"{dept}: {count}ê°œ (ì €ì¥ë¨: {saved_for_dept}ê°œ)")
                        else:
                            dept_summary.append(f"{dept}: {count}ê°œ (ë¯¸ì €ì¥)")
                
                if dept_summary:
                    st.info("ë¶€ì„œë³„ ì„ íƒ: " + ", ".join(dept_summary))
                
                # ì¼ê´„ ì™„ë£Œ ì²˜ë¦¬ ë²„íŠ¼ (ì €ì¥ëœ ì„ íƒì´ ìˆì„ ë•Œë§Œ í™œì„±í™”)
                if st.button("ğŸš€ ëª¨ë“  ë¶€ì„œ ì„ íƒ í•­ëª© ì¼ê´„ ì™„ë£Œ ì²˜ë¦¬", 
                           type="primary", 
                           key="batch_complete_all",
                           disabled=(saved_count == 0),
                           help="ê° ë¶€ì„œì—ì„œ 'ì„ íƒ ì €ì¥'ì„ ë¨¼ì € ëˆŒëŸ¬ì£¼ì„¸ìš”" if saved_count == 0 else "ì €ì¥ëœ ì„ íƒ í•­ëª©ì„ ì¼ê´„ ì™„ë£Œ ì²˜ë¦¬í•©ë‹ˆë‹¤"):
                    with st.spinner("ì¼ê´„ ì™„ë£Œ ì²˜ë¦¬ ì¤‘... (S3 ì €ì¥ ë° í†µí•© ì‘ì—… ìˆ˜í–‰)"):
                        all_completed_items = []
                        all_indices_to_remove = []
                        
                        # ëª¨ë“  ë¶€ì„œì˜ ì„ íƒëœ í•­ëª© ìˆ˜ì§‘ (í†µí•©ëœ í‚¤ ì‚¬ìš©)
                    for dept in dept_options:
                        dept_data = df_date[df_date['ë¶€ì„œëª…'] == dept]
                        for idx, row in dept_data.iterrows():
                            try:
                                date_val = pd.to_datetime(row.get('ë‚ ì§œ', 'N/A')).strftime('%Y-%m-%d')
                            except:
                                date_val = str(row.get('ë‚ ì§œ', 'N/A'))
                            dept_key_val = str(row.get('ë¶€ì„œëª…', 'N/A'))
                            code_key_val = str(row.get('ë¬¼í’ˆì½”ë“œ', 'N/A'))
                            # ë¶€ì„œë³„ íƒ­ê³¼ ë™ì¼í•œ í‚¤ í˜•ì‹ ì‚¬ìš© (ë¶€ì„œ ì ‘ë¯¸ì‚¬ ì œê±°)
                            state_key = f"sel_{date_val}_{dept_key_val}_{code_key_val}"
                            
                            if st.session_state.get(state_key, False):
                                original_idx = row.get('original_index', idx)
                                all_indices_to_remove.append(original_idx)
                                all_completed_items.append({
                                    'ë‚ ì§œ': date_val,
                                    'ë¶€ì„œëª…': dept_key_val,
                                    'ë¬¼í’ˆì½”ë“œ': code_key_val,
                                    'ë¬¼í’ˆëª…': row.get('ë¬¼í’ˆëª…', 'N/A'),
                                    'ì²­êµ¬ëŸ‰': row.get('ì²­êµ¬ëŸ‰', 0),
                                    'ìˆ˜ë ¹ëŸ‰': row.get('ìˆ˜ë ¹ëŸ‰', 0),
                                    'ì°¨ì´': row.get('ì°¨ì´', 0),
                                    'ëˆ„ë½': row.get('ëˆ„ë½', ''),
                                    'ì²˜ë¦¬ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                    'original_index': original_idx
                                })
                                # ì„ íƒ ìƒíƒœ ì´ˆê¸°í™”
                                if state_key in st.session_state:
                                    del st.session_state[state_key]
                    
                    # ì¼ê´„ ì²˜ë¦¬ ì‹¤í–‰
                    if all_indices_to_remove:
                        # mismatch_dataì—ì„œ ì œê±°
                        st.session_state.mismatch_data = st.session_state.mismatch_data.drop(
                            all_indices_to_remove
                        ).reset_index(drop=True)
                        
                        # ë‚ ì§œë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ S3ì— ì €ì¥ (í†µí•© ì‘ì—… ì œê±°ë¡œ ì†ë„ í–¥ìƒ)
                        save_all_date_mismatches(s3_handler, st.session_state.mismatch_data)
                        
                        # ì „ì²´ í†µí•© íŒŒì¼ ì—…ë°ì´íŠ¸ ì œê±° (ë¶€ì„œë³„ í†µê³„ íƒ­ì—ì„œ ìˆ˜ë™ ë³‘í•©)
                        # s3_handler.update_full_mismatches_json()  # ì£¼ì„ ì²˜ë¦¬
                        
                        # ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥
                        if all_completed_items:
                            log_result = s3_handler.save_completion_log(all_completed_items)
                            if log_result["status"] != "success":
                                st.warning("ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                            # ì„¸ì…˜ ìƒíƒœì—ë„ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì¶”ê°€
                            if 'completion_logs' not in st.session_state:
                                st.session_state.completion_logs = []
                            st.session_state.completion_logs.extend(all_completed_items)
                            
                            # ì¤‘ë³µ ì œê±°
                            if st.session_state.completion_logs:
                                temp_df = pd.DataFrame(st.session_state.completion_logs)
                                if 'ì²˜ë¦¬ì‹œê°„' in temp_df.columns:
                                    temp_df['ì²˜ë¦¬ì‹œê°„'] = pd.to_datetime(temp_df['ì²˜ë¦¬ì‹œê°„'])
                                    temp_df = temp_df.sort_values('ì²˜ë¦¬ì‹œê°„', ascending=False)
                                    temp_df = temp_df.drop_duplicates(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='first')
                                    st.session_state.completion_logs = temp_df.to_dict('records')
                        
                        # ì„ íƒ ì €ì¥ í”Œë˜ê·¸ ëª¨ë‘ ì •ë¦¬
                        if 'saved_selections' in st.session_state:
                            st.session_state.saved_selections.clear()
                    
                    if all_indices_to_remove:
                        st.success(f"âœ… ì´ {len(all_indices_to_remove)}ê°œ í•­ëª©ì´ ì¼ê´„ ì™„ë£Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤! (ë‚ ì§œë³„ ì €ì¥ ì™„ë£Œ)")
                        st.info("ğŸ’¡ ë¶€ì„œë³„ í†µê³„ë¥¼ ë³´ë ¤ë©´ 'ë‚ ì§œë³„ ì‘ì—… ë‚´ìš© ë³‘í•©' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                        st.balloons()
                    else:
                        st.warning("ì„ íƒëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            else:
                st.info("ğŸ’¡ ê° ë¶€ì„œ íƒ­ì—ì„œ ì™„ë£Œ ì²˜ë¦¬í•  í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
            
            st.markdown("---")
            
            # ì—‘ì…€ê³¼ PDFì˜ ë¶€ì„œ ë¹„êµ
            st.subheader("ğŸ“‹ PDF & ì—‘ì…€ ë¶€ì„œ ë¹„êµ")
            try:
                # ì—‘ì…€ì˜ ë¶€ì„œ ëª©ë¡
                excel_depts = set()
                if 'excel_data' in st.session_state and not st.session_state.excel_data.empty:
                    # ë‚ ì§œ í˜•ì‹ ë³€í™˜
                    if not pd.api.types.is_datetime64_any_dtype(st.session_state.excel_data['ë‚ ì§œ']):
                        st.session_state.excel_data['ë‚ ì§œ'] = pd.to_datetime(st.session_state.excel_data['ë‚ ì§œ'], format='%Y-%m-%d', errors='coerce')
                    
                    excel_date_data = st.session_state.excel_data[
                        st.session_state.excel_data['ë‚ ì§œ'].dt.strftime('%Y-%m-%d') == selected_date_in_tab
                    ]
                    excel_depts = set(excel_date_data['ë¶€ì„œëª…'].unique())
                
                # PDFì˜ ë¶€ì„œ ëª©ë¡
                pdf_depts = set()
                if selected_date_in_tab in st.session_state.get('departments_with_pages_by_date', {}):
                    dept_page_tuples = st.session_state.departments_with_pages_by_date[selected_date_in_tab]
                    pdf_depts = {dept for dept, page in dept_page_tuples}
                
                # ë¶€ì„œ ë¹„êµ ê²°ê³¼ í‘œì‹œ
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("ì—‘ì…€ ë¶€ì„œ ìˆ˜", len(excel_depts))
                with col2:
                    st.metric("PDF ë¶€ì„œ ìˆ˜", len(pdf_depts))
                with col3:
                    common_depts = excel_depts & pdf_depts
                    st.metric("ê³µí†µ ë¶€ì„œ ìˆ˜", len(common_depts))
                
                # PDFì—ë§Œ ìˆëŠ” ë¶€ì„œ (ëˆ„ë½ëœ ë¶€ì„œ)
                pdf_only_depts = pdf_depts - excel_depts
                if pdf_only_depts:
                    st.warning(f"âš ï¸ PDFì—ë§Œ ìˆëŠ” ë¶€ì„œ ({len(pdf_only_depts)}ê°œ)")
                    st.write("**ëˆ„ë½ëœ ë¶€ì„œ ëª©ë¡:**", ", ".join(sorted(pdf_only_depts)))
                    
                    # ëˆ„ë½ëœ ë¶€ì„œì˜ PDF ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
                    st.subheader("ğŸ“„ ëˆ„ë½ëœ ë¶€ì„œ PDF ë¯¸ë¦¬ë³´ê¸°")
                    for dept in sorted(pdf_only_depts):
                        with st.expander(f"ğŸ“ {dept} ë¶€ì„œ PDF ë¯¸ë¦¬ë³´ê¸°"):
                            dept_pages = get_department_pages(selected_date_in_tab, dept)
                            if dept_pages:
                                # PDF ì›ë³¸ ë‹¤ìš´ë¡œë“œ
                                pdf_key = st.session_state.pdf_paths_by_date.get(selected_date_in_tab)
                                if pdf_key:
                                    pdf_result = s3_handler.download_file(pdf_key)
                                    if pdf_result["status"] == "success":
                                        pdf_bytes = pdf_result["data"]
                                        
                                        # ë¶€ì„œì˜ ê° í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸° í‘œì‹œ
                                        cols = st.columns(min(2, len(dept_pages)))
                                        for i, page_num in enumerate(dept_pages[:2]):  # ìµœëŒ€ 2ê°œ í˜ì´ì§€ë§Œ í‘œì‹œ
                                            with cols[i % 2]:
                                                img = extract_pdf_preview(
                                                    io.BytesIO(pdf_bytes), 
                                                    page_num-1, 
                                                    dpi=100, 
                                                    thumbnail_size=(400, 600)
                                                )
                                                if img:
                                                    st.image(img, caption=f"í˜ì´ì§€ {page_num}", width=300)
                                        
                                        if len(dept_pages) > 2:
                                            st.info(f"ì´ {len(dept_pages)}ê°œ í˜ì´ì§€ ì¤‘ 2ê°œë§Œ í‘œì‹œë¨")
                            else:
                                st.info("í•´ë‹¹ ë¶€ì„œì˜ í˜ì´ì§€ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                # ì—‘ì…€ì—ë§Œ ìˆëŠ” ë¶€ì„œ
                excel_only_depts = excel_depts - pdf_depts
                if excel_only_depts:
                    st.info(f"â„¹ï¸ ì—‘ì…€ì—ë§Œ ìˆëŠ” ë¶€ì„œ ({len(excel_only_depts)}ê°œ): {', '.join(sorted(excel_only_depts))}")
                
                if not pdf_only_depts and not excel_only_depts:
                    st.success("âœ… ëª¨ë“  ë¶€ì„œê°€ ì—‘ì…€ê³¼ PDFì— ì¼ì¹˜í•©ë‹ˆë‹¤!")
                    
            except Exception as e:
                logger.error(f"ë¶€ì„œ ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
                st.error("ë¶€ì„œ ë¹„êµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        
        # ê° ë¶€ì„œë³„ íƒ­
        for i, dept in enumerate(dept_options, 1):
            with dept_tabs[i]:
                df_filtered_dept = df_date[df_date['ë¶€ì„œëª…'] == dept].copy() # df_filtered ëŒ€ì‹  df_filtered_dept ì‚¬ìš©
                
                st.subheader("PDF & ì—‘ì…€ í’ˆëª© ë¹„êµ")
                try:
                    excel_items_result = get_excel_items(selected_date_in_tab, dept) # selected_date_in_tab ì‚¬ìš©
                    if excel_items_result["status"] == "success":
                        excel_data_from_func = excel_items_result["data"]
                        
                        if isinstance(excel_data_from_func, pd.DataFrame):
                            excel_df = excel_data_from_func
                        elif isinstance(excel_data_from_func, list):
                            logger.warning(f"get_excel_itemsê°€ listë¥¼ ë°˜í™˜ (ë¶€ì„œ: {dept}). DataFrame ë³€í™˜ ì‹œë„.")
                            try:
                                if excel_data_from_func and isinstance(excel_data_from_func[0], str):
                                    excel_df = pd.DataFrame(excel_data_from_func, columns=['ë¬¼í’ˆì½”ë“œ'])
                                    logger.info(f"ë‹¨ìˆœ listë¥¼ 'ë¬¼í’ˆì½”ë“œ' ì»¬ëŸ¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ë¶€ì„œ: {dept})")
                                else:
                                    excel_df = pd.DataFrame(excel_data_from_func)
                                
                                if excel_df.empty and excel_data_from_func:
                                    logger.warning(f"ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë¹ˆ DataFrame ìƒì„± (ë¶€ì„œ: {dept}). ì˜ˆìƒ ì»¬ëŸ¼ìœ¼ë¡œ ì¬ìƒì„±.")
                                    excel_df = pd.DataFrame(columns=['ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰'])
                                elif not excel_df.empty and 'ë¬¼í’ˆì½”ë“œ' not in excel_df.columns:
                                    logger.warning(f"ìƒì„±ëœ DataFrameì— 'ë¬¼í’ˆì½”ë“œ' ì»¬ëŸ¼ ì—†ìŒ (ë¶€ì„œ: {dept}). ì˜ˆìƒ ì»¬ëŸ¼ìœ¼ë¡œ ì¬ìƒì„±.")
                                    excel_df = pd.DataFrame(columns=['ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰'])

                            except Exception as e:
                                logger.error(f"listë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ (ë¶€ì„œ: {dept}): {e}")
                                excel_df = pd.DataFrame(columns=['ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰'])
                        else:
                            logger.warning(f"get_excel_itemsê°€ ì˜ˆìƒì¹˜ ì•Šì€ íƒ€ì…({type(excel_data_from_func)})ì„ ë°˜í™˜ (ë¶€ì„œ: {dept}). ë¹ˆ DataFrame ì‚¬ìš©.")
                            excel_df = pd.DataFrame(columns=['ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰'])
                        
                        pdf_item_set = get_department_items(selected_date_in_tab, dept) # selected_date_in_tab ì‚¬ìš©
                        
                        if pdf_item_set: 
                            pdf_df = pd.DataFrame({"ë¬¼í’ˆì½”ë“œ": list(pdf_item_set)})
                        else: 
                            pdf_df = pd.DataFrame(columns=["ë¬¼í’ˆì½”ë“œ"])

                        if not pdf_df.empty:
                            if not excel_df.empty and 'ë¬¼í’ˆì½”ë“œ' in excel_df.columns: 
                                pdf_only_codes = pdf_df[~pdf_df['ë¬¼í’ˆì½”ë“œ'].isin(excel_df['ë¬¼í’ˆì½”ë“œ'])]['ë¬¼í’ˆì½”ë“œ'].tolist()
                            else: 
                                pdf_only_codes = pdf_df['ë¬¼í’ˆì½”ë“œ'].tolist()
                            
                            if pdf_only_codes:
                                missing_from_excel_items = []
                                item_db = st.session_state.get("item_db", {}) 
                                for code in pdf_only_codes:
                                    item_name = item_db.get(code, "ì•Œ ìˆ˜ ì—†ëŠ” ë¬¼í’ˆ") 
                                    pdf_quantity = 1 
                                    missing_item_details = {
                                        'ë‚ ì§œ': selected_date_in_tab, 'ë¶€ì„œëª…': dept, 'ë¬¼í’ˆì½”ë“œ': code, # selected_date_in_tab
                                        'ë¬¼í’ˆëª…': item_name, 'ì²­êµ¬ëŸ‰': 0, 'ìˆ˜ë ¹ëŸ‰': pdf_quantity, 
                                        'ì°¨ì´': pdf_quantity, 'ëˆ„ë½': 'ì „ì‚°ëˆ„ë½' 
                                    }
                                    missing_from_excel_items.append(missing_item_details)
                                if missing_from_excel_items:
                                    missing_df = pd.DataFrame(missing_from_excel_items)
                                    # df_filtered_dept = pd.concat([df_filtered_dept, missing_df], ignore_index=True).drop_duplicates(
                                    #     subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='last'
                                    # ) # ë°”ë¡œ í•©ì¹˜ì§€ ì•ŠìŒ

                                    # ì‚¬ìš©ìì—ê²Œ ê°ì§€ëœ ëˆ„ë½ í•­ëª© ë³´ì—¬ì£¼ê¸°
                                    with st.expander(f"ìë™ ê°ì§€ëœ ì „ì‚°ëˆ„ë½ í›„ë³´ ({len(missing_df)}ê°œ) - ê²€í†  í›„ ì €ì¥í•˜ì„¸ìš”", expanded=True):
                                        st.info("ì•„ë˜ ëª©ë¡ì€ PDF ì¸ìˆ˜ì¦ì—ëŠ” ìˆì§€ë§Œ ì—‘ì…€ ë°ì´í„°ì—ëŠ” ì—†ëŠ” í’ˆëª©ë“¤ì…ë‹ˆë‹¤. ê²€í†  í›„ 'ì „ì‚°ëˆ„ë½ ì €ì¥' ë²„íŠ¼ì„ ëˆŒëŸ¬ì•¼ í†µê³„ì— ë°˜ì˜ë©ë‹ˆë‹¤.")
                                        st.dataframe(missing_df[['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ìˆ˜ë ¹ëŸ‰']].rename(columns={'ìˆ˜ë ¹ëŸ‰':'PDFìˆ˜ëŸ‰'}))

                                        form_key_missing_save = f"form_missing_save_{selected_date_in_tab}_{dept}"
                                        with st.form(key=form_key_missing_save):
                                            save_detected_missing_button = st.form_submit_button("âœ… ìœ„ ì „ì‚°ëˆ„ë½ í›„ë³´ ì €ì¥")

                                            if save_detected_missing_button:
                                                try:
                                                    s3_handler = S3Handler()
                                                    result = s3_handler.save_missing_items_by_date(missing_df, date_str=selected_date_in_tab)
                                                    if result["status"] == "success":
                                                        # ì„¸ì…˜ ìƒíƒœì˜ mismatch_dataë„ ì—…ë°ì´íŠ¸
                                                        if 'mismatch_data' not in st.session_state:
                                                            st.session_state.mismatch_data = pd.DataFrame()
                                                        
                                                        # ê¸°ì¡´ ë°ì´í„°ì™€ ìƒˆ ì „ì‚°ëˆ„ë½ ë°ì´í„° ë³‘í•©
                                                        combined_session = pd.concat([st.session_state.mismatch_data, missing_df], ignore_index=True)
                                                        # ì¤‘ë³µ ì œê±°
                                                        combined_session = combined_session.drop_duplicates(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='last')
                                                        st.session_state.mismatch_data = combined_session
                                                        
                                                        st.success(f"{len(missing_df)}ê°œ ì „ì‚°ëˆ„ë½ í•­ëª© ì €ì¥ ì™„ë£Œ.")
                                                except Exception as e:
                                                    st.error(f"ì „ì‚°ëˆ„ë½ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}")
                                                    logger.error(f"ì „ì‚°ëˆ„ë½ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)
                           
                    else: 
                        st.error(f"'{dept}' ë¶€ì„œì˜ ì—‘ì…€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {excel_items_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
                        excel_df = pd.DataFrame(columns=['ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰']) 
                
                except Exception as e:
                    logger.error(f"PDF & ì—‘ì…€ í’ˆëª© ë¹„êµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({dept}): {e}", exc_info=True)
                    st.error("PDF & ì—‘ì…€ í’ˆëª© ë¹„êµ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
                    # ì´ ê²½ìš°ì—ë„ excel_dfê°€ ì •ì˜ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë˜ëŠ” try ë¸”ë¡ ì‹œì‘ ì „ì— ì´ˆê¸°í™” í•„ìš”
                    # df_filteredëŠ” ì´ë¯¸ ì´ tryë¸”ë¡ ì™¸ë¶€ì—ì„œ í•´ë‹¹ deptë¡œ í•„í„°ë§ëœ ë°ì´í„°ë¡œ ì¡´ì¬í•¨

                # ìµœì¢…ì ìœ¼ë¡œ df_filtered_deptë¥¼ ì‚¬ìš©í•´ display_mismatch_content í˜¸ì¶œ
                if df_filtered_dept.empty: 
                    st.info(f"{dept} ë¶€ì„œì—ëŠ” ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    display_mismatch_content(df_filtered_dept, selected_date_in_tab, dept, s3_handler) # df_filtered_dept, selected_date_in_tab
                
    except Exception as e:
        logger.error(f"display_mismatch_tab ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"ë°ì´í„° í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

def get_excel_items(date_str, dept_name):
    """
    íŠ¹ì • ë‚ ì§œì™€ ë¶€ì„œì˜ ì—‘ì…€ í’ˆëª© ì •ë³´(ë¬¼í’ˆì½”ë“œ, ë¬¼í’ˆëª…, ì²­êµ¬ëŸ‰)ë¥¼ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    (í•„ìš” ì»¬ëŸ¼ ì—†ì„ ë•Œë„ ì—ëŸ¬ ì•ˆë‚˜ê³ , í•­ìƒ ì»¬ëŸ¼ëª… ìœ ì§€)
    """
    try:
        if 'excel_data' in st.session_state and not st.session_state.excel_data.empty:
            dept_excel_data = st.session_state.excel_data[
                (st.session_state.excel_data['ë‚ ì§œ'] == date_str) &
                (st.session_state.excel_data['ë¶€ì„œëª…'] == dept_name)
            ].copy()
            # ê¸°ë³¸ ë°˜í™˜ ì»¬ëŸ¼
            required_cols = ['ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰']
            # ì‹¤ì œ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì¶”ì¶œ, ì—†ìœ¼ë©´ ë¹ˆ DF
            if not dept_excel_data.empty:
                available_cols = [col for col in required_cols if col in dept_excel_data.columns]
                if 'ë¬¼í’ˆì½”ë“œ' not in available_cols:
                    return {"status": "error", "message": "í•„ìˆ˜ ì»¬ëŸ¼ 'ë¬¼í’ˆì½”ë“œ'ê°€ ì—†ìŠµë‹ˆë‹¤."}
                return {"status": "success", "data": dept_excel_data[available_cols]}
            else:
                return {"status": "success", "data": pd.DataFrame(columns=required_cols)}
        return {"status": "error", "message": "ì„¸ì…˜ì— ì—‘ì…€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."}
    except Exception as e:
        logger.error(f"get_excel_items ì˜¤ë¥˜: {e}", exc_info=True)
        return {"status": "error", "message": str(e)}


def display_mismatch_content(df_filtered, selected_date, sel_dept, s3_handler):
    """ë¶ˆì¼ì¹˜ ë°ì´í„° í‘œì‹œ ë‚´ìš©ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜"""
    try:
        # original_index ì»¬ëŸ¼ ì¶”ê°€
        if 'original_index' not in df_filtered.columns:
            df_filtered['original_index'] = df_filtered.index
            
        if df_filtered.empty:
            st.info(f"'{sel_dept}' ë¶€ì„œì—ëŠ” ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
            
        st.markdown("**ì™„ë£Œ ì²˜ë¦¬í•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”.**")
        
        # ì„ íƒ ì €ì¥ form (ì²´í¬ë°•ìŠ¤ í¬í•¨)
        form_key_selection = f"selection_form_{selected_date}_{sel_dept}"
        with st.form(key=form_key_selection):
            # 'ëˆ„ë½' ì—´ì„ í¬í•¨í•˜ì—¬ widths ë¦¬ìŠ¤íŠ¸ ìˆ˜ì • (ì´ 9ê°œ ì—´)
            widths = [0.5, 1.2, 0.8, 0.8, 2.5, 0.7, 0.7, 0.7, 1] 
            
            # í—¤ë” í‘œì‹œ
            header_cols = st.columns(widths)
            column_names = ["ì„ íƒ", "ë‚ ì§œ", "ë¶€ì„œëª…", "ë¬¼í’ˆì½”ë“œ", "ë¬¼í’ˆëª…", "ì²­êµ¬ëŸ‰", "ìˆ˜ë ¹ëŸ‰", "ì°¨ì´", "ëˆ„ë½"]
            for i, name in enumerate(column_names):
                if i < len(header_cols):
                    header_cols[i].markdown(f"**{name}**")
            
            # ì²´í¬ë°•ìŠ¤ì™€ ë°ì´í„° í‘œì‹œ (form ì•ˆì—ì„œ)
            selected_items = []
            for idx, row in df_filtered.iterrows():
                try:
                    date_val = pd.to_datetime(row.get('ë‚ ì§œ', 'N/A')).strftime('%Y-%m-%d')
                except:
                    date_val = str(row.get('ë‚ ì§œ', 'N/A'))
                    
                dept_key_val = str(row.get('ë¶€ì„œëª…', 'N/A'))
                code_key_val = str(row.get('ë¬¼í’ˆì½”ë“œ', 'N/A'))
                # ì „ì²´ íƒ­ê³¼ ë™ì¼í•œ í‚¤ í˜•ì‹ ì‚¬ìš© (ë¶€ì„œ ì ‘ë¯¸ì‚¬ ì œê±°)
                state_key = f"sel_{date_val}_{dept_key_val}_{code_key_val}"
                
                cols = st.columns(widths)
                # labelì„ ê³ ìœ í•˜ê²Œ ë§Œë“¤ê³  ìˆ¨ê¹€ ì²˜ë¦¬
                checkbox_label = f"select_{state_key}"
                is_selected = cols[0].checkbox(
                    label=checkbox_label, 
                    key=f"{form_key_selection}_{state_key}",  # form ë‚´ë¶€ ê³ ìœ  í‚¤ ì‚¬ìš©
                    value=st.session_state.get(state_key, False),
                    label_visibility="collapsed"
                )
                
                if is_selected:
                    selected_items.append((state_key, row))
                
                try:
                    # ê° ì»¬ëŸ¼ì— í•´ë‹¹í•˜ëŠ” ê°’ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì™€ì„œ í‘œì‹œ
                    col_values = [
                        date_val,
                        dept_key_val,
                        code_key_val,
                        str(row.get('ë¬¼í’ˆëª…', row.get('í’ˆëª©', 'N/A'))),
                        str(row.get('ì²­êµ¬ëŸ‰', 'N/A')),
                        str(row.get('ìˆ˜ë ¹ëŸ‰', 'N/A')),
                        str(row.get('ì°¨ì´', 'N/A')),
                        str(row.get('ëˆ„ë½', ''))
                    ]
                    for i, value in enumerate(col_values):
                        if (i + 1) < len(cols):
                            cols[i+1].write(value)
                except Exception as row_err:
                    logger.error(f"ë¶ˆì¼ì¹˜ ë¦¬ìŠ¤íŠ¸ í–‰ ê°’ í‘œì‹œ ì˜¤ë¥˜ (ì¸ë±ìŠ¤: {idx}, ë°ì´í„°: {row.to_dict()}): {row_err}")
                    # ì˜¤ë¥˜ ë°œìƒ ì‹œ ëŒ€ì²´ í…ìŠ¤íŠ¸ í‘œì‹œ (ì„ íƒ ì—´ ì œì™¸)
                    for i in range(1, len(cols)):
                        cols[i].write("-")
            
            st.markdown("---")
            col1, col2 = st.columns([1, 1])
            with col1:
                save_selection_button = st.form_submit_button("ğŸ’¾ ì„ íƒ ì €ì¥", type="secondary", 
                                                            help="ì²´í¬ë°•ìŠ¤ ì„ íƒì„ ì„¸ì…˜ì— ì €ì¥í•©ë‹ˆë‹¤ (UI ìƒˆë¡œê³ ì¹¨ ì—†ìŒ)")
            with col2:
                immediate_complete_button = st.form_submit_button("âœ… ì¦‰ì‹œ ì™„ë£Œ ì²˜ë¦¬", type="primary",
                                                                help="ì„ íƒí•œ í•­ëª©ì„ ë°”ë¡œ ì™„ë£Œ ì²˜ë¦¬í•©ë‹ˆë‹¤ (UI ìƒˆë¡œê³ ì¹¨ ë°œìƒ)")

            # ì„ íƒ ì €ì¥ ì²˜ë¦¬ (UI ìƒˆë¡œê³ ì¹¨ ì—†ìŒ, S3 ì‘ì—… ì—†ìŒ) - ìµœì í™”ë¨
            if save_selection_button:
                # 1. ì„ íƒëœ í•­ëª©ë“¤ì˜ í‚¤ ì§‘í•© ìƒì„± (ë¹ ë¥¸ ê²€ìƒ‰ìš©)
                selected_keys = {state_key for state_key, row in selected_items}
                
                # 2. ì„ íƒëœ í•­ëª©ë“¤ì„ Trueë¡œ ì„¤ì •
                for state_key in selected_keys:
                    st.session_state[state_key] = True
                
                # 3. ì„ íƒë˜ì§€ ì•Šì€ í•­ëª©ë“¤ì„ Falseë¡œ ì„¤ì • (ìµœì í™”)
                # ë‚ ì§œ ë³€í™˜ì„ í•œ ë²ˆë§Œ ìˆ˜í–‰
                try:
                    date_val = pd.to_datetime(df_filtered['ë‚ ì§œ'].iloc[0]).strftime('%Y-%m-%d')
                except:
                    date_val = str(df_filtered['ë‚ ì§œ'].iloc[0])
                
                dept_key_val = str(df_filtered['ë¶€ì„œëª…'].iloc[0])  # ê°™ì€ ë¶€ì„œì´ë¯€ë¡œ ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
                
                # ë²¡í„°í™”ëœ í‚¤ ìƒì„±
                code_values = df_filtered['ë¬¼í’ˆì½”ë“œ'].astype(str)
                all_keys = {f"sel_{date_val}_{dept_key_val}_{code}" for code in code_values}
                
                # ì„ íƒë˜ì§€ ì•Šì€ í‚¤ë“¤ë§Œ Falseë¡œ ì„¤ì •
                unselected_keys = all_keys - selected_keys
                for key in unselected_keys:
                    st.session_state[key] = False
                
                # 4. ì„ íƒ ì €ì¥ ì™„ë£Œ í”Œë˜ê·¸ ì„¤ì • (ì „ì²´ íƒ­ì—ì„œ í™•ì¸ìš©)
                if 'saved_selections' not in st.session_state:
                    st.session_state.saved_selections = {}
                st.session_state.saved_selections[f"{selected_date}_{sel_dept}"] = len(selected_items)
                
                st.success(f"âœ… {len(selected_items)}ê°œ í•­ëª© ì„ íƒì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì „ì²´ íƒ­ì—ì„œ ì¼ê´„ ì²˜ë¦¬í•˜ì„¸ìš”.")
                st.info("ğŸ’¡ ì´ ì‘ì—…ì€ ì„¸ì…˜ì—ë§Œ ì €ì¥ë˜ë©° S3 ì‘ì—…ì´ ì—†ì–´ ë¹ ë¦…ë‹ˆë‹¤.")

            # ì¦‰ì‹œ ì™„ë£Œ ì²˜ë¦¬ (S3 ì‘ì—… í¬í•¨, ì‹œê°„ ì†Œìš”)
            if immediate_complete_button:
                if selected_items:
                    with st.spinner("ì™„ë£Œ ì²˜ë¦¬ ì¤‘... (S3 ì €ì¥ ë° í†µí•© ì‘ì—… ìˆ˜í–‰)"):
                        items_to_remove_keys = []
                        items_to_remove_indices = []
                        completed_items = []

                    for state_key, row in selected_items:
                        try:
                            date_k = pd.to_datetime(row.get('ë‚ ì§œ', 'N/A')).strftime('%Y-%m-%d')
                        except:
                            date_k = str(row.get('ë‚ ì§œ', 'N/A'))
                            
                        dept_k = str(row.get('ë¶€ì„œëª…', 'N/A'))
                        code_k = str(row.get('ë¬¼í’ˆì½”ë“œ', 'N/A'))
                        original_idx = row['original_index']
                        
                        items_to_remove_keys.append(state_key)
                        items_to_remove_indices.append(original_idx)
                        completed_items.append({
                            'ë‚ ì§œ': date_k,
                            'ë¶€ì„œëª…': dept_k,
                            'ë¬¼í’ˆì½”ë“œ': code_k,
                            'ë¬¼í’ˆëª…': row.get('ë¬¼í’ˆëª…', 'N/A'),
                            'ì²­êµ¬ëŸ‰': row.get('ì²­êµ¬ëŸ‰', 0),
                            'ìˆ˜ë ¹ëŸ‰': row.get('ìˆ˜ë ¹ëŸ‰', 0),
                            'ì°¨ì´': row.get('ì°¨ì´', 0),
                            'ëˆ„ë½': row.get('ëˆ„ë½', ''),
                            'ì²˜ë¦¬ì‹œê°„': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                            'original_index': original_idx
                        })

                    if items_to_remove_indices:
                        st.session_state.mismatch_data = st.session_state.mismatch_data.drop(items_to_remove_indices).reset_index(drop=True)
                        
                        # ë‚ ì§œë³„ë¡œ ë¶„ë¦¬í•˜ì—¬ S3ì— ì €ì¥ (í†µí•© ì‘ì—… ì œê±°ë¡œ ì†ë„ í–¥ìƒ)
                        save_all_date_mismatches(s3_handler, st.session_state.mismatch_data)
                        
                        # ì „ì²´ í†µí•© íŒŒì¼ ì—…ë°ì´íŠ¸ ì œê±° (ë¶€ì„œë³„ í†µê³„ íƒ­ì—ì„œ ìˆ˜ë™ ë³‘í•©)
                        # s3_handler.update_full_mismatches_json()  # ì£¼ì„ ì²˜ë¦¬
                        
                        if completed_items:
                            # S3ì— ì €ì¥
                            log_result = s3_handler.save_completion_log(completed_items)
                            if log_result["status"] != "success":
                                st.warning("ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                            
                            # ì„¸ì…˜ ìƒíƒœì—ë„ ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ ì¶”ê°€
                            if 'completion_logs' not in st.session_state:
                                st.session_state.completion_logs = []
                            st.session_state.completion_logs.extend(completed_items)
                            
                            # ì¤‘ë³µ ì œê±°
                            if st.session_state.completion_logs:
                                temp_df = pd.DataFrame(st.session_state.completion_logs)
                                if 'ì²˜ë¦¬ì‹œê°„' in temp_df.columns:
                                    temp_df['ì²˜ë¦¬ì‹œê°„'] = pd.to_datetime(temp_df['ì²˜ë¦¬ì‹œê°„'])
                                    temp_df = temp_df.sort_values('ì²˜ë¦¬ì‹œê°„', ascending=False)
                                    temp_df = temp_df.drop_duplicates(subset=['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ'], keep='first')
                                    st.session_state.completion_logs = temp_df.to_dict('records')
                        
                        # ì„¸ì…˜ ì •ë¦¬ (ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©ë“¤)
                        for key in items_to_remove_keys:
                            if key in st.session_state:
                                del st.session_state[key]
                        
                        # ì„ íƒ ì €ì¥ í”Œë˜ê·¸ë„ ì •ë¦¬
                        if 'saved_selections' in st.session_state:
                            key_to_remove = f"{selected_date}_{sel_dept}"
                            if key_to_remove in st.session_state.saved_selections:
                                del st.session_state.saved_selections[key_to_remove]
                    
                    st.success(f"âœ… {len(items_to_remove_indices)}ê°œ í•­ëª©ì´ ì™„ë£Œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. (ë‚ ì§œë³„ ì €ì¥ ì™„ë£Œ)")
                    st.info("ğŸ’¡ ë¶€ì„œë³„ í†µê³„ë¥¼ ë³´ë ¤ë©´ 'ë‚ ì§œë³„ ì‘ì—… ë‚´ìš© ë³‘í•©' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
                else:
                    st.warning("ì™„ë£Œ ì²˜ë¦¬í•  í•­ëª©ì„ ì„ íƒí•˜ì„¸ìš”.")

        # PDF ì„¹ì…˜ í‘œì‹œ
        st.markdown("---")
        display_pdf_section(selected_date, sel_dept, tab_prefix=f"mismatch_tab_{sel_dept}")
        
    except Exception as e:
        logger.error(f"display_mismatch_content ì˜¤ë¥˜: {e}", exc_info=True)
        st.error(f"ë°ì´í„° í‘œì‹œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def display_filter_tab():
    st.header("ë¶€ì„œë³„ í†µê³„ (ë¶ˆì¼ì¹˜ ë° ëˆ„ë½ í•­ëª©)")

    # 1. ì‚¬ì´ë“œë°” ê¸°ê°„ ì„¤ì • í™•ì¸
    if 'work_start_date' not in st.session_state or 'work_end_date' not in st.session_state:
        st.warning("ì‘ì—… ê¸°ê°„ì„ ì„¤ì •í•˜ì„¸ìš” (ì‚¬ì´ë“œë°”ì—ì„œ).")
        return
    
    s3_handler = S3Handler()
    
    # 2. ë°ì´í„° ê´€ë¦¬ ë²„íŠ¼ë“¤
    col1, col2, col3 = st.columns([2, 1, 1])
    with col2:
        if st.button("ğŸ“Š ë‚ ì§œë³„ ì‘ì—… ë‚´ìš© ë³‘í•©", help="ê° ë‚ ì§œë³„ mismatches.jsonì„ í†µí•©í•˜ì—¬ mismatches_full.json ìƒì„±"):
            with st.spinner("ë‚ ì§œë³„ ì‘ì—… ë‚´ìš©ì„ ë³‘í•©í•˜ëŠ” ì¤‘..."):
                update_result = s3_handler.update_full_mismatches_json()
                if update_result["status"] == "success":
                    st.success(f"âœ… ë‚ ì§œë³„ ì‘ì—… ë‚´ìš© ë³‘í•© ì™„ë£Œ! ì´ {update_result.get('count', 0)}ê°œ í•­ëª©")
                    # ë³‘í•© í›„ ìë™ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨ í”Œë˜ê·¸ ì„¤ì •
                    st.session_state.force_refresh = True
                else:
                    st.error(f"âŒ ë³‘í•© ì‹¤íŒ¨: {update_result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}")
    
    with col3:
        if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨", help="ìµœì‹  í†µí•© ë°ì´í„°ë¥¼ S3ì—ì„œ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤"):
            # ì„ íƒ ìƒíƒœ ë³´ì¡´ì„ ìœ„í•´ mismatch_dataëŠ” ì‚­ì œí•˜ì§€ ì•ŠìŒ
            # ëŒ€ì‹  ìºì‹œë§Œ í´ë¦¬ì–´í•˜ì—¬ ìµœì‹  ë°ì´í„° ë¡œë“œ
            # if 'mismatch_data' in st.session_state:
            #     del st.session_state.mismatch_data  # ì£¼ì„ ì²˜ë¦¬
            
            # ìºì‹œ í´ë¦¬ì–´ (í•„ìš”ì‹œ)
            st.cache_data.clear()
            
            # ìƒˆë¡œê³ ì¹¨ í”Œë˜ê·¸ ì„¤ì • (rerun ì œê±°)
            st.session_state.force_refresh = True
            st.success("ë°ì´í„° ìƒˆë¡œê³ ì¹¨ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # S3ì—ì„œ ê¸°ì¡´ í†µí•© ë°ì´í„° ë¡œë“œ (ìë™ í†µí•© ì‘ì—… ì œê±°)
    with st.spinner("S3ì—ì„œ í†µí•© ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” ì¤‘..."):
        # ê°•ì œ ìƒˆë¡œê³ ì¹¨ í”Œë˜ê·¸ ì´ˆê¸°í™” (í†µí•© ì‘ì—…ì€ ë³„ë„ ë²„íŠ¼ì—ì„œ ìˆ˜í–‰)
        if st.session_state.get('force_refresh', False):
            st.session_state.force_refresh = False
        
        # ê¸°ì¡´ í†µí•© íŒŒì¼ë§Œ ë¡œë“œ (í†µí•© ì‘ì—… ì—†ìŒ)
        df_full = s3_handler.load_full_mismatches()
        if df_full is None or df_full.empty:
            st.info("ë¶ˆì¼ì¹˜ ë˜ëŠ” ëˆ„ë½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n\në¨¼ì € 'ë‚ ì§œë³„ ì‘ì—…' íƒ­ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê±°ë‚˜ 'ë‚ ì§œë³„ ì‘ì—… ë‚´ìš© ë³‘í•©' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            return
    
    # ì‚¬ì´ë“œë°” ë‚ ì§œ ë²”ìœ„ì— í•´ë‹¹í•˜ëŠ” ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ë§Œ ì‚¬ìš©í•˜ì—¬ í•„í„°ë§
    try:
        completion_logs = st.session_state.get('completion_logs', [])
        if completion_logs:
            # ì‚¬ì´ë“œë°” ë‚ ì§œ ë²”ìœ„ë¡œ ì™„ë£Œ ë¡œê·¸ í•„í„°ë§ í›„ ì ìš©
            date_range = (st.session_state.work_start_date, st.session_state.work_end_date)
            before_filter = len(df_full)
            mismatch_df = filter_completed_items(df_full, completion_logs, date_range)
            after_filter = len(mismatch_df)
            logger.info(f"ë¶€ì„œë³„ í†µê³„ íƒ­ ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ (ê¸°ê°„: {date_range[0]} ~ {date_range[1]}): {before_filter}ê°œ â†’ {after_filter}ê°œ")
        else:
            mismatch_df = df_full.copy()
            logger.info("ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ê°€ ì—†ì–´ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        logger.warning(f"ë¶€ì„œë³„ í†µê³„ íƒ­ ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ì˜¤ë¥˜: {e}")
        mismatch_df = df_full.copy()
    
    # ì„¸ì…˜ ìƒíƒœ ë®ì–´ì“°ê¸° ë°©ì§€ - ë‚ ì§œë³„ ì‘ì—… íƒ­ì˜ ì„ íƒ ìƒíƒœë¥¼ ë³´í˜¸
    # ëŒ€ì‹  ë¡œì»¬ ë³€ìˆ˜ë¡œë§Œ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ íƒ­ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ
    # st.session_state.mismatch_data = mismatch_df.copy()  # ì£¼ì„ ì²˜ë¦¬
    # ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ë©”ì‹œì§€ ê°„ì†Œí™”
    st.info(f"ğŸ“Š ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(mismatch_df)}ê°œ í•­ëª©")

    # 3. ë°ì´í„° ì¤€ë¹„ ë° ê²€ì¦
    if mismatch_df is None or mismatch_df.empty:
        st.info("ë¶ˆì¼ì¹˜ ë˜ëŠ” ëˆ„ë½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 4. ë°ì´í„° ê²€ì¦ (ì´ë¯¸ ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ì´ ì ìš©ëœ ìƒíƒœ)
    filtered_df = mismatch_df

    # 5. ë‚ ì§œ ì»¬ëŸ¼ ë³€í™˜ ë° ê²°ì¸¡ì¹˜ ì œê±°
    filtered_df['ë‚ ì§œ_dt'] = pd.to_datetime(filtered_df['ë‚ ì§œ'], errors='coerce')
    before_dropna = len(filtered_df)
    filtered_df = filtered_df.dropna(subset=['ë‚ ì§œ_dt'])
    after_dropna = len(filtered_df)
    
    if before_dropna != after_dropna:
        st.warning(f"âš ï¸ ë‚ ì§œ ë³€í™˜ ì‹¤íŒ¨ë¡œ {before_dropna - after_dropna}ê°œ í•­ëª© ì œì™¸ë¨")
    
    # 6. ì‚¬ì´ë“œë°” ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§
    mask = (
        (filtered_df['ë‚ ì§œ_dt'].dt.date >= st.session_state.work_start_date) &
        (filtered_df['ë‚ ì§œ_dt'].dt.date <= st.session_state.work_end_date)
    )
    date_filtered_df = filtered_df.loc[mask].copy()
    
    # ê¸°ê°„ í•„í„°ë§ ê²°ê³¼ ê°„ë‹¨ í‘œì‹œ
    if not date_filtered_df.empty:
        filtered_date_min = date_filtered_df['ë‚ ì§œ_dt'].min().strftime('%Y-%m-%d')
        filtered_date_max = date_filtered_df['ë‚ ì§œ_dt'].max().strftime('%Y-%m-%d')
        st.info(f"ğŸ“Š ê¸°ê°„ {st.session_state.work_start_date} ~ {st.session_state.work_end_date}: {len(date_filtered_df)}ê°œ í•­ëª©")

    if date_filtered_df.empty:
        st.warning("ì„ íƒí•œ ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
        
    # 7. ë¶€ì„œ í•„í„° (ì‚¬ì´ë“œë°” ê¸°ê°„ìœ¼ë¡œ í•„í„°ë§ëœ ë°ì´í„° ê¸°ì¤€)
    dept_options = ["ì „ì²´"] + sorted(date_filtered_df['ë¶€ì„œëª…'].dropna().unique())
    selected_dept = st.selectbox("ë¶€ì„œ ì„ íƒ", dept_options, key="filter_dept_select")

    if selected_dept == "ì „ì²´":
        view_df = date_filtered_df
    else:
        view_df = date_filtered_df[date_filtered_df['ë¶€ì„œëª…'] == selected_dept]

    # 10. ìµœì¢… ì»¬ëŸ¼ ì •ë¦¬ ë° ë°ì´í„° í‘œì‹œ
    display_columns = ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰', 'ìˆ˜ë ¹ëŸ‰', 'ì°¨ì´', 'ëˆ„ë½']
    
    # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬ (í•„í„° ìƒíƒœê°€ ë³€ê²½ë˜ì—ˆì„ ë•Œë§Œ ì¬ì²˜ë¦¬)
    current_filter_state = selected_dept
    if 'processed_view_df' not in st.session_state or st.session_state.get('last_filter_state') != current_filter_state:
        st.session_state.processed_view_df = view_df.copy()
        for col in display_columns:
            if col not in st.session_state.processed_view_df.columns:
                st.session_state.processed_view_df.loc[:, col] = ""
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ ì²˜ë¦¬
        for col in ['ì²­êµ¬ëŸ‰', 'ìˆ˜ë ¹ëŸ‰', 'ì°¨ì´']:
            st.session_state.processed_view_df.loc[:, col] = pd.to_numeric(
                st.session_state.processed_view_df.loc[:, col], 
                errors='coerce'
            ).fillna(0).astype(int)
        
        # ë‚ ì§œ í¬ë§· ë³€í™˜
        st.session_state.processed_view_df.loc[:, 'ë‚ ì§œ'] = pd.to_datetime(
            st.session_state.processed_view_df.loc[:, 'ë‚ ì§œ'], 
            errors='coerce'
        ).dt.strftime('%Y-%m-%d')
        
        # ëˆ„ë½ ì»¬ëŸ¼ ì²˜ë¦¬
        st.session_state.processed_view_df.loc[:, 'ëˆ„ë½'] = st.session_state.processed_view_df.loc[:, 'ëˆ„ë½'].fillna('').astype(str)
        
        # ì»¬ëŸ¼ ìˆœì„œ ì •ë¦¬
        st.session_state.processed_view_df = st.session_state.processed_view_df[display_columns]
        
        # í˜„ì¬ í•„í„° ìƒíƒœ ì €ì¥
        st.session_state.last_filter_state = current_filter_state

    # ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ í‘œì‹œ
    st.dataframe(st.session_state.processed_view_df, use_container_width=True)

    # 11. í†µê³„ ìš”ì•½
    st.markdown("---")
    st.subheader("í†µê³„ ìš”ì•½")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ë¯¸ì™„ë£Œ í•­ëª© ìˆ˜", len(st.session_state.processed_view_df))
        st.metric("í‘œì‹œëœ ë¶€ì„œ ìˆ˜", st.session_state.processed_view_df.loc[:, 'ë¶€ì„œëª…'].nunique())
    with col2:
        st.metric("ê¸°ê°„", f"{st.session_state.work_start_date} ~ {st.session_state.work_end_date}")
        # ì „ì‚°ëˆ„ë½ í•­ëª© ìˆ˜ ê³„ì‚°
        missing_count = st.session_state.processed_view_df.loc[:, 'ëˆ„ë½'].str.contains('ëˆ„ë½', na=False).sum()
        st.metric("ì „ì‚°ëˆ„ë½ í’ˆëª©", missing_count)
    with col3:
        # ê¸°ë³¸ ë¶ˆì¼ì¹˜ vs ì „ì‚°ëˆ„ë½ ë¹„ìœ¨
        total_items = len(st.session_state.processed_view_df)
        basic_mismatch = total_items - missing_count
        st.metric("ê¸°ë³¸ ë¶ˆì¼ì¹˜", basic_mismatch)
        if total_items > 0:
            missing_ratio = (missing_count / total_items) * 100
            st.metric("ì „ì‚°ëˆ„ë½ ë¹„ìœ¨", f"{missing_ratio:.1f}%")

    # 12. ì—‘ì…€ ë‹¤ìš´ë¡œë“œ
    st.markdown("---")
    if st.button("ì—‘ì…€ë¡œ ë‹¤ìš´ë¡œë“œ"):
        # ì‚¬ì´ë“œë°” ê¸°ê°„ ë‚´ì˜ ëª¨ë“  ë‚ ì§œ ì‚¬ìš©
        available_dates_in_period = sorted(date_filtered_df['ë‚ ì§œ_dt'].dt.strftime('%Y-%m-%d').unique())
        excel_data, file_name = download_department_excel(available_dates_in_period)
        if excel_data:
            st.download_button(
                label="ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                data=excel_data,
                file_name=file_name,
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )
        else:
            st.error("ì—‘ì…€ íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")


def get_department_pages(date_str, dept_name):
    """íŠ¹ì • ë‚ ì§œì™€ ë¶€ì„œì˜ PDF í˜ì´ì§€ ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        dept_page_tuples = st.session_state.departments_with_pages_by_date.get(date_str, [])
        return [page for dept, page in dept_page_tuples if dept == dept_name]
    except Exception as e:
        logger.error(f"get_department_pages ì˜¤ë¥˜: {e}", exc_info=True)
        return []


def get_department_items(date_str, dept_name):
    """íŠ¹ì • ë‚ ì§œì™€ ë¶€ì„œì˜ í’ˆëª© ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        aggregated_ocr_items = st.session_state.get('aggregated_ocr_items_by_date', {}).get(date_str, {})
        return set(map(str, aggregated_ocr_items.get(dept_name, [])))
    except Exception as e:
        logger.error(f"get_department_items ì˜¤ë¥˜: {e}", exc_info=True)
        return set()


def get_mismatch_items(date_str, dept_name):
    """íŠ¹ì • ë‚ ì§œì™€ ë¶€ì„œì˜ ë¶ˆì¼ì¹˜ í•­ëª©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    try:
        if 'mismatch_data' in st.session_state and not st.session_state.mismatch_data.empty:
            mismatch_data = st.session_state.mismatch_data[
                (st.session_state.mismatch_data['ë‚ ì§œ'] == date_str) &
                (st.session_state.mismatch_data['ë¶€ì„œëª…'] == dept_name)
            ]
            return mismatch_data.copy()
        return pd.DataFrame()
    except Exception as e:
        logger.error(f"get_mismatch_items ì˜¤ë¥˜: {e}", exc_info=True)
        return pd.DataFrame()

# ì™„ë£Œ í•­ëª© ê´€ë¦¬ íƒ­ í‘œì‹œ í•¨ìˆ˜
def display_completed_items_tab():
    """ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª© ê´€ë¦¬ íƒ­ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
    try:
        st.header("ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©")
        
        # â­ï¸ ì„¸ì…˜ ìƒíƒœë§Œ ì‚¬ìš© (main()ì—ì„œ ì´ë¯¸ ë¡œë“œë¨)
        completion_logs = st.session_state.get('completion_logs', [])
        
        # ì„¸ì…˜ì— ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€
        if not completion_logs and 'completion_logs' not in st.session_state:
            st.info("ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ë¥¼ ë¡œë“œ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
            return
                
        if not completion_logs: # ì—¬ê¸°ì„œ íŒŒì¼ì´ ì—†ê±°ë‚˜, ìˆì–´ë„ ë‚´ìš©ì´ ë¹„ì—ˆê±°ë‚˜, ë¡œë“œ ì‹¤íŒ¨ í›„ ë¹ˆë¦¬ìŠ¤íŠ¸ê°€ ëœ ëª¨ë“  ê²½ìš° ì²˜ë¦¬
            st.info("ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        # DataFrame ìƒì„±
        completed_df = pd.DataFrame(completion_logs)

        # ë‚ ì§œ í˜•ì‹ ë³€í™˜ (ì •ë ¬/í•„í„°ìš©)
        try:
            if 'ë‚ ì§œ' in completed_df.columns and completed_df['ë‚ ì§œ'].dtype == 'object':
                completed_df['ë‚ ì§œ_ì •ë ¬ìš©'] = pd.to_datetime(completed_df['ë‚ ì§œ'], format='%Y-%m-%d', errors='coerce')
                mask = completed_df['ë‚ ì§œ_ì •ë ¬ìš©'].isna()
                if mask.any():
                    completed_df.loc[mask, 'ë‚ ì§œ_ì •ë ¬ìš©'] = pd.to_datetime(
                        completed_df.loc[mask, 'ë‚ ì§œ'],
                        format='ISO8601',
                        errors='coerce'
                    )
        except Exception as e:
            logger.error(f"ë‚ ì§œ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            st.error("ë‚ ì§œ í˜•ì‹ ë³€í™˜ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
            return

        # ë‚ ì§œ í•„í„°ë§
        if 'ë‚ ì§œ_ì •ë ¬ìš©' in completed_df.columns:
            min_date = completed_df['ë‚ ì§œ_ì •ë ¬ìš©'].min()
            max_date = completed_df['ë‚ ì§œ_ì •ë ¬ìš©'].max()
            date_range = st.date_input(
                "ë‚ ì§œ ë²”ìœ„ ì„ íƒ",
                value=(min_date, max_date),
                min_value=min_date,
                max_value=max_date
            )
            if len(date_range) == 2:
                start_date, end_date = date_range
                completed_df = completed_df[
                    (completed_df['ë‚ ì§œ_ì •ë ¬ìš©'] >= pd.Timestamp(start_date)) &
                    (completed_df['ë‚ ì§œ_ì •ë ¬ìš©'] <= pd.Timestamp(end_date))
                ]

        # ë¶€ì„œ í•„í„°ë§
        if 'ë¶€ì„œëª…' in completed_df.columns:
            dept_options = ['ì „ì²´'] + sorted(completed_df['ë¶€ì„œëª…'].unique().tolist())
            selected_dept = st.selectbox("ë¶€ì„œ ì„ íƒ", dept_options)
            if selected_dept != 'ì „ì²´':
                completed_df = completed_df[completed_df['ë¶€ì„œëª…'] == selected_dept]

        # ì •ë ¬
        if 'ë‚ ì§œ_ì •ë ¬ìš©' in completed_df.columns:
            completed_df = completed_df.sort_values('ë‚ ì§œ_ì •ë ¬ìš©', ascending=False)

        # ê³ ìœ í‚¤ ì»¬ëŸ¼ ìƒì„± (ë‚ ì§œ_ë¶€ì„œëª…_ë¬¼í’ˆì½”ë“œ)
        completed_df['ê³ ìœ í‚¤'] = completed_df.apply(
            lambda row: f"{row['ë‚ ì§œ']}_{row['ë¶€ì„œëª…']}_{row['ë¬¼í’ˆì½”ë“œ']}", axis=1
        )

        # ì²´í¬ë°•ìŠ¤ ìƒíƒœë¥¼ ìœ„í•œ ì„¸ì…˜ ë³€ìˆ˜
        if 'completed_cancel_check' not in st.session_state:
            st.session_state.completed_cancel_check = {k: False for k in completed_df['ê³ ìœ í‚¤']}

        # UI: ì²´í¬ë°•ìŠ¤ì™€ í•¨ê»˜ í–‰ í‘œì‹œ
        st.write("**ì™„ë£Œ ì·¨ì†Œí•  í•­ëª©ì„ ì²´í¬í•˜ì„¸ìš”:**")
        checked_rows = []
        # í‘œì‹œí•  ì»¬ëŸ¼
        show_cols = ['ë‚ ì§œ', 'ë¶€ì„œëª…', 'ë¬¼í’ˆì½”ë“œ', 'ë¬¼í’ˆëª…', 'ì²­êµ¬ëŸ‰', 'ìˆ˜ë ¹ëŸ‰', 'ì°¨ì´', 'ì²˜ë¦¬ì‹œê°„']
        if 'ëˆ„ë½' in completed_df.columns:
            show_cols.append('ëˆ„ë½')

        # í…Œì´ë¸”+ì²´í¬ë°•ìŠ¤: í–‰ ë‹¨ìœ„ë¡œ
        for idx, row in completed_df.iterrows():
            with st.container():
                col1, col2 = st.columns([0.05, 0.95])
                key = row['ê³ ìœ í‚¤']
                checked = col1.checkbox(
                    "ì™„ë£Œ ì·¨ì†Œ ì„ íƒ",   # label í•„ìˆ˜
                    key=f"completed_cancel_{key}",
                    value=st.session_state.completed_cancel_check.get(key, False),
                    label_visibility="collapsed"
                )
                if checked:
                    checked_rows.append(key)
                # ë°ì´í„° í‘œì‹œ (row[show_cols])
                row_data = " | ".join(str(row[c]) for c in show_cols if c in row)
                col2.markdown(row_data)

        # ì™„ë£Œì·¨ì†Œ ë²„íŠ¼
        if st.button("ì„ íƒí•œ í•­ëª© ì™„ë£Œ ì·¨ì†Œ(ë˜ëŒë¦¬ê¸°)", disabled=(not checked_rows)):
            # ì²´í¬ëœ í–‰ë§Œ ì œì™¸í•˜ê³  ìƒˆë¡œ ì €ì¥
            new_df = completed_df[~completed_df['ê³ ìœ í‚¤'].isin(checked_rows)]
            new_logs = new_df.drop('ê³ ìœ í‚¤', axis=1).to_dict(orient="records")
            
            # S3Handler ìƒì„± (ì™„ë£Œ ì·¨ì†Œ ì‹œì—ë§Œ í•„ìš”)
            s3_handler = S3Handler()
            save_result = s3_handler.save_completion_log(new_logs)
            st.session_state.completion_logs = new_logs
            # ì²´í¬ ìƒíƒœ ì´ˆê¸°í™”
            st.session_state.completed_cancel_check = {k: False for k in new_df['ê³ ìœ í‚¤']}
            if save_result.get("status") == "success":
                st.success("ì„ íƒí•œ í•­ëª©ì˜ ì™„ë£Œ ì²˜ë¦¬ê°€ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            else:
                st.error("ì™„ë£Œ ì·¨ì†Œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        # í†µê³„ ì •ë³´ í‘œì‹œ
        st.markdown("---")
        st.subheader("í†µê³„ ì •ë³´")
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì´ ì™„ë£Œ í•­ëª© ìˆ˜", len(completed_df))
            if 'ë¶€ì„œëª…' in completed_df.columns:
                st.metric("ì²˜ë¦¬ëœ ë¶€ì„œ ìˆ˜", completed_df['ë¶€ì„œëª…'].nunique())
        with col2:
            if 'ë‚ ì§œ_ì •ë ¬ìš©' in completed_df.columns:
                st.metric("ì²˜ë¦¬ ê¸°ê°„", f"{(completed_df['ë‚ ì§œ_ì •ë ¬ìš©'].max() - completed_df['ë‚ ì§œ_ì •ë ¬ìš©'].min()).days + 1}ì¼")
            if 'ì²˜ë¦¬ì‹œê°„' in completed_df.columns:
                latest_process = pd.to_datetime(completed_df['ì²˜ë¦¬ì‹œê°„']).max()
                st.metric("ìµœê·¼ ì²˜ë¦¬ ì‹œê°„", latest_process.strftime('%Y-%m-%d %H:%M:%S'))

    except Exception as e:
        logger.error(f"ì™„ë£Œ í•­ëª© íƒ­ í‘œì‹œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
        st.error(f"ì™„ë£Œ í•­ëª©ì„ í‘œì‹œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")


def update_metadata_with_pdf(s3_handler, date_str):
    """PDF í‚¤ê°€ ì—†ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì •"""
    try:
        # 1. í˜„ì¬ ë©”íƒ€ë°ì´í„° ë¡œë“œ
        meta_result = s3_handler.load_metadata(date_str)
        if meta_result["status"] != "success":
            return False
            
        metadata = meta_result["data"]
        
        # 2. PDF í‚¤ê°€ ì—†ìœ¼ë©´ ì¶”ê°€
        if "pdf_key" not in metadata:
            # PDF íŒŒì¼ ì°¾ê¸°
            pdf_prefix = f"{S3_DIRS['PDF']}{date_str}/"
            response = s3_handler.s3_client.list_objects_v2(
                Bucket=s3_handler.bucket,
                Prefix=pdf_prefix
            )
            
            if 'Contents' in response:
                # ì²« ë²ˆì§¸ PDF íŒŒì¼ ì‚¬ìš©
                pdf_key = response['Contents'][0]['Key']
                metadata["pdf_key"] = pdf_key
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                s3_handler.save_metadata(date_str, metadata)
                logger.info(f"ë©”íƒ€ë°ì´í„° PDF í‚¤ ì¶”ê°€ ì„±ê³µ: {date_str}")
                return True
                
        return False
    except Exception as e:
        logger.error(f"ë©”íƒ€ë°ì´í„° ìˆ˜ì • ì‹¤íŒ¨: {e}")
        return False

def force_reload_excel_data(s3_handler):
    """ì—‘ì…€ ë°ì´í„° ê°•ì œ ë¦¬ë¡œë“œ"""
    try:
        # 1. ëˆ„ì  ì—‘ì…€ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        excel_key = f"{S3_DIRS['EXCEL']}latest/cumulative_excel.xlsx"
        excel_result = s3_handler.download_file(excel_key)
        
        if excel_result["status"] == "success":
            # 2. ì—‘ì…€ ë°ì´í„° ë¡œë“œ
            excel_data = pd.read_excel(io.BytesIO(excel_result["data"]))
            
            # 3. ì„¸ì…˜ì— ì €ì¥
            st.session_state.excel_data = excel_data
            logger.info(f"ì—‘ì…€ ë°ì´í„° ê°•ì œ ë¦¬ë¡œë“œ ì„±ê³µ: {len(excel_data)} í–‰")
            return True
            
        return False
    except Exception as e:
        logger.error(f"ì—‘ì…€ ë°ì´í„° ë¦¬ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False

def recalculate_mismatches(s3_handler):
    """ë¶ˆì¼ì¹˜ ë°ì´í„°ë¥¼ ì¬ê³„ì‚°í•˜ê³  S3ì— ì €ì¥"""
    try:
        if 'excel_data' in st.session_state and not st.session_state.excel_data.empty:
            # ë°ì´í„°í”„ë ˆì„ ë³µì‚¬ë³¸ ìƒì„±
            excel_df = st.session_state.excel_data.copy()
            logger.info(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ì¬ê³„ì‚° ì‹œì‘: ì—‘ì…€ ë°ì´í„° {len(excel_df)}ê°œ í–‰")
            
            # ë¶ˆì¼ì¹˜ ë°ì´í„° ê³„ì‚°
            mismatch_result = data_analyzer.find_mismatches(excel_df)
            
            if mismatch_result["status"] == "success":
                mismatch_data = mismatch_result["data"]
                logger.info(f"ì´ˆê¸° ë¶ˆì¼ì¹˜ ë°ì´í„° ê³„ì‚° ì™„ë£Œ: {len(mismatch_data)}ê°œ í•­ëª©")
                
                # ì œì™¸í•  ë¬¼í’ˆì½”ë“œ ë¦¬ìŠ¤íŠ¸ (í•˜ë“œì½”ë”©)
                excluded_item_codes = [
                    'L505001', 'L505002', 'L505003', 'L505004', 'L505005', 'L505006', 'L505007', 
                    'L505008', 'L505009', 'L505010', 'L505011', 'L505012', 'L505013', 'L505014',
                    'L605001', 'L605002', 'L605003', 'L605004', 'L605005', 'L605006'
                ]
                
                # ë¬¼í’ˆì½”ë“œ í•„í„°ë§
                if not mismatch_data.empty and 'ë¬¼í’ˆì½”ë“œ' in mismatch_data.columns:
                    before_exclude = len(mismatch_data)
                    mismatch_data = mismatch_data[
                        ~mismatch_data['ë¬¼í’ˆì½”ë“œ'].astype(str).isin(excluded_item_codes)
                    ]
                    after_exclude = len(mismatch_data)
                    logger.info(f"ë¬¼í’ˆì½”ë“œ ì œì™¸ í•„í„°ë§: {before_exclude}ê°œ â†’ {after_exclude}ê°œ (ì œì™¸ëœ í•­ëª©: {before_exclude - after_exclude}ê°œ)")
                
                # ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ í•„í„°ë§ (ì„¸ì…˜ ìƒíƒœ ì‚¬ìš©)
                try:
                    # ì„¸ì…˜ ìƒíƒœì—ì„œ ì™„ë£Œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸° (S3 ë¡œë”© ì—†ìŒ)
                    completion_logs = st.session_state.get('completion_logs', [])
                    logger.info(f"ì™„ë£Œ ì²˜ë¦¬ ë¡œê·¸ (ì„¸ì…˜): {len(completion_logs)}ê°œ í•­ëª©")
                    
                    if not mismatch_data.empty and completion_logs:
                        # ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª© í•„í„°ë§
                        before_completion_filter = len(mismatch_data)
                        filtered_data = filter_completed_items(mismatch_data, completion_logs)
                        after_completion_filter = len(filtered_data)
                        st.session_state.mismatch_data = filtered_data.reset_index(drop=True)
                        logger.info(f"ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§: {before_completion_filter}ê°œ â†’ {after_completion_filter}ê°œ (ì œì™¸ëœ í•­ëª©: {before_completion_filter - after_completion_filter}ê°œ)")
                    else:
                        # ì™„ë£Œ ì²˜ë¦¬ëœ í•­ëª©ì´ ì—†ê±°ë‚˜ ë¶ˆì¼ì¹˜ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŒ
                        st.session_state.mismatch_data = mismatch_data.reset_index(drop=True)
                        logger.info(f"ì™„ë£Œ ì²˜ë¦¬ í•„í„°ë§ ê±´ë„ˆëœ€: ìµœì¢… {len(st.session_state.mismatch_data)}ê°œ í•­ëª©")
                except Exception as filter_err:
                    logger.error(f"ì™„ë£Œ í•­ëª© í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(filter_err)}", exc_info=True)
                    # í•„í„°ë§ ì˜¤ë¥˜ì‹œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©
                    st.session_state.mismatch_data = mismatch_data.reset_index(drop=True)
                    logger.info(f"í•„í„°ë§ ì˜¤ë¥˜ë¡œ ì›ë³¸ ë°ì´í„° ì‚¬ìš©: {len(st.session_state.mismatch_data)}ê°œ í•­ëª©")
                
                # ë‚ ì§œë³„ë¡œ S3ì— ì €ì¥ (ì¤‘ìš”: ê¸°ë³¸ ë¶ˆì¼ì¹˜ ë°ì´í„°ë¥¼ ë‚ ì§œë³„ë¡œ ì €ì¥)
                try:
                    logger.info(f"ì„¸ì…˜ì— ì €ì¥ëœ ìµœì¢… ë¶ˆì¼ì¹˜ ë°ì´í„°: {len(st.session_state.mismatch_data)}ê°œ í•­ëª©")
                    save_all_date_mismatches(s3_handler, st.session_state.mismatch_data)
                    
                    # ì „ì²´ í†µí•© íŒŒì¼ë„ ì—…ë°ì´íŠ¸
                    update_result = s3_handler.update_full_mismatches_json()
                    if update_result["status"] == "success":
                        logger.info(f"ì „ì²´ í†µí•© íŒŒì¼ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {update_result.get('count', 0)}ê°œ í•­ëª©")
                        logger.info(f"ê¸°ë³¸ ë¶ˆì¼ì¹˜ ë°ì´í„° ì¬ê³„ì‚° ë° ì €ì¥ ì„±ê³µ: ì„¸ì…˜ {len(st.session_state.mismatch_data)}ê°œ í•­ëª©")
                        return True
                    else:
                        logger.error(f"ì „ì²´ í†µí•© íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {update_result['message']}")
                        return False
                except Exception as save_err:
                    logger.error(f"S3 ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(save_err)}", exc_info=True)
                    return False
            else:
                logger.error(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ê³„ì‚° ì‹¤íŒ¨: {mismatch_result['message']}")
                return False
        else:
            logger.warning("ì—‘ì…€ ë°ì´í„°ê°€ ì—†ì–´ ë¶ˆì¼ì¹˜ ë°ì´í„°ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False
    except Exception as e:
        logger.error(f"ë¶ˆì¼ì¹˜ ë°ì´í„° ì¬ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}", exc_info=True)
        return False

class ImageCache:
    def __init__(self):
        self._cache = {}
    
    def get_cache_key(self, date_str, dept_name, page_num):
        return f"{date_str}_{dept_name}_{page_num}"
    
    @lru_cache(maxsize=100)
    def get_image(self, cache_key):
        return self._cache.get(cache_key)
    
    def set_image(self, cache_key, image_data):
        self._cache[cache_key] = image_data

def process_images_parallel(images: List[Dict], max_workers: int = 4):
    """ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ìˆ˜í–‰"""
    results = []
    
    def process_single_image(img_info):
        try:
            img_bytes = get_pdf_preview_image_from_s3(img_info["file_key"])
            return {
                "status": "success",
                "data": img_bytes,
                "info": img_info
            }
        except Exception as e:
            logger.error(f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "status": "error",
                "message": str(e),
                "info": img_info
            }
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_image = {
            executor.submit(process_single_image, img_info): img_info 
            for img_info in images
        }
        
        for future in concurrent.futures.as_completed(future_to_image):
            result = future.result()
            results.append(result)
    
    return results

@st.cache_data(ttl=3600) # 1ì‹œê°„ ìºì‹œ
def get_preview_images_for_s3(date_str, s3_handler_dirs, s3_handler_bucket, s3_handler_aws_config):
    s3_config_temp = {
        "aws_access_key_id": s3_handler_aws_config["aws_access_key_id"],
        "aws_secret_access_key": s3_handler_aws_config["aws_secret_access_key"],
        "region_name": s3_handler_aws_config["region_name"]
    }
    s3_client_temp = boto3.client('s3', **s3_config_temp)
    
    # ì„ì‹œ S3Handler ìœ ì‚¬ ê°ì²´ ë˜ëŠ” ì§ì ‘ s3_client_temp ì‚¬ìš©
    # ì—¬ê¸°ì„œëŠ” s3_handler.load_metadata í˜¸ì¶œì„ ëª¨ë°©
    metadata_key = f"{s3_handler_dirs['METADATA']}{date_str}/metadata.json"
    try:
        response = s3_client_temp.get_object(Bucket=s3_handler_bucket, Key=metadata_key)
        metadata = json.loads(response['Body'].read())
        return metadata.get("preview_images", [])
    except ClientError as e:
        if e.response['Error']['Code'] == 'NoSuchKey':
            return [] # ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        logger.error(f"ë©”íƒ€ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨ ({date_str}) in get_preview_images_for_s3: {e}")
        return [] # ì˜¤ë¥˜ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
    except Exception as e:
        logger.error(f"get_preview_images_for_s3 ì˜ˆì™¸ ({date_str}): {e}")
        return []


def get_all_dept_images_for_dates(dates_to_load_tuple, selected_dept_filter, s3_handler_dirs, s3_handler_bucket, s3_handler_aws_config):
    all_dept_images = {}
    for date_str in dates_to_load_tuple:
        # ìºì‹±ëœ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ íŠ¹ì • ë‚ ì§œì˜ preview_imagesë¥¼ ê°€ì ¸ì˜´
        preview_images = get_preview_images_for_s3(date_str, s3_handler_dirs, s3_handler_bucket, s3_handler_aws_config)
        for img_info in preview_images:
            dept = img_info.get("dept")
            if not dept: continue
            if selected_dept_filter == "ì „ì²´" or dept == selected_dept_filter:
                if dept not in all_dept_images:
                    all_dept_images[dept] = []
                img_info['date'] = date_str 
                all_dept_images[dept].append(img_info)
    return all_dept_images

if __name__ == "__main__":
    # S3 ì—°ê²° í™•ì¸
    if not check_s3_connection():
        st.error("S3 ìŠ¤í† ë¦¬ì§€ ì—°ê²°ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”.")
        exit()
    
    main() 